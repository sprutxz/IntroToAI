{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required libraries imported below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_dangerous(color_choices):\n",
    "    if 3 in color_choices and 1 not in color_choices:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def one_hot_encode(color):\n",
    "    arr = np.zeros(4)    \n",
    "    arr[color-1] = 1\n",
    "    return arr\n",
    "    \n",
    "def generate_wiring_diagram(part): #part signfies the part of the project the data is being generated for (1 or 2)\n",
    "    if part != 1 and part != 2:\n",
    "        print(\"invalid parameter values entered while calling generate_wiring_diagram(part) function, part can either be 1 or 2\")\n",
    "        return\n",
    "    \n",
    "    diagram = np.zeros((20,20,4), dtype=int) # Initializing a 20x20 blank diagram\n",
    "    color_choices = [1, 2, 3, 4] # 1: Red, 2: Blue, 3: Yellow, 4: Green\n",
    "    row_choices = list(range(0,20))\n",
    "    col_choices= list(range(0,20))\n",
    "    dangerous = False\n",
    "    wire_to_cut = 0 # 0: No wire to cut, 1: Red, 2: Blue, 3: Yellow, 4: Green\n",
    "\n",
    "    start_with_row = random.choice([True, False])\n",
    "    \n",
    "    if start_with_row: # True: Start with row, False: Start with column\n",
    "        for i in range(4):\n",
    "            if i%2 == 0:\n",
    "                row = random.choice(row_choices) # Choosing a random row\n",
    "                color = random.choice(color_choices) # Choosing a random color\n",
    "                diagram[row, :] = one_hot_encode(color)\n",
    "                row_choices.remove(row)\n",
    "                color_choices.remove(color)\n",
    "                if not dangerous:\n",
    "                    dangerous = is_dangerous(color_choices) # checking if the diagram is dangerous based on the remaining colors\n",
    "            else:\n",
    "                col = random.choice(col_choices) # Choosing a random column\n",
    "                color = random.choice(color_choices) # Choosing a random color\n",
    "                diagram[:, col] = one_hot_encode(color)\n",
    "                col_choices.remove(col)\n",
    "                color_choices.remove(color)\n",
    "                if i == 1 and not dangerous:\n",
    "                    dangerous = is_dangerous(color_choices) # checking if the diagram is dangerous based on the remaining colors\n",
    "                    \n",
    "            if dangerous and i==3: # If the diagram is dangerous, the third wire placed is the one to cut\n",
    "                wire_to_cut = color\n",
    "                \n",
    "    else:\n",
    "        for i in range(4):\n",
    "            if i%2 == 0:\n",
    "                col = random.choice(col_choices) # Choosing a random column\n",
    "                color = random.choice(color_choices) # Choosing a random color\n",
    "                diagram[col, :] = one_hot_encode(color)\n",
    "                col_choices.remove(col)\n",
    "                color_choices.remove(color)\n",
    "                if not dangerous:\n",
    "                    dangerous = is_dangerous(color_choices) # checking if the diagram is dangerous based on the remaining colors\n",
    "            else:\n",
    "                row = random.choice(row_choices) # Choosing a random row\n",
    "                color = random.choice(color_choices) # Choosing a random color\n",
    "                diagram[:, row] = one_hot_encode(color)\n",
    "                row_choices.remove(row)\n",
    "                color_choices.remove(color)\n",
    "                if i == 1 and not dangerous:\n",
    "                    dangerous = is_dangerous(color_choices) # checking if the diagram is dangerous based on the remaining colors\n",
    "                    \n",
    "            if dangerous and i==3:\n",
    "                wire_to_cut = color # If the diagram is dangerous, the third wire placed is the one to cut\n",
    "\n",
    "    if part == 1: \n",
    "        return diagram, dangerous # If part is 1, return the diagram and whether it is dangerous or not\n",
    "    elif part == 2:\n",
    "        return diagram, wire_to_cut # If part is 2, return the diagram and the wire to cut\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory Class to store dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a class to create our \"dataset\" for model training\n",
    "class Memory:\n",
    "    # Constructor\n",
    "    def __init__(self, max_memory):\n",
    "        self.max_memory = max_memory # maximum amount of samples to remember\n",
    "        self.samples = [] # the samples\n",
    "\n",
    "    # Adding a sample to memory\n",
    "    def add_sample(self, diagram_vector, y):\n",
    "        if [diagram_vector, y] not in self.samples: # Checking if the sample is already in memory\n",
    "            self.samples.append((diagram_vector, y))\n",
    "        if len(self.samples) > self.max_memory: # Removing the earliest sample if we reach maximum memory\n",
    "            self.samples.pop(0)\n",
    "        return len(self.samples)\n",
    "    \n",
    "\n",
    "    # Sampling the samples we have in memory\n",
    "    def sample(self, no_samples): #no_samples is the number of samples you need\n",
    "        if no_samples > len(self.samples):\n",
    "            return random.sample(self.samples, len(self.samples))\n",
    "        else:\n",
    "            return random.sample(self.samples, no_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findConstants(diagram):\n",
    "    curr = diagram\n",
    "    colors = [[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]\n",
    "    constants = np.zeros(4) #An array that will contain the constant that represents relatively how much each wire is from its original color \n",
    "    for x in range(len(colors)): #For each color wire, we want to find the distance from edges\n",
    "        found_in_row = False\n",
    "        found_in_col = False\n",
    "        for i in range(len(curr)): #Finds if the color wire is present as a row\n",
    "            element_count = np.count_nonzero(np.all(curr[i] == colors[x], axis=1))#Finds the number of occurences of that color in that row\n",
    "            if(element_count/len(curr[i]) >= 0.5):\n",
    "                found_in_row = True\n",
    "                constants[x] = element_count/len(curr[i]) * 100.0\n",
    "                break\n",
    "        if(not found_in_row):\n",
    "            for i in range(len(curr[0])): # Finds if the color wire is present as a col\n",
    "                element_count = np.count_nonzero(np.all(curr[:, i] == colors[x], axis=1)) # Finds the number of occurences of that color in that column\n",
    "                if(element_count/len(curr) >= 0.5):\n",
    "                    found_in_col = True\n",
    "                    constants[x] = element_count/len(curr)\n",
    "                    break\n",
    "    return np.tanh(constants/100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data generation for part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagrams = 10000 #number of max diagrams to be stored in Memory class\n",
    "memory = Memory(diagrams) #more than diagrams and the oldest diagram stored will be removed\n",
    "diagram_arr = []\n",
    "training_size = 5000\n",
    "testing_size = 5000\n",
    "for i in range(diagrams):\n",
    "    diagram, y = generate_wiring_diagram(1) #type:ignore\n",
    "    memory.add_sample(diagram, int(y))\n",
    "samples = memory.sample(training_size+testing_size)\n",
    "training_data = samples[:training_size]\n",
    "testing_data = samples[training_size:]\n",
    "x_train = []\n",
    "y_train = []\n",
    "x_test = []\n",
    "y_test = []\n",
    "for diagram in training_data:\n",
    "    additional_features = findConstants(diagram[0])\n",
    "    x_train.append(np.concatenate(([1], diagram[0].flatten(), additional_features))) #A vector with each component being a vector of the each sample diagram's features\n",
    "    y_train.append(diagram[1])\n",
    "for diagram in testing_data:\n",
    "    additional_features = findConstants(diagram[0])\n",
    "    x_test.append(np.concatenate(([1], diagram[0].flatten(), additional_features))) #A vector with each component being a vector of the each sample diagram's features\n",
    "    y_test.append(diagram[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data gen leroy's implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagrams = 10000 #number of max diagrams to be stored in Memory class\n",
    "train_size = 5000 #number of samples to be generated\n",
    "test_size = 1000 #number of samples to be used for testing\n",
    "memory = Memory(diagrams) #more than diagrams and the oldest diagram stored will be removed\n",
    "\n",
    "#generating 5000 diagrams and storing them in memory\n",
    "len_samples = 0\n",
    "while len_samples!=diagrams:\n",
    "    diagram, y = generate_wiring_diagram(1) #type:ignore\n",
    "    if y != 0:\n",
    "        len_samples = memory.add_sample(diagram, y)\n",
    "samples = memory.sample(train_size+test_size)\n",
    "\n",
    "train_samples = samples[:train_size]\n",
    "test_samples = samples[train_size:]\n",
    "\n",
    "diagram_arr = []\n",
    "y_arr = []\n",
    "for diagrams, y in train_samples:\n",
    "    diagram_arr.append(diagrams)\n",
    "    y_arr.append(y)\n",
    "    \n",
    "test_diagram_arr = []\n",
    "test_y_arr = []    \n",
    "for test_diagrams, test_y in test_samples:\n",
    "    test_diagram_arr.append(test_diagrams)\n",
    "    test_y_arr.append(test_y)\n",
    "\n",
    "y_arr = np.array(y_arr) #converting the y_arr to a numpy array for easier computation\n",
    "test_y_arr = np.array(test_y_arr) #converting the test_y_arr to a numpy array for easier computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_part1(diagram_arr, y_arr):\n",
    "    flattened_diagrams = np.array([diagram.flatten() for diagram in diagram_arr])\n",
    "    additional_features = np.array([diagram.sum(axis=(0, 1)) for diagram in diagram_arr])\n",
    "    additional_features = np.square(np.subtract(additional_features[:, [0,2]], 17))\n",
    "    X_combined = np.concatenate((flattened_diagrams, additional_features), axis=1)\n",
    "    X = np.c_[X_combined, np.ones(X_combined.shape[0])]\n",
    "    Y = y_arr.astype(int)\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = process_data_part1(diagram_arr, y_arr)\n",
    "X_test, Y_test = process_data_part1(test_diagram_arr, test_y_arr)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"Y_train shape: {Y_train.shape}\")\n",
    "print(f\"First element of X_train: {X_train[0]}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"Y_test shape: {Y_test.shape}\")\n",
    "print(f\"First element of X_test: {X_test[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "    \n",
    "    def __init__(self,W,b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.test_loss_history = []\n",
    "        self.train_loss_history = []\n",
    "        self.epsilon = 1e-10\n",
    "    \n",
    "    def fit(self, X_train, Y_train, X_test, Y_test, learning_rate=0.001, epochs=10):\n",
    "        m = X_train.shape[0]\n",
    "        for epoch in range (epochs):\n",
    "            z = np.dot(X_train, self.W) + self.b\n",
    "            A = sigmoid(z)\n",
    "            \n",
    "            loss = -np.mean(y * np.log(A + self.epsilon) + (1 - y) * np.log(1 - A + self.epsilon))\n",
    "            self.train_loss_history.append(loss)\n",
    "            \n",
    "            z_test = np.dot(X_test, self.W) + self.b\n",
    "            A_test = sigmoid(z_test)\n",
    "            \n",
    "            loss_test = -np.mean(Y_test * np.log(A_test + self.epsilon) + (1 - Y_test) * np.log(1 - A_test + self.epsilon))\n",
    "            self.test_loss_history.append(loss_test)\n",
    "            \n",
    "            dz = A - Y_train\n",
    "            dW = np.dot(X_train.T, dz) / m\n",
    "            db = np.sum(dz) / m\n",
    "            \n",
    "            self.W -= learning_rate * dW\n",
    "            self.b -= learning_rate * db\n",
    "            \n",
    "            print(\"Epoch: \", epoch, \"Loss: \", loss)\n",
    "        \n",
    "        self.plot_loss(self.train_loss_history, self.test_loss_history)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        z = np.dot(X, self.W) + self.b\n",
    "        A = sigmoid(z)\n",
    "        return np.round(A)\n",
    "    \n",
    "    def plot_loss(self, train_loss, test_loss):\n",
    "        plt.plot(train_loss, label=\"Train Loss\")\n",
    "        plt.plot(test_loss, label=\"Test Loss\")\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Loss vs Epoch')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m num_features \u001b[38;5;241m=\u001b[39m \u001b[43mX_train\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m      2\u001b[0m W \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(num_features)\n\u001b[0;32m      3\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "num_features = X_train.shape[1]\n",
    "W = np.random.randn(num_features)\n",
    "b = 0\n",
    "model = LogisticRegression(W, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model\n",
    "model.fit(X_train, Y_train, X_test, Y_test, learning_rate=0.1, epochs = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "\n",
    "correct_predictions = np.sum(predictions == test_y_arr)\n",
    "print(correct_predictions)\n",
    "accuracy = (correct_predictions / Y_test.shape[0])*100\n",
    "print(f\"Accuracy on test set: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model for Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Loss:  0.35299321332869704\n",
      "Epoch:  1 Loss:  0.31791408437182594\n",
      "Epoch:  2 Loss:  0.30942784487171454\n",
      "Epoch:  3 Loss:  0.300378562297497\n",
      "Epoch:  4 Loss:  0.29161980575328417\n",
      "Epoch:  5 Loss:  0.2832614073395157\n",
      "Epoch:  6 Loss:  0.27530823563908147\n",
      "Epoch:  7 Loss:  0.26775373290475557\n",
      "Epoch:  8 Loss:  0.26058939235251966\n",
      "Epoch:  9 Loss:  0.2538055591055492\n",
      "Epoch:  10 Loss:  0.24739141699011313\n",
      "Epoch:  11 Loss:  0.241334958783193\n",
      "Epoch:  12 Loss:  0.23562302498804938\n",
      "Epoch:  13 Loss:  0.23024141707846651\n",
      "Epoch:  14 Loss:  0.22517507507599116\n",
      "Epoch:  15 Loss:  0.22040830175037535\n",
      "Epoch:  16 Loss:  0.2159250121359299\n",
      "Epoch:  17 Loss:  0.21170898695852114\n",
      "Epoch:  18 Loss:  0.20774411141487886\n",
      "Epoch:  19 Loss:  0.20401458562294728\n",
      "Epoch:  20 Loss:  0.20050509875493192\n",
      "Epoch:  21 Loss:  0.1972009641460825\n",
      "Epoch:  22 Loss:  0.19408821667838\n",
      "Epoch:  23 Loss:  0.1911536761768977\n",
      "Epoch:  24 Loss:  0.1883849816357122\n",
      "Epoch:  25 Loss:  0.18577060126806727\n",
      "Epoch:  26 Loss:  0.18329982310614878\n",
      "Epoch:  27 Loss:  0.18096273046397005\n",
      "Epoch:  28 Loss:  0.17875016615643755\n",
      "Epoch:  29 Loss:  0.17665368895651612\n",
      "Epoch:  30 Loss:  0.17466552534364893\n",
      "Epoch:  31 Loss:  0.1727785191294123\n",
      "Epoch:  32 Loss:  0.1709860810453765\n",
      "Epoch:  33 Loss:  0.16928213986998197\n",
      "Epoch:  34 Loss:  0.1676610961908424\n",
      "Epoch:  35 Loss:  0.16611777947699527\n",
      "Epoch:  36 Loss:  0.1646474087910911\n",
      "Epoch:  37 Loss:  0.1632455572103848\n",
      "Epoch:  38 Loss:  0.16190811984276532\n",
      "Epoch:  39 Loss:  0.1606312852085236\n",
      "Epoch:  40 Loss:  0.15941150969581117\n",
      "Epoch:  41 Loss:  0.15824549477352856\n",
      "Epoch:  42 Loss:  0.15713016664718274\n",
      "Epoch:  43 Loss:  0.15606265806116054\n",
      "Epoch:  44 Loss:  0.15504029197738778\n",
      "Epoch:  45 Loss:  0.15406056689045294\n",
      "Epoch:  46 Loss:  0.1531211435695849\n",
      "Epoch:  47 Loss:  0.15221983304651088\n",
      "Epoch:  48 Loss:  0.15135458569412807\n",
      "Epoch:  49 Loss:  0.1505234812637407\n",
      "Epoch:  50 Loss:  0.1497247197683034\n",
      "Epoch:  51 Loss:  0.14895661311592584\n",
      "Epoch:  52 Loss:  0.1482175774120378\n",
      "Epoch:  53 Loss:  0.14750612586051925\n",
      "Epoch:  54 Loss:  0.14682086220404122\n",
      "Epoch:  55 Loss:  0.14616047465212634\n",
      "Epoch:  56 Loss:  0.14552373025235332\n",
      "Epoch:  57 Loss:  0.1449094696658583\n",
      "Epoch:  58 Loss:  0.1443166023130883\n",
      "Epoch:  59 Loss:  0.1437441018597519\n",
      "Epoch:  60 Loss:  0.1431910020162725\n",
      "Epoch:  61 Loss:  0.1426563926268684\n",
      "Epoch:  62 Loss:  0.142139416026763\n",
      "Epoch:  63 Loss:  0.14163926364807816\n",
      "Epoch:  64 Loss:  0.14115517285666956\n",
      "Epoch:  65 Loss:  0.14068642400369938\n",
      "Epoch:  66 Loss:  0.14023233767702525\n",
      "Epoch:  67 Loss:  0.13979227213864545\n",
      "Epoch:  68 Loss:  0.13936562093545363\n",
      "Epoch:  69 Loss:  0.1389518106714876\n",
      "Epoch:  70 Loss:  0.13855029893064635\n",
      "Epoch:  71 Loss:  0.13816057233964416\n",
      "Epoch:  72 Loss:  0.1377821447616271\n",
      "Epoch:  73 Loss:  0.13741455561152824\n",
      "Epoch:  74 Loss:  0.13705736828481913\n",
      "Epoch:  75 Loss:  0.1367101686918844\n",
      "Epoch:  76 Loss:  0.13637256389072386\n",
      "Epoch:  77 Loss:  0.13604418081120645\n",
      "Epoch:  78 Loss:  0.13572466506452802\n",
      "Epoch:  79 Loss:  0.13541367983195066\n",
      "Epoch:  80 Loss:  0.13511090482731403\n",
      "Epoch:  81 Loss:  0.1348160353281635\n",
      "Epoch:  82 Loss:  0.13452878127070872\n",
      "Epoch:  83 Loss:  0.13424886640414097\n",
      "Epoch:  84 Loss:  0.13397602750016918\n",
      "Epoch:  85 Loss:  0.13371001361389195\n",
      "Epoch:  86 Loss:  0.13345058539243396\n",
      "Epoch:  87 Loss:  0.13319751442798944\n",
      "Epoch:  88 Loss:  0.1329505826521951\n",
      "Epoch:  89 Loss:  0.13270958176892503\n",
      "Epoch:  90 Loss:  0.13247431272286875\n",
      "Epoch:  91 Loss:  0.1322445852013907\n",
      "Epoch:  92 Loss:  0.13202021716737383\n",
      "Epoch:  93 Loss:  0.13180103442092922\n",
      "Epoch:  94 Loss:  0.13158687018797438\n",
      "Epoch:  95 Loss:  0.1313775647338608\n",
      "Epoch:  96 Loss:  0.13117296500033784\n",
      "Epoch:  97 Loss:  0.130972924264291\n",
      "Epoch:  98 Loss:  0.13077730181677152\n",
      "Epoch:  99 Loss:  0.1305859626609787\n",
      "Epoch:  100 Loss:  0.13039877722792753\n",
      "Epoch:  101 Loss:  0.13021562110863222\n",
      "Epoch:  102 Loss:  0.13003637480173263\n",
      "Epoch:  103 Loss:  0.12986092347553949\n",
      "Epoch:  104 Loss:  0.1296891567435849\n",
      "Epoch:  105 Loss:  0.12952096845279767\n",
      "Epoch:  106 Loss:  0.12935625648349766\n",
      "Epoch:  107 Loss:  0.12919492256046855\n",
      "Epoch:  108 Loss:  0.1290368720744014\n",
      "Epoch:  109 Loss:  0.12888201391306822\n",
      "Epoch:  110 Loss:  0.12873026030162513\n",
      "Epoch:  111 Loss:  0.1285815266514695\n",
      "Epoch:  112 Loss:  0.12843573141714665\n",
      "Epoch:  113 Loss:  0.12829279596080567\n",
      "Epoch:  114 Loss:  0.12815264442375104\n",
      "Epoch:  115 Loss:  0.1280152036046761\n",
      "Epoch:  116 Loss:  0.12788040284417063\n",
      "Epoch:  117 Loss:  0.12774817391513557\n",
      "Epoch:  118 Loss:  0.12761845091877133\n",
      "Epoch:  119 Loss:  0.12749117018579995\n",
      "Epoch:  120 Loss:  0.12736627018263214\n",
      "Epoch:  121 Loss:  0.12724369142219127\n",
      "Epoch:  122 Loss:  0.12712337637913362\n",
      "Epoch:  123 Loss:  0.12700526940921172\n",
      "Epoch:  124 Loss:  0.1268893166725615\n",
      "Epoch:  125 Loss:  0.12677546606067563\n",
      "Epoch:  126 Loss:  0.12666366712688543\n",
      "Epoch:  127 Loss:  0.1265538710201324\n",
      "Epoch:  128 Loss:  0.1264460304218758\n",
      "Epoch:  129 Loss:  0.1263400994859492\n",
      "Epoch:  130 Loss:  0.12623603378121503\n",
      "Epoch:  131 Loss:  0.12613379023687152\n",
      "Epoch:  132 Loss:  0.12603332709026183\n",
      "Epoch:  133 Loss:  0.12593460383706925\n",
      "Epoch:  134 Loss:  0.12583758118375604\n",
      "Epoch:  135 Loss:  0.12574222100214685\n",
      "Epoch:  136 Loss:  0.12564848628603253\n",
      "Epoch:  137 Loss:  0.12555634110970082\n",
      "Epoch:  138 Loss:  0.12546575058829296\n",
      "Epoch:  139 Loss:  0.12537668083989023\n",
      "Epoch:  140 Loss:  0.12528909894924967\n",
      "Epoch:  141 Loss:  0.12520297293310503\n",
      "Epoch:  142 Loss:  0.12511827170695017\n",
      "Epoch:  143 Loss:  0.1250349650532436\n",
      "Epoch:  144 Loss:  0.12495302359095148\n",
      "Epoch:  145 Loss:  0.12487241874637292\n",
      "Epoch:  146 Loss:  0.12479312272518378\n",
      "Epoch:  147 Loss:  0.12471510848563741\n",
      "Epoch:  148 Loss:  0.1246383497128731\n",
      "Epoch:  149 Loss:  0.1245628207942692\n",
      "Epoch:  150 Loss:  0.12448849679581028\n",
      "Epoch:  151 Loss:  0.12441535343939669\n",
      "Epoch:  152 Loss:  0.12434336708107235\n",
      "Epoch:  153 Loss:  0.12427251469012005\n",
      "Epoch:  154 Loss:  0.12420277382898454\n",
      "Epoch:  155 Loss:  0.12413412263398688\n",
      "Epoch:  156 Loss:  0.12406653979679191\n",
      "Epoch:  157 Loss:  0.12400000454660118\n",
      "Epoch:  158 Loss:  0.12393449663302626\n",
      "Epoch:  159 Loss:  0.12386999630962903\n",
      "Epoch:  160 Loss:  0.12380648431808199\n",
      "Epoch:  161 Loss:  0.1237439418729348\n",
      "Epoch:  162 Loss:  0.12368235064695247\n",
      "Epoch:  163 Loss:  0.12362169275700263\n",
      "Epoch:  164 Loss:  0.12356195075046925\n",
      "Epoch:  165 Loss:  0.12350310759216789\n",
      "Epoch:  166 Loss:  0.12344514665174519\n",
      "Epoch:  167 Loss:  0.12338805169153053\n",
      "Epoch:  168 Loss:  0.12333180685484155\n",
      "Epoch:  169 Loss:  0.12327639665469955\n",
      "Epoch:  170 Loss:  0.12322180596295829\n",
      "Epoch:  171 Loss:  0.12316801999981489\n",
      "Epoch:  172 Loss:  0.12311502432369653\n",
      "Epoch:  173 Loss:  0.1230628048214999\n",
      "Epoch:  174 Loss:  0.12301134769917801\n",
      "Epoch:  175 Loss:  0.1229606394726441\n",
      "Epoch:  176 Loss:  0.12291066695900582\n",
      "Epoch:  177 Loss:  0.12286141726808522\n",
      "Epoch:  178 Loss:  0.12281287779423708\n",
      "Epoch:  179 Loss:  0.12276503620844689\n",
      "Epoch:  180 Loss:  0.12271788045068893\n",
      "Epoch:  181 Loss:  0.12267139872254824\n",
      "Epoch:  182 Loss:  0.12262557948008102\n",
      "Epoch:  183 Loss:  0.12258041142692103\n",
      "Epoch:  184 Loss:  0.12253588350760725\n",
      "Epoch:  185 Loss:  0.12249198490112993\n",
      "Epoch:  186 Loss:  0.12244870501469372\n",
      "Epoch:  187 Loss:  0.1224060334776739\n",
      "Epoch:  188 Loss:  0.1223639601357763\n",
      "Epoch:  189 Loss:  0.12232247504537794\n",
      "Epoch:  190 Loss:  0.12228156846805088\n",
      "Epoch:  191 Loss:  0.12224123086525937\n",
      "Epoch:  192 Loss:  0.1222014528932247\n",
      "Epoch:  193 Loss:  0.12216222539794815\n",
      "Epoch:  194 Loss:  0.12212353941039258\n",
      "Epoch:  195 Loss:  0.12208538614181211\n",
      "Epoch:  196 Loss:  0.12204775697922327\n",
      "Epoch:  197 Loss:  0.12201064348101831\n",
      "Epoch:  198 Loss:  0.12197403737271015\n",
      "Epoch:  199 Loss:  0.12193793054280683\n",
      "Epoch:  200 Loss:  0.12190231503881058\n",
      "Epoch:  201 Loss:  0.12186718306333519\n",
      "Epoch:  202 Loss:  0.12183252697034186\n",
      "Epoch:  203 Loss:  0.121798339261483\n",
      "Epoch:  204 Loss:  0.12176461258255566\n",
      "Epoch:  205 Loss:  0.12173133972006207\n",
      "Epoch:  206 Loss:  0.12169851359786236\n",
      "Epoch:  207 Loss:  0.1216661272739335\n",
      "Epoch:  208 Loss:  0.121634173937215\n",
      "Epoch:  209 Loss:  0.12160264690454817\n",
      "Epoch:  210 Loss:  0.12157153961770195\n",
      "Epoch:  211 Loss:  0.12154084564048234\n",
      "Epoch:  212 Loss:  0.12151055865592772\n",
      "Epoch:  213 Loss:  0.12148067246357416\n",
      "Epoch:  214 Loss:  0.12145118097680588\n",
      "Epoch:  215 Loss:  0.12142207822027513\n",
      "Epoch:  216 Loss:  0.12139335832739498\n",
      "Epoch:  217 Loss:  0.12136501553789728\n",
      "Epoch:  218 Loss:  0.12133704419546426\n",
      "Epoch:  219 Loss:  0.12130943874541696\n",
      "Epoch:  220 Loss:  0.12128219373247234\n",
      "Epoch:  221 Loss:  0.12125530379855676\n",
      "Epoch:  222 Loss:  0.12122876368068097\n",
      "Epoch:  223 Loss:  0.12120256820886927\n",
      "Epoch:  224 Loss:  0.1211767123041455\n",
      "Epoch:  225 Loss:  0.1211511909765707\n",
      "Epoch:  226 Loss:  0.12112599932333415\n",
      "Epoch:  227 Loss:  0.1211011325268903\n",
      "Epoch:  228 Loss:  0.12107658585315195\n",
      "Epoch:  229 Loss:  0.12105235464972008\n",
      "Epoch:  230 Loss:  0.12102843434416716\n",
      "Epoch:  231 Loss:  0.12100482044236094\n",
      "Epoch:  232 Loss:  0.12098150852683096\n",
      "Epoch:  233 Loss:  0.12095849425517932\n",
      "Epoch:  234 Loss:  0.12093577335852755\n",
      "Epoch:  235 Loss:  0.12091334164000384\n",
      "Epoch:  236 Loss:  0.12089119497326958\n",
      "Epoch:  237 Loss:  0.12086932930108313\n",
      "Epoch:  238 Loss:  0.12084774063389424\n",
      "Epoch:  239 Loss:  0.12082642504847929\n",
      "Epoch:  240 Loss:  0.12080537868660539\n",
      "Epoch:  241 Loss:  0.12078459775373107\n",
      "Epoch:  242 Loss:  0.12076407851773235\n",
      "Epoch:  243 Loss:  0.1207438173076694\n",
      "Epoch:  244 Loss:  0.12072381051257067\n",
      "Epoch:  245 Loss:  0.12070405458025875\n",
      "Epoch:  246 Loss:  0.12068454601619263\n",
      "Epoch:  247 Loss:  0.12066528138234707\n",
      "Epoch:  248 Loss:  0.12064625729611142\n",
      "Epoch:  249 Loss:  0.12062747042921892\n",
      "Epoch:  250 Loss:  0.12060891750669846\n",
      "Epoch:  251 Loss:  0.12059059530585198\n",
      "Epoch:  252 Loss:  0.12057250065525471\n",
      "Epoch:  253 Loss:  0.12055463043378081\n",
      "Epoch:  254 Loss:  0.12053698156964651\n",
      "Epoch:  255 Loss:  0.12051955103948159\n",
      "Epoch:  256 Loss:  0.12050233586741702\n",
      "Epoch:  257 Loss:  0.12048533312419449\n",
      "Epoch:  258 Loss:  0.12046853992629913\n",
      "Epoch:  259 Loss:  0.12045195343510587\n",
      "Epoch:  260 Loss:  0.12043557085605053\n",
      "Epoch:  261 Loss:  0.12041938943781752\n",
      "Epoch:  262 Loss:  0.12040340647154352\n",
      "Epoch:  263 Loss:  0.12038761929003983\n",
      "Epoch:  264 Loss:  0.12037202526703675\n",
      "Epoch:  265 Loss:  0.12035662181643474\n",
      "Epoch:  266 Loss:  0.12034140639158178\n",
      "Epoch:  267 Loss:  0.12032637648455885\n",
      "Epoch:  268 Loss:  0.1203115296254861\n",
      "Epoch:  269 Loss:  0.12029686338184223\n",
      "Epoch:  270 Loss:  0.1202823753577968\n",
      "Epoch:  271 Loss:  0.12026806319355912\n",
      "Epoch:  272 Loss:  0.12025392456474002\n",
      "Epoch:  273 Loss:  0.12023995718172698\n",
      "Epoch:  274 Loss:  0.12022615878907415\n",
      "Epoch:  275 Loss:  0.1202125271649013\n",
      "Epoch:  276 Loss:  0.12019906012031072\n",
      "Epoch:  277 Loss:  0.1201857554988108\n",
      "Epoch:  278 Loss:  0.12017261117575703\n",
      "Epoch:  279 Loss:  0.12015962505779859\n",
      "Epoch:  280 Loss:  0.12014679508234229\n",
      "Epoch:  281 Loss:  0.1201341192170246\n",
      "Epoch:  282 Loss:  0.12012159545919142\n",
      "Epoch:  283 Loss:  0.12010922183539616\n",
      "Epoch:  284 Loss:  0.12009699640089999\n",
      "Epoch:  285 Loss:  0.12008491723918857\n",
      "Epoch:  286 Loss:  0.12007298246149395\n",
      "Epoch:  287 Loss:  0.12006119020632752\n",
      "Epoch:  288 Loss:  0.12004953863902293\n",
      "Epoch:  289 Loss:  0.12003802595129033\n",
      "Epoch:  290 Loss:  0.12002665036077237\n",
      "Epoch:  291 Loss:  0.1200154101106167\n",
      "Epoch:  292 Loss:  0.12000430346905255\n",
      "Epoch:  293 Loss:  0.11999332872897726\n",
      "Epoch:  294 Loss:  0.11998248420754984\n",
      "Epoch:  295 Loss:  0.11997176824579367\n",
      "Epoch:  296 Loss:  0.11996117920820507\n",
      "Epoch:  297 Loss:  0.11995071548237128\n",
      "Epoch:  298 Loss:  0.11994037547859665\n",
      "Epoch:  299 Loss:  0.11993015762953146\n",
      "Epoch:  300 Loss:  0.11992006038981323\n",
      "Epoch:  301 Loss:  0.11991008223571023\n",
      "Epoch:  302 Loss:  0.11990022166477735\n",
      "Epoch:  303 Loss:  0.11989047719551271\n",
      "Epoch:  304 Loss:  0.11988084736702267\n",
      "Epoch:  305 Loss:  0.11987133073869517\n",
      "Epoch:  306 Loss:  0.11986192588987797\n",
      "Epoch:  307 Loss:  0.1198526314195604\n",
      "Epoch:  308 Loss:  0.1198434459460653\n",
      "Epoch:  309 Loss:  0.11983436810674455\n",
      "Epoch:  310 Loss:  0.11982539655767815\n",
      "Epoch:  311 Loss:  0.11981652997338346\n",
      "Epoch:  312 Loss:  0.11980776704652522\n",
      "Epoch:  313 Loss:  0.11979910648763487\n",
      "Epoch:  314 Loss:  0.11979054702483163\n",
      "Epoch:  315 Loss:  0.1197820874035502\n",
      "Epoch:  316 Loss:  0.11977372638627312\n",
      "Epoch:  317 Loss:  0.11976546275226954\n",
      "Epoch:  318 Loss:  0.11975729529733464\n",
      "Epoch:  319 Loss:  0.1197492228335384\n",
      "Epoch:  320 Loss:  0.11974124418897694\n",
      "Epoch:  321 Loss:  0.11973335820752531\n",
      "Epoch:  322 Loss:  0.11972556374860178\n",
      "Epoch:  323 Loss:  0.1197178596869268\n",
      "Epoch:  324 Loss:  0.11971024491229704\n",
      "Epoch:  325 Loss:  0.11970271832935228\n",
      "Epoch:  326 Loss:  0.11969527885735627\n",
      "Epoch:  327 Loss:  0.11968792542997517\n",
      "Epoch:  328 Loss:  0.11968065699505992\n",
      "Epoch:  329 Loss:  0.11967347251443838\n",
      "Epoch:  330 Loss:  0.11966637096370261\n",
      "Epoch:  331 Loss:  0.11965935133200707\n",
      "Epoch:  332 Loss:  0.11965241262186538\n",
      "Epoch:  333 Loss:  0.11964555384895398\n",
      "Epoch:  334 Loss:  0.11963877404191695\n",
      "Epoch:  335 Loss:  0.11963207224217215\n",
      "Epoch:  336 Loss:  0.11962544750372978\n",
      "Epoch:  337 Loss:  0.11961889889300108\n",
      "Epoch:  338 Loss:  0.1196124254886212\n",
      "Epoch:  339 Loss:  0.11960602638126694\n",
      "Epoch:  340 Loss:  0.11959970067348552\n",
      "Epoch:  341 Loss:  0.11959344747951804\n",
      "Epoch:  342 Loss:  0.1195872659251313\n",
      "Epoch:  343 Loss:  0.11958115514745138\n",
      "Epoch:  344 Loss:  0.1195751142948002\n",
      "Epoch:  345 Loss:  0.11956914252653082\n",
      "Epoch:  346 Loss:  0.11956323901287286\n",
      "Epoch:  347 Loss:  0.1195574029347728\n",
      "Epoch:  348 Loss:  0.11955163348374413\n",
      "Epoch:  349 Loss:  0.11954592986171224\n",
      "Epoch:  350 Loss:  0.11954029128086929\n",
      "Epoch:  351 Loss:  0.11953471696352547\n",
      "Epoch:  352 Loss:  0.11952920614196758\n",
      "Epoch:  353 Loss:  0.11952375805831554\n",
      "Epoch:  354 Loss:  0.1195183719643847\n",
      "Epoch:  355 Loss:  0.1195130471215475\n",
      "Epoch:  356 Loss:  0.11950778280060038\n",
      "Epoch:  357 Loss:  0.11950257828163076\n",
      "Epoch:  358 Loss:  0.11949743285388503\n",
      "Epoch:  359 Loss:  0.11949234581564269\n",
      "Epoch:  360 Loss:  0.11948731647408722\n",
      "Epoch:  361 Loss:  0.11948234414518581\n",
      "Epoch:  362 Loss:  0.11947742815356192\n",
      "Epoch:  363 Loss:  0.1194725678323797\n",
      "Epoch:  364 Loss:  0.11946776252322233\n",
      "Epoch:  365 Loss:  0.11946301157597569\n",
      "Epoch:  366 Loss:  0.11945831434871558\n",
      "Epoch:  367 Loss:  0.1194536702075928\n",
      "Epoch:  368 Loss:  0.11944907852672203\n",
      "Epoch:  369 Loss:  0.11944453868807262\n",
      "Epoch:  370 Loss:  0.11944005008136119\n",
      "Epoch:  371 Loss:  0.11943561210394511\n",
      "Epoch:  372 Loss:  0.11943122416071761\n",
      "Epoch:  373 Loss:  0.11942688566400605\n",
      "Epoch:  374 Loss:  0.11942259603346864\n",
      "Epoch:  375 Loss:  0.11941835469599617\n",
      "Epoch:  376 Loss:  0.11941416108561373\n",
      "Epoch:  377 Loss:  0.11941001464338281\n",
      "Epoch:  378 Loss:  0.11940591481730775\n",
      "Epoch:  379 Loss:  0.1194018610622399\n",
      "Epoch:  380 Loss:  0.11939785283978677\n",
      "Epoch:  381 Loss:  0.11939388961822126\n",
      "Epoch:  382 Loss:  0.11938997087238959\n",
      "Epoch:  383 Loss:  0.11938609608362571\n",
      "Epoch:  384 Loss:  0.11938226473966425\n",
      "Epoch:  385 Loss:  0.11937847633455155\n",
      "Epoch:  386 Loss:  0.11937473036856538\n",
      "Epoch:  387 Loss:  0.11937102634812938\n",
      "Epoch:  388 Loss:  0.11936736378573232\n",
      "Epoch:  389 Loss:  0.11936374219984594\n",
      "Epoch:  390 Loss:  0.11936016111484829\n",
      "Epoch:  391 Loss:  0.11935662006094278\n",
      "Epoch:  392 Loss:  0.11935311857408062\n",
      "Epoch:  393 Loss:  0.11934965619588783\n",
      "Epoch:  394 Loss:  0.11934623247358825\n",
      "Epoch:  395 Loss:  0.1193428469599295\n",
      "Epoch:  396 Loss:  0.1193394992131117\n",
      "Epoch:  397 Loss:  0.11933618879671418\n",
      "Epoch:  398 Loss:  0.11933291527962621\n",
      "Epoch:  399 Loss:  0.11932967823597687\n",
      "Epoch:  400 Loss:  0.11932647724506645\n",
      "Epoch:  401 Loss:  0.11932331189129919\n",
      "Epoch:  402 Loss:  0.1193201817641161\n",
      "Epoch:  403 Loss:  0.11931708645792921\n",
      "Epoch:  404 Loss:  0.11931402557205861\n",
      "Epoch:  405 Loss:  0.11931099871066647\n",
      "Epoch:  406 Loss:  0.11930800548269527\n",
      "Epoch:  407 Loss:  0.11930504550180653\n",
      "Epoch:  408 Loss:  0.1193021183863171\n",
      "Epoch:  409 Loss:  0.11929922375914212\n",
      "Epoch:  410 Loss:  0.11929636124773493\n",
      "Epoch:  411 Loss:  0.11929353048402697\n",
      "Epoch:  412 Loss:  0.11929073110437105\n",
      "Epoch:  413 Loss:  0.11928796274948536\n",
      "Epoch:  414 Loss:  0.11928522506439619\n",
      "Epoch:  415 Loss:  0.11928251769838383\n",
      "Epoch:  416 Loss:  0.11927984030492567\n",
      "Epoch:  417 Loss:  0.11927719254164733\n",
      "Epoch:  418 Loss:  0.11927457407026379\n",
      "Epoch:  419 Loss:  0.11927198455653087\n",
      "Epoch:  420 Loss:  0.11926942367019386\n",
      "Epoch:  421 Loss:  0.11926689108493396\n",
      "Epoch:  422 Loss:  0.11926438647832219\n",
      "Epoch:  423 Loss:  0.11926190953176635\n",
      "Epoch:  424 Loss:  0.11925945993046437\n",
      "Epoch:  425 Loss:  0.11925703736335576\n",
      "Epoch:  426 Loss:  0.11925464152307433\n",
      "Epoch:  427 Loss:  0.11925227210590184\n",
      "Epoch:  428 Loss:  0.1192499288117216\n",
      "Epoch:  429 Loss:  0.11924761134397155\n",
      "Epoch:  430 Loss:  0.11924531940960331\n",
      "Epoch:  431 Loss:  0.11924305271903499\n",
      "Epoch:  432 Loss:  0.11924081098610671\n",
      "Epoch:  433 Loss:  0.11923859392804183\n",
      "Epoch:  434 Loss:  0.11923640126540033\n",
      "Epoch:  435 Loss:  0.11923423272203891\n",
      "Epoch:  436 Loss:  0.11923208802506935\n",
      "Epoch:  437 Loss:  0.11922996690481909\n",
      "Epoch:  438 Loss:  0.11922786909478883\n",
      "Epoch:  439 Loss:  0.11922579433161462\n",
      "Epoch:  440 Loss:  0.11922374235502832\n",
      "Epoch:  441 Loss:  0.11922171290781845\n",
      "Epoch:  442 Loss:  0.11921970573579496\n",
      "Epoch:  443 Loss:  0.11921772058774655\n",
      "Epoch:  444 Loss:  0.11921575721540849\n",
      "Epoch:  445 Loss:  0.11921381537342479\n",
      "Epoch:  446 Loss:  0.11921189481931004\n",
      "Epoch:  447 Loss:  0.11920999531341775\n",
      "Epoch:  448 Loss:  0.11920811661890274\n",
      "Epoch:  449 Loss:  0.11920625850168459\n",
      "Epoch:  450 Loss:  0.11920442073041951\n",
      "Epoch:  451 Loss:  0.1192026030764605\n",
      "Epoch:  452 Loss:  0.11920080531382793\n",
      "Epoch:  453 Loss:  0.11919902721917569\n",
      "Epoch:  454 Loss:  0.11919726857175814\n",
      "Epoch:  455 Loss:  0.11919552915339922\n",
      "Epoch:  456 Loss:  0.11919380874846083\n",
      "Epoch:  457 Loss:  0.11919210714381069\n",
      "Epoch:  458 Loss:  0.11919042412879231\n",
      "Epoch:  459 Loss:  0.11918875949519689\n",
      "Epoch:  460 Loss:  0.1191871130372282\n",
      "Epoch:  461 Loss:  0.11918548455147984\n",
      "Epoch:  462 Loss:  0.11918387383689984\n",
      "Epoch:  463 Loss:  0.11918228069476583\n",
      "Epoch:  464 Loss:  0.11918070492865555\n",
      "Epoch:  465 Loss:  0.11917914634442017\n",
      "Epoch:  466 Loss:  0.1191776047501531\n",
      "Epoch:  467 Loss:  0.11917607995616765\n",
      "Epoch:  468 Loss:  0.11917457177496582\n",
      "Epoch:  469 Loss:  0.11917308002121435\n",
      "Epoch:  470 Loss:  0.11917160451171732\n",
      "Epoch:  471 Loss:  0.1191701450653914\n",
      "Epoch:  472 Loss:  0.1191687015032387\n",
      "Epoch:  473 Loss:  0.11916727364832455\n",
      "Epoch:  474 Loss:  0.11916586132574802\n",
      "Epoch:  475 Loss:  0.1191644643626221\n",
      "Epoch:  476 Loss:  0.11916308258804532\n",
      "Epoch:  477 Loss:  0.11916171583308233\n",
      "Epoch:  478 Loss:  0.11916036393073622\n",
      "Epoch:  479 Loss:  0.11915902671592662\n",
      "Epoch:  480 Loss:  0.11915770402546767\n",
      "Epoch:  481 Loss:  0.11915639569804479\n",
      "Epoch:  482 Loss:  0.11915510157419071\n",
      "Epoch:  483 Loss:  0.11915382149626602\n",
      "Epoch:  484 Loss:  0.11915255530843516\n",
      "Epoch:  485 Loss:  0.11915130285664535\n",
      "Epoch:  486 Loss:  0.11915006398860649\n",
      "Epoch:  487 Loss:  0.11914883855376823\n",
      "Epoch:  488 Loss:  0.11914762640330015\n",
      "Epoch:  489 Loss:  0.11914642739007245\n",
      "Epoch:  490 Loss:  0.11914524136863308\n",
      "Epoch:  491 Loss:  0.11914406819518916\n",
      "Epoch:  492 Loss:  0.11914290772758883\n",
      "Epoch:  493 Loss:  0.11914175982529922\n",
      "Epoch:  494 Loss:  0.11914062434938726\n",
      "Epoch:  495 Loss:  0.1191395011625026\n",
      "Epoch:  496 Loss:  0.11913839012885821\n",
      "Epoch:  497 Loss:  0.11913729111421073\n",
      "Epoch:  498 Loss:  0.11913620398584276\n",
      "Epoch:  499 Loss:  0.11913512861254669\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.19614375 -1.67468212 -0.22317791 ...  0.31464221 12.61366716\n",
      " -0.26125289]\n"
     ]
    }
   ],
   "source": [
    "class LogisticRegressionModel():\n",
    "    def __init__(self, weights, alpha, epochs, lambdaVar):\n",
    "        self.weights = weights\n",
    "        self.alpha = alpha\n",
    "        self.epochs = epochs\n",
    "        self.train_loss_history = []\n",
    "        self.test_loss_history = []\n",
    "        self.lambdaVar = lambdaVar\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def log(self, x):\n",
    "        return np.log10(x)\n",
    "\n",
    "    def costFunction (self, features, y): #Loss Function\n",
    "        loss = -1 * y * self.log(self.sigmoid(np.dot(self.weights, features)) - (1-y) * self.log(self.sigmoid(1-np.dot(self.weights, features))))\n",
    "        return loss\n",
    "\n",
    "    def gradientDescent(self, features, weights, alpha, y):\n",
    "        regularization = np.dot(self.lambdaVar / len(weights), np.concatenate(([0], weights[1:])))    \n",
    "        gradient = np.zeros(len(weights))\n",
    "        func = self.sigmoid(np.dot(features, self.weights))\n",
    "        gradient = np.dot((func - y), features) + regularization\n",
    "        self.weights = self.weights - alpha * gradient\n",
    "        #print(\"Gradient: \", gradient)\n",
    "        #print(\"Regularization: \", regularization)\n",
    "        #print(self.weights)\n",
    "\n",
    "    def fit(self, weights, alpha, x_train, y_train, x_test, y_test, epochs):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "        for epoch in range(epochs): #Epochs\n",
    "            regularization = self.lambdaVar / (2 * len(weights)) * np.sum(weights[1:]**2) #L2 Regularization\n",
    "            for j in range(len(x_train)): #Loop through each sample in the dataset\n",
    "                self.gradientDescent(self.x_train[j], self.weights, self.alpha, self.y_train[j]) \n",
    "            loss_train = 0\n",
    "            for x in range(len(x_train)): #Calculate the loss for the training data\n",
    "                loss_train+=self.costFunction(self.x_train[x], self.y_train[x])\n",
    "            loss_train /= len(self.x_train)\n",
    "            loss_train+=regularization\n",
    "            self.train_loss_history.append(loss_train)\n",
    "            loss_test = 0 \n",
    "            for x in range(len(x_test)): #Calculate the loss for the testing data\n",
    "                loss_test+=self.costFunction(self.x_test[x], self.y_test[x])\n",
    "            loss_test /= len(self.x_test)\n",
    "            loss_test+=regularization\n",
    "            self.test_loss_history.append(loss_test)\n",
    "            \n",
    "            print(\"Epoch: \", epoch, \"Loss: \", loss_train)\n",
    "        self.plot_loss(self.train_loss_history, self.test_loss_history)\n",
    "        print(self.weights)\n",
    "\n",
    "    \n",
    "    def predict(self, X): #Check if this method works correctly!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        A = np.empty(len(X))\n",
    "        for i in range(len(X)):\n",
    "            z = np.dot(X[i], self.weights)\n",
    "            A[i] = sigmoid(z)\n",
    "        return np.round(A)\n",
    "\n",
    "\n",
    "    def plot_loss(self, train_loss, test_loss): #Plot the Epochs vs both Training and Testing Loss Curves\n",
    "        plt.plot(train_loss, label=\"Train Loss\")\n",
    "        plt.plot(test_loss, label=\"Test Loss\")\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Loss vs Epoch')\n",
    "        plt.legend()\n",
    "        plt.show()   \n",
    "        \n",
    "    #saving and loading the model\n",
    "    def save_model(self, name):\n",
    "        np.savez(name, weights = self.weights)\n",
    "         \n",
    "\n",
    "weights = np.random.randn(len(x_train[0])) #Randomize weights\n",
    "alpha = 0.01 # Represents step-size\n",
    "epochs = 500 #Number of times you loop through the training set \n",
    "lambdaVariable = 0.15\n",
    "model = LogisticRegressionModel(weights, alpha, epochs, lambdaVariable)\n",
    "model.fit(weights, alpha, x_train, y_train, x_test, y_test, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4441\n",
      "Accuracy on test set: 88.82\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(x_test) \n",
    "\n",
    "correct_predictions = np.sum(predictions == y_test)\n",
    "print(correct_predictions)\n",
    "accuracy = (correct_predictions / np.array(y_test).shape[0])*100\n",
    "print(f\"Accuracy on test set: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"model_non_linear.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data generation for part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagrams = 6000 #number of max diagrams to be stored in Memory class\n",
    "train_size = 500 #number of samples to be generated\n",
    "test_size = 1000 #number of samples to be used for testing\n",
    "memory = Memory(diagrams) #more than diagrams and the oldest diagram stored will be removed\n",
    "\n",
    "#generating 5000 diagrams and storing them in memory\n",
    "len_samples = 0\n",
    "while len_samples!=diagrams:\n",
    "    diagram, y = generate_wiring_diagram(2) #type:ignore\n",
    "    if y != 0:\n",
    "        len_samples = memory.add_sample(diagram, y)\n",
    "samples = memory.sample(train_size+test_size)\n",
    "\n",
    "train_samples = samples[:train_size]\n",
    "test_samples = samples[train_size:]\n",
    "\n",
    "diagram_arr = []\n",
    "y_arr = []\n",
    "for diagrams, y in train_samples:\n",
    "    diagram_arr.append(diagrams)\n",
    "    y_arr.append(y)\n",
    "    \n",
    "test_diagram_arr = []\n",
    "test_y_arr = []    \n",
    "for test_diagrams, test_y in test_samples:\n",
    "    test_diagram_arr.append(test_diagrams)\n",
    "    test_y_arr.append(test_y)\n",
    "\n",
    "y_arr = np.array(y_arr) #converting the y_arr to a numpy array for easier computation\n",
    "test_y_arr = np.array(test_y_arr) #converting the test_y_arr to a numpy array for easier computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_part2(diagram_arr, y_arr):\n",
    "    flattened_diagrams = np.array([diagram.flatten() for diagram in diagram_arr])\n",
    "    additional_features = np.array([diagram.sum(axis=(0, 1)) for diagram in diagram_arr])\n",
    "    additional_features = (np.exp(np.subtract(additional_features, 18)))\n",
    "    X_combined = np.concatenate((flattened_diagrams, additional_features), axis=1)\n",
    "    X = np.c_[X_combined, np.ones(X_combined.shape[0])]\n",
    "    Y = np.eye(4)[np.subtract(y_arr,1)]\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         ... 2.71828183 2.71828183 1.        ]\n",
      "X_train shape: (500, 1605)\n",
      "Y_train shape: (500, 4)\n",
      "First element of X_train: [0.         0.         0.         ... 2.71828183 2.71828183 1.        ]\n",
      "X_test shape: (1000, 1605)\n",
      "Y_test shape: (1000, 4)\n",
      "First element of X_test: [0.         0.         0.         ... 2.71828183 7.3890561  1.        ]\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train = process_data_part2(diagram_arr, y_arr)\n",
    "X_test, Y_test = process_data_part2(test_diagram_arr, test_y_arr)\n",
    "X_train_bad, Y_train_bad = process_data_bad(diagram_arr, y_arr)\n",
    "X_test_bad, Y_test_bad = process_data_bad(test_diagram_arr, test_y_arr)\n",
    "\n",
    "print(X_train[0])\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"Y_train shape: {Y_train.shape}\")\n",
    "print(f\"First element of X_train: {X_train[0]}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"Y_test shape: {Y_test.shape}\")\n",
    "print(f\"First element of X_test: {X_test[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2 implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the softmax function\n",
    "def softmax(x): \n",
    "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return e_x / e_x.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.epsilon = 1e-10\n",
    "        self.regularization = 0.01\n",
    "        self.train_loss_history = []\n",
    "        self.test_loss_history = []\n",
    "        \n",
    "    def fit(self, X_train, Y_train, X_test, Y_test, learning_rate = 0.001, epochs = 10):\n",
    "        m = X_train.shape[0]\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            #forward propagation for training\n",
    "            Z_train = np.dot(X_train, self.W) + self.b\n",
    "            A_train = softmax(Z_train)\n",
    "            \n",
    "            loss_train = -np.mean(np.sum(Y_train * np.log(A_train + self.epsilon), axis=1)) #calculating the loss\n",
    "            loss_train += 0.5 * self.regularization * np.sum(self.W ** 2) #calculating the regularization loss\n",
    "            \n",
    "            self.train_loss_history.append(loss_train)\n",
    "            \n",
    "            # Forward pass for test data\n",
    "            Z_test = np.dot(X_test, self.W) + self.b\n",
    "            A_test = softmax(Z_test)\n",
    "            \n",
    "            loss_test = -np.mean(np.sum(Y_test * np.log(A_test + self.epsilon), axis=1))\n",
    "            loss_test += 0.5 * self.regularization * np.sum(self.W ** 2)\n",
    "            self.test_loss_history.append(loss_test)\n",
    "            \n",
    "            #calculating the gradients\n",
    "            dW = (1/m) * np.dot(X_train.T, (A_train-Y_train)) + (self.regularization * self.W)\n",
    "            db = (1/m) * np.sum(A_train-Y_train, axis=0) + (self.regularization * self.b)\n",
    "            \n",
    "            #updating the weights and biases\n",
    "            self.W -= learning_rate * dW \n",
    "            self.b -= learning_rate * db\n",
    "            \n",
    "            print(\"Epoch: \", epoch, \"Loss: \", loss_train)\n",
    "        \n",
    "        self.plot_loss(self.train_loss_history, self.test_loss_history)\n",
    "\n",
    "    def predict(self, X): #prediction function\n",
    "        Z = np.dot(X, self.W) + self.b\n",
    "        probs = softmax(Z)\n",
    "        return np.argmax(probs, axis=1) + 1\n",
    "    \n",
    "    #plotting the loss\n",
    "    def plot_loss(self, train_loss_history, test_loss_history):\n",
    "        plt.plot(train_loss_history, label = \"train loss\")\n",
    "        plt.plot(test_loss_history, label = \"test loss\")\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Loss vs Epoch')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    #saving and loading the model\n",
    "    def save_model(self, name):\n",
    "        np.savez(name, W=self.W, b=self.b)\n",
    "        \n",
    "    def load_model(self, name):\n",
    "        loaded_data = np.load(name)\n",
    "        self.W = loaded_data['W']\n",
    "        self.b = loaded_data['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 4\n",
    "num_features = X_train.shape[1]\n",
    "W = np.random.rand(num_features, num_classes)\n",
    "b = np.zeros(num_classes)\n",
    "model = Model(W, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_model('model_non_linear.npz')  # Load weights from a saved file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model\n",
    "model.fit(X_train, Y_train, X_test, Y_test, learning_rate=0.1, epochs = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "\n",
    "correct_predictions = np.sum(predictions == test_y_arr)\n",
    "print(correct_predictions)\n",
    "accuracy = (correct_predictions / Y_test.shape[0])*100\n",
    "print(f\"Accuracy on test set: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_train)\n",
    "\n",
    "correct_predictions = np.sum(predictions == y_arr)\n",
    "print(correct_predictions)\n",
    "accuracy = (correct_predictions / Y_train.shape[0])*100\n",
    "print(f\"Accuracy on training set: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"model_non_linear.npz\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
