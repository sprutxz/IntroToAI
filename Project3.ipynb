{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required libraries imported below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_dangerous(color_choices):\n",
    "    if 3 in color_choices and 1 not in color_choices:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def one_hot_encode(color):\n",
    "    arr = np.zeros(4)    \n",
    "    arr[color-1] = 1\n",
    "    return arr\n",
    "    \n",
    "def generate_wiring_diagram(part): #part signfies the part of the project the data is being generated for (1 or 2)\n",
    "    if part != 1 and part != 2:\n",
    "        print(\"invalid parameter values entered while calling generate_wiring_diagram(part) function, part can either be 1 or 2\")\n",
    "        return\n",
    "    \n",
    "    diagram = np.zeros((20,20,4), dtype=int) # Initializing a 20x20 blank diagram\n",
    "    color_choices = [1, 2, 3, 4] # 1: Red, 2: Blue, 3: Yellow, 4: Green\n",
    "    row_choices = list(range(0,20))\n",
    "    col_choices= list(range(0,20))\n",
    "    dangerous = False\n",
    "    wire_to_cut = 0 # 0: No wire to cut, 1: Red, 2: Blue, 3: Yellow, 4: Green\n",
    "\n",
    "    start_with_row = random.choice([True, False])\n",
    "    \n",
    "    if start_with_row: # True: Start with row, False: Start with column\n",
    "        for i in range(4):\n",
    "            if i%2 == 0:\n",
    "                row = random.choice(row_choices) # Choosing a random row\n",
    "                color = random.choice(color_choices) # Choosing a random color\n",
    "                diagram[row, :] = one_hot_encode(color)\n",
    "                row_choices.remove(row)\n",
    "                color_choices.remove(color)\n",
    "                if not dangerous:\n",
    "                    dangerous = is_dangerous(color_choices) # checking if the diagram is dangerous based on the remaining colors\n",
    "            else:\n",
    "                col = random.choice(col_choices) # Choosing a random column\n",
    "                color = random.choice(color_choices) # Choosing a random color\n",
    "                diagram[:, col] = one_hot_encode(color)\n",
    "                col_choices.remove(col)\n",
    "                color_choices.remove(color)\n",
    "                if i == 1 and not dangerous:\n",
    "                    dangerous = is_dangerous(color_choices) # checking if the diagram is dangerous based on the remaining colors\n",
    "                    \n",
    "            if dangerous and i==3: # If the diagram is dangerous, the third wire placed is the one to cut\n",
    "                wire_to_cut = color\n",
    "                \n",
    "    else:\n",
    "        for i in range(4):\n",
    "            if i%2 == 0:\n",
    "                col = random.choice(col_choices) # Choosing a random column\n",
    "                color = random.choice(color_choices) # Choosing a random color\n",
    "                diagram[col, :] = one_hot_encode(color)\n",
    "                col_choices.remove(col)\n",
    "                color_choices.remove(color)\n",
    "                if not dangerous:\n",
    "                    dangerous = is_dangerous(color_choices) # checking if the diagram is dangerous based on the remaining colors\n",
    "            else:\n",
    "                row = random.choice(row_choices) # Choosing a random row\n",
    "                color = random.choice(color_choices) # Choosing a random color\n",
    "                diagram[:, row] = one_hot_encode(color)\n",
    "                row_choices.remove(row)\n",
    "                color_choices.remove(color)\n",
    "                if i == 1 and not dangerous:\n",
    "                    dangerous = is_dangerous(color_choices) # checking if the diagram is dangerous based on the remaining colors\n",
    "                    \n",
    "            if dangerous and i==3:\n",
    "                wire_to_cut = color # If the diagram is dangerous, the third wire placed is the one to cut\n",
    "\n",
    "    if part == 1: \n",
    "        return diagram, dangerous # If part is 1, return the diagram and whether it is dangerous or not\n",
    "    elif part == 2:\n",
    "        return diagram, wire_to_cut # If part is 2, return the diagram and the wire to cut\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory Class to store dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a class to create our \"dataset\" for model training\n",
    "class Memory:\n",
    "    # Constructor\n",
    "    def __init__(self, max_memory):\n",
    "        self.max_memory = max_memory # maximum amount of samples to remember\n",
    "        self.samples = [] # the samples\n",
    "\n",
    "    # Adding a sample to memory\n",
    "    def add_sample(self, diagram_vector, y):\n",
    "        if [diagram_vector, y] not in self.samples: # Checking if the sample is already in memory\n",
    "            self.samples.append((diagram_vector, y))\n",
    "        if len(self.samples) > self.max_memory: # Removing the earliest sample if we reach maximum memory\n",
    "            self.samples.pop(0)\n",
    "        return len(self.samples)\n",
    "    \n",
    "\n",
    "    # Sampling the samples we have in memory\n",
    "    def sample(self, no_samples): #no_samples is the number of samples you need\n",
    "        if no_samples > len(self.samples):\n",
    "            return random.sample(self.samples, len(self.samples))\n",
    "        else:\n",
    "            return random.sample(self.samples, no_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data generation for part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagrams = 5000 #number of max diagrams to be stored in Memory class\n",
    "memory = Memory(diagrams) #more than diagrams and the oldest diagram stored will be removed\n",
    "diagram_arr = []\n",
    "training_size = 100\n",
    "testing_size = 100\n",
    "for i in range(diagrams):\n",
    "    diagram, y = generate_wiring_diagram(1) #type:ignore\n",
    "    memory.add_sample(diagram, int(y == True))\n",
    "samples = memory.sample(training_size+testing_size)\n",
    "training_data = samples[:training_size]\n",
    "testing_data = samples[training_size:]\n",
    "x_train = []\n",
    "y_train = []\n",
    "x_test = []\n",
    "y_test = []\n",
    "for diagram in training_data:\n",
    "    x_train.append(np.concatenate(([1], diagram[0].flatten()))) #A vector with each component being a vector of the each sample diagram's features\n",
    "    y_train.append(diagram[1])\n",
    "for diagram in testing_data:\n",
    "    x_test.append(np.concatenate(([1], diagram[0].flatten()))) #A vector with each component being a vector of the each sample diagram's features\n",
    "    y_test.append(diagram[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model for Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionModel():\n",
    "    def __init__(self, weights, alpha, epochs):\n",
    "        self.weights = weights\n",
    "        self.alpha = alpha\n",
    "        self.epochs = epochs\n",
    "        self.train_loss_history = []\n",
    "        self.test_loss_history = []\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def log(self, x):\n",
    "        return np.log10(x)\n",
    "\n",
    "    def costFunction (self, features, weights, y): #Loss Function\n",
    "        loss = 0\n",
    "        for i in range(len(weights)):\n",
    "            loss += -1 * self.y[i] * self.log(self.sigmoid(np.dot(weights[i], features[i])) - (1-y[i]) * self.log(self.sigmoid(1-np.dot(weights[i], features[i]))))\n",
    "        loss /= 1/len(weights-1)\n",
    "        return loss\n",
    "\n",
    "    def gradientDescent(self, features, weights, alpha, y):\n",
    "        gradient = np.zeros(len(weights))\n",
    "        for j in range(len(weights)): #Loop through each through weights to update them\n",
    "            for i in range(len(features)): #Loop through each index for each pixel in the the diagram\n",
    "                func = self.sigmoid(np.dot(features, weights))\n",
    "                gradient[j] += (func-y) * features[i]\n",
    "        weights = weights - alpha * gradient\n",
    "\n",
    "    def fit(self, weights, alpha, x_train, y_train, x_test, y_test, epochs):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "\n",
    "        for i in range(epochs):\n",
    "            for j in range(len(x_train)): #Loop through each sample in the dataset\n",
    "                self.gradientDescent(x_train[j], weights, alpha, y_train[j])           \n",
    "\n",
    "weights = np.random.randn(len(x_train[0]))    \n",
    "alpha = 0.05 # Left at 0.05 for now\n",
    "epochs = 5\n",
    "model = LogisticRegressionModel(weights, alpha, epochs)\n",
    "model.fit(weights, alpha, x_train, y_train, x_test, y_test, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data generation for part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagrams = 6000 #number of max diagrams to be stored in Memory class\n",
    "train_size = 500 #number of samples to be generated\n",
    "test_size = 1000 #number of samples to be used for testing\n",
    "memory = Memory(diagrams) #more than diagrams and the oldest diagram stored will be removed\n",
    "\n",
    "#generating 5000 diagrams and storing them in memory\n",
    "len_samples = 0\n",
    "while len_samples!=diagrams:\n",
    "    diagram, y = generate_wiring_diagram(2) #type:ignore\n",
    "    if y != 0:\n",
    "        len_samples = memory.add_sample(diagram, y)\n",
    "samples = memory.sample(train_size+test_size)\n",
    "\n",
    "train_samples = samples[:train_size]\n",
    "test_samples = samples[train_size:]\n",
    "\n",
    "diagram_arr = []\n",
    "y_arr = []\n",
    "for diagrams, y in train_samples:\n",
    "    diagram_arr.append(diagrams)\n",
    "    y_arr.append(y)\n",
    "    \n",
    "test_diagram_arr = []\n",
    "test_y_arr = []    \n",
    "for test_diagrams, test_y in test_samples:\n",
    "    test_diagram_arr.append(test_diagrams)\n",
    "    test_y_arr.append(test_y)\n",
    "\n",
    "y_arr = np.array(y_arr) #converting the y_arr to a numpy array for easier computation\n",
    "test_y_arr = np.array(test_y_arr) #converting the test_y_arr to a numpy array for easier computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(diagram_arr, y_arr):\n",
    "    flattened_diagrams = np.array([diagram.flatten() for diagram in diagram_arr])\n",
    "    additional_features = np.array([diagram.sum(axis=(0, 1)) for diagram in diagram_arr])\n",
    "    additional_features = (np.exp(np.subtract(additional_features, 18)))\n",
    "    X_combined = np.concatenate((flattened_diagrams, additional_features), axis=1)\n",
    "    X = np.c_[X_combined, np.ones(X_combined.shape[0])]\n",
    "    Y = np.eye(4)[np.subtract(y_arr,1)]\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (500, 1605)\n",
      "Y_train shape: (500, 4)\n",
      "First element of X_train: [0.         0.         0.         ... 2.71828183 2.71828183 1.        ]\n",
      "X_test shape: (1000, 1605)\n",
      "Y_test shape: (1000, 4)\n",
      "First element of X_test: [0.         0.         0.         ... 7.3890561  2.71828183 1.        ]\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train = process_data(diagram_arr, y_arr)\n",
    "X_test, Y_test = process_data(test_diagram_arr, test_y_arr)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"Y_train shape: {Y_train.shape}\")\n",
    "print(f\"First element of X_train: {X_train[0]}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"Y_test shape: {Y_test.shape}\")\n",
    "print(f\"First element of X_test: {X_test[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2 implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the softmax function\n",
    "def softmax(x): \n",
    "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return e_x / e_x.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.epsilon = 1e-10\n",
    "        self.regularization = 0.01\n",
    "        self.train_loss_history = []\n",
    "        self.test_loss_history = []\n",
    "        \n",
    "    def fit(self, X_train, Y_train, X_test, Y_test, learning_rate = 0.001, epochs = 10):\n",
    "        m = X_train.shape[0]\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            #forward propagation for training\n",
    "            Z_train = np.dot(X_train, self.W) + self.b\n",
    "            A_train = softmax(Z_train)\n",
    "            \n",
    "            loss_train = -np.mean(np.sum(Y_train * np.log(A_train + self.epsilon), axis=1)) #calculating the loss\n",
    "            loss_train += 0.5 * self.regularization * np.sum(self.W ** 2) #calculating the regularization loss\n",
    "            \n",
    "            self.train_loss_history.append(loss_train)\n",
    "            \n",
    "            # Forward pass for test data\n",
    "            Z_test = np.dot(X_test, self.W) + self.b\n",
    "            A_test = softmax(Z_test)\n",
    "            \n",
    "            loss_test = -np.mean(np.sum(Y_test * np.log(A_test + self.epsilon), axis=1))\n",
    "            loss_test += 0.5 * self.regularization * np.sum(self.W ** 2)\n",
    "            self.test_loss_history.append(loss_test)\n",
    "            \n",
    "            #calculating the gradients\n",
    "            dW = (1/m) * np.dot(X_train.T, (A_train-Y_train)) + (self.regularization * self.W)\n",
    "            db = (1/m) * np.sum(A_train-Y_train, axis=0) + (self.regularization * self.b)\n",
    "            \n",
    "            #updating the weights and biases\n",
    "            self.W -= learning_rate * dW \n",
    "            self.b -= learning_rate * db\n",
    "            \n",
    "            print(\"Epoch: \", epoch, \"Loss: \", loss_train)\n",
    "        \n",
    "        self.plot_loss(self.train_loss_history, self.test_loss_history)\n",
    "                \n",
    "    def predict(self, X): #prediction function\n",
    "        Z = np.dot(X, self.W) + self.b\n",
    "        probs = softmax(Z)\n",
    "        return np.argmax(probs, axis=1) + 1\n",
    "    \n",
    "    #plotting the loss\n",
    "    def plot_loss(self, train_loss_history, test_loss_history):\n",
    "        plt.plot(train_loss_history, label = \"train loss\")\n",
    "        plt.plot(test_loss_history, label = \"test loss\")\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Loss vs Epoch')\n",
    "        plt.show()\n",
    "    \n",
    "    #saving and loading the model\n",
    "    def save_model(self, name):\n",
    "        np.savez(name, W=self.W, b=self.b)\n",
    "        \n",
    "    def load_model(self, name):\n",
    "        loaded_data = np.load(name)\n",
    "        self.W = loaded_data['W']\n",
    "        self.b = loaded_data['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1605, 4)\n"
     ]
    }
   ],
   "source": [
    "num_classes = 4\n",
    "num_features = X_train.shape[1]\n",
    "W = np.random.rand(num_features, num_classes)\n",
    "b = np.zeros(num_classes)\n",
    "model = Model(W, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_model('model_non_linear.npz')  # Load weights from a saved file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Loss:  14.009130667809378\n",
      "Epoch:  1 Loss:  12.527897147461895\n",
      "Epoch:  2 Loss:  11.971611775539856\n",
      "Epoch:  3 Loss:  11.607801968186234\n",
      "Epoch:  4 Loss:  11.352055775190552\n",
      "Epoch:  5 Loss:  11.165659395838789\n",
      "Epoch:  6 Loss:  11.025381051802084\n",
      "Epoch:  7 Loss:  10.916411257969514\n",
      "Epoch:  8 Loss:  10.829110810966203\n",
      "Epoch:  9 Loss:  10.757109306592167\n",
      "Epoch:  10 Loss:  10.696129111887139\n",
      "Epoch:  11 Loss:  10.643248791788482\n",
      "Epoch:  12 Loss:  10.596437838284029\n",
      "Epoch:  13 Loss:  10.554259262394796\n",
      "Epoch:  14 Loss:  10.515677432578784\n",
      "Epoch:  15 Loss:  10.47993263746716\n",
      "Epoch:  16 Loss:  10.446458186828847\n",
      "Epoch:  17 Loss:  10.414824787913807\n",
      "Epoch:  18 Loss:  10.384702584089865\n",
      "Epoch:  19 Loss:  10.355834794318953\n",
      "Epoch:  20 Loss:  10.328019101760768\n",
      "Epoch:  21 Loss:  10.30109431058796\n",
      "Epoch:  22 Loss:  10.274930645661474\n",
      "Epoch:  23 Loss:  10.249422610754928\n",
      "Epoch:  24 Loss:  10.224483669101547\n",
      "Epoch:  25 Loss:  10.200042238219641\n",
      "Epoch:  26 Loss:  10.176038643338897\n",
      "Epoch:  27 Loss:  10.152422777241389\n",
      "Epoch:  28 Loss:  10.129152285702974\n",
      "Epoch:  29 Loss:  10.106191147597919\n",
      "Epoch:  30 Loss:  10.08350855398757\n",
      "Epoch:  31 Loss:  10.061078015686206\n",
      "Epoch:  32 Loss:  10.038876646926282\n",
      "Epoch:  33 Loss:  10.01688458590506\n",
      "Epoch:  34 Loss:  9.995084522616612\n",
      "Epoch:  35 Loss:  9.973461311458022\n",
      "Epoch:  36 Loss:  9.952001651351289\n",
      "Epoch:  37 Loss:  9.930693820043635\n",
      "Epoch:  38 Loss:  9.909527452197365\n",
      "Epoch:  39 Loss:  9.888493353113128\n",
      "Epoch:  40 Loss:  9.86758334163417\n",
      "Epoch:  41 Loss:  9.846790117088899\n",
      "Epoch:  42 Loss:  9.826107146143762\n",
      "Epoch:  43 Loss:  9.805528566230443\n",
      "Epoch:  44 Loss:  9.785049102834426\n",
      "Epoch:  45 Loss:  9.764663998425398\n",
      "Epoch:  46 Loss:  9.74436895120389\n",
      "Epoch:  47 Loss:  9.724160062154816\n",
      "Epoch:  48 Loss:  9.704033789154224\n",
      "Epoch:  49 Loss:  9.6839869070837\n",
      "Epoch:  50 Loss:  9.664016473076703\n",
      "Epoch:  51 Loss:  9.644119796161037\n",
      "Epoch:  52 Loss:  9.624294410676953\n",
      "Epoch:  53 Loss:  9.604538052946188\n",
      "Epoch:  54 Loss:  9.584848640746836\n",
      "Epoch:  55 Loss:  9.565224255215615\n",
      "Epoch:  56 Loss:  9.545663124854837\n",
      "Epoch:  57 Loss:  9.52616361136841\n",
      "Epoch:  58 Loss:  9.506724197090659\n",
      "Epoch:  59 Loss:  9.48734347380533\n",
      "Epoch:  60 Loss:  9.468020132780408\n",
      "Epoch:  61 Loss:  9.448752955868464\n",
      "Epoch:  62 Loss:  9.429540807542798\n",
      "Epoch:  63 Loss:  9.410382627757036\n",
      "Epoch:  64 Loss:  9.391277425530847\n",
      "Epoch:  65 Loss:  9.372224273177173\n",
      "Epoch:  66 Loss:  9.353222301097404\n",
      "Epoch:  67 Loss:  9.334270693080242\n",
      "Epoch:  68 Loss:  9.31536868204827\n",
      "Epoch:  69 Loss:  9.296515546203038\n",
      "Epoch:  70 Loss:  9.277710605525739\n",
      "Epoch:  71 Loss:  9.25895321859557\n",
      "Epoch:  72 Loss:  9.240242779692558\n",
      "Epoch:  73 Loss:  9.221578716155506\n",
      "Epoch:  74 Loss:  9.20296048596911\n",
      "Epoch:  75 Loss:  9.184387575557341\n",
      "Epoch:  76 Loss:  9.165859497762739\n",
      "Epoch:  77 Loss:  9.147375789993585\n",
      "Epoch:  78 Loss:  9.128936012522853\n",
      "Epoch:  79 Loss:  9.11053974692465\n",
      "Epoch:  80 Loss:  9.09218659463535\n",
      "Epoch:  81 Loss:  9.073876175628017\n",
      "Epoch:  82 Loss:  9.055608127189819\n",
      "Epoch:  83 Loss:  9.037382102793336\n",
      "Epoch:  84 Loss:  9.019197771053426\n",
      "Epoch:  85 Loss:  9.001054814762279\n",
      "Epoch:  86 Loss:  8.982952929995944\n",
      "Epoch:  87 Loss:  8.964891825286259\n",
      "Epoch:  88 Loss:  8.94687122085279\n",
      "Epoch:  89 Loss:  8.928890847889738\n",
      "Epoch:  90 Loss:  8.910950447903424\n",
      "Epoch:  91 Loss:  8.893049772096225\n",
      "Epoch:  92 Loss:  8.875188580793253\n",
      "Epoch:  93 Loss:  8.857366642908481\n",
      "Epoch:  94 Loss:  8.839583735447158\n",
      "Epoch:  95 Loss:  8.821839643041773\n",
      "Epoch:  96 Loss:  8.804134157519002\n",
      "Epoch:  97 Loss:  8.786467077495285\n",
      "Epoch:  98 Loss:  8.768838207998906\n",
      "Epoch:  99 Loss:  8.751247360116633\n",
      "Epoch:  100 Loss:  8.733694350663075\n",
      "Epoch:  101 Loss:  8.716179001871145\n",
      "Epoch:  102 Loss:  8.698701141102116\n",
      "Epoch:  103 Loss:  8.681260600573838\n",
      "Epoch:  104 Loss:  8.663857217105875\n",
      "Epoch:  105 Loss:  8.646490831880346\n",
      "Epoch:  106 Loss:  8.629161290217402\n",
      "Epoch:  107 Loss:  8.61186844136433\n",
      "Epoch:  108 Loss:  8.594612138297327\n",
      "Epoch:  109 Loss:  8.577392237535141\n",
      "Epoch:  110 Loss:  8.56020859896373\n",
      "Epoch:  111 Loss:  8.543061085671225\n",
      "Epoch:  112 Loss:  8.525949563792521\n",
      "Epoch:  113 Loss:  8.508873902362877\n",
      "Epoch:  114 Loss:  8.491833973179897\n",
      "Epoch:  115 Loss:  8.474829650673394\n",
      "Epoch:  116 Loss:  8.457860811782613\n",
      "Epoch:  117 Loss:  8.440927335840348\n",
      "Epoch:  118 Loss:  8.424029104463509\n",
      "Epoch:  119 Loss:  8.407166001449752\n",
      "Epoch:  120 Loss:  8.390337912679781\n",
      "Epoch:  121 Loss:  8.373544726024981\n",
      "Epoch:  122 Loss:  8.356786331260038\n",
      "Epoch:  123 Loss:  8.340062619980264\n",
      "Epoch:  124 Loss:  8.32337348552333\n",
      "Epoch:  125 Loss:  8.306718822895148\n",
      "Epoch:  126 Loss:  8.290098528699632\n",
      "Epoch:  127 Loss:  8.273512501072155\n",
      "Epoch:  128 Loss:  8.256960639616429\n",
      "Epoch:  129 Loss:  8.240442845344656\n",
      "Epoch:  130 Loss:  8.223959020620725\n",
      "Epoch:  131 Loss:  8.207509069106285\n",
      "Epoch:  132 Loss:  8.191092895709572\n",
      "Epoch:  133 Loss:  8.174710406536743\n",
      "Epoch:  134 Loss:  8.158361508845694\n",
      "Epoch:  135 Loss:  8.142046111002093\n",
      "Epoch:  136 Loss:  8.125764122437616\n",
      "Epoch:  137 Loss:  8.109515453610198\n",
      "Epoch:  138 Loss:  8.093300015966214\n",
      "Epoch:  139 Loss:  8.077117721904461\n",
      "Epoch:  140 Loss:  8.06096848474188\n",
      "Epoch:  141 Loss:  8.044852218680884\n",
      "Epoch:  142 Loss:  8.028768838778243\n",
      "Epoch:  143 Loss:  8.012718260915399\n",
      "Epoch:  144 Loss:  7.996700401770184\n",
      "Epoch:  145 Loss:  7.98071517878983\n",
      "Epoch:  146 Loss:  7.964762510165212\n",
      "Epoch:  147 Loss:  7.948842314806271\n",
      "Epoch:  148 Loss:  7.932954512318553\n",
      "Epoch:  149 Loss:  7.91709902298078\n",
      "Epoch:  150 Loss:  7.901275767723448\n",
      "Epoch:  151 Loss:  7.885484668108362\n",
      "Epoch:  152 Loss:  7.8697256463090595\n",
      "Epoch:  153 Loss:  7.853998625092111\n",
      "Epoch:  154 Loss:  7.838303527799223\n",
      "Epoch:  155 Loss:  7.82264027833011\n",
      "Epoch:  156 Loss:  7.8070088011261065\n",
      "Epoch:  157 Loss:  7.791409021154478\n",
      "Epoch:  158 Loss:  7.775840863893384\n",
      "Epoch:  159 Loss:  7.760304255317494\n",
      "Epoch:  160 Loss:  7.744799121884179\n",
      "Epoch:  161 Loss:  7.729325390520296\n",
      "Epoch:  162 Loss:  7.713882988609513\n",
      "Epoch:  163 Loss:  7.698471843980134\n",
      "Epoch:  164 Loss:  7.683091884893451\n",
      "Epoch:  165 Loss:  7.667743040032543\n",
      "Epoch:  166 Loss:  7.652425238491526\n",
      "Epoch:  167 Loss:  7.63713840976525\n",
      "Epoch:  168 Loss:  7.621882483739379\n",
      "Epoch:  169 Loss:  7.606657390680878\n",
      "Epoch:  170 Loss:  7.591463061228869\n",
      "Epoch:  171 Loss:  7.5762994263858365\n",
      "Epoch:  172 Loss:  7.561166417509179\n",
      "Epoch:  173 Loss:  7.546063966303084\n",
      "Epoch:  174 Loss:  7.530992004810697\n",
      "Epoch:  175 Loss:  7.5159504654066165\n",
      "Epoch:  176 Loss:  7.500939280789645\n",
      "Epoch:  177 Loss:  7.485958383975816\n",
      "Epoch:  178 Loss:  7.471007708291688\n",
      "Epoch:  179 Loss:  7.4560871873678805\n",
      "Epoch:  180 Loss:  7.441196755132845\n",
      "Epoch:  181 Loss:  7.4263363458068605\n",
      "Epoch:  182 Loss:  7.411505893896255\n",
      "Epoch:  183 Loss:  7.396705334187817\n",
      "Epoch:  184 Loss:  7.381934601743434\n",
      "Epoch:  185 Loss:  7.367193631894889\n",
      "Epoch:  186 Loss:  7.352482360238855\n",
      "Epoch:  187 Loss:  7.337800722632079\n",
      "Epoch:  188 Loss:  7.323148655186698\n",
      "Epoch:  189 Loss:  7.308526094265745\n",
      "Epoch:  190 Loss:  7.2939329764788035\n",
      "Epoch:  191 Loss:  7.279369238677795\n",
      "Epoch:  192 Loss:  7.264834817952929\n",
      "Epoch:  193 Loss:  7.250329651628776\n",
      "Epoch:  194 Loss:  7.235853677260463\n",
      "Epoch:  195 Loss:  7.221406832630029\n",
      "Epoch:  196 Loss:  7.20698905574285\n",
      "Epoch:  197 Loss:  7.192600284824227\n",
      "Epoch:  198 Loss:  7.178240458316056\n",
      "Epoch:  199 Loss:  7.163909514873607\n",
      "Epoch:  200 Loss:  7.149607393362432\n",
      "Epoch:  201 Loss:  7.135334032855324\n",
      "Epoch:  202 Loss:  7.121089372629422\n",
      "Epoch:  203 Loss:  7.106873352163371\n",
      "Epoch:  204 Loss:  7.092685911134581\n",
      "Epoch:  205 Loss:  7.078526989416576\n",
      "Epoch:  206 Loss:  7.0643965270764175\n",
      "Epoch:  207 Loss:  7.050294464372208\n",
      "Epoch:  208 Loss:  7.036220741750667\n",
      "Epoch:  209 Loss:  7.0221752998447835\n",
      "Epoch:  210 Loss:  7.008158079471539\n",
      "Epoch:  211 Loss:  6.994169021629687\n",
      "Epoch:  212 Loss:  6.980208067497614\n",
      "Epoch:  213 Loss:  6.966275158431248\n",
      "Epoch:  214 Loss:  6.952370235962034\n",
      "Epoch:  215 Loss:  6.938493241794967\n",
      "Epoch:  216 Loss:  6.924644117806681\n",
      "Epoch:  217 Loss:  6.910822806043584\n",
      "Epoch:  218 Loss:  6.897029248720063\n",
      "Epoch:  219 Loss:  6.8832633882167125\n",
      "Epoch:  220 Loss:  6.86952516707864\n",
      "Epoch:  221 Loss:  6.855814528013796\n",
      "Epoch:  222 Loss:  6.8421314138913605\n",
      "Epoch:  223 Loss:  6.828475767740162\n",
      "Epoch:  224 Loss:  6.81484753274716\n",
      "Epoch:  225 Loss:  6.801246652255939\n",
      "Epoch:  226 Loss:  6.787673069765264\n",
      "Epoch:  227 Loss:  6.774126728927662\n",
      "Epoch:  228 Loss:  6.760607573548048\n",
      "Epoch:  229 Loss:  6.7471155475823785\n",
      "Epoch:  230 Loss:  6.733650595136339\n",
      "Epoch:  231 Loss:  6.720212660464077\n",
      "Epoch:  232 Loss:  6.706801687966946\n",
      "Epoch:  233 Loss:  6.6934176221923005\n",
      "Epoch:  234 Loss:  6.680060407832307\n",
      "Epoch:  235 Loss:  6.666729989722787\n",
      "Epoch:  236 Loss:  6.6534263128420985\n",
      "Epoch:  237 Loss:  6.640149322310019\n",
      "Epoch:  238 Loss:  6.626898963386684\n",
      "Epoch:  239 Loss:  6.613675181471532\n",
      "Epoch:  240 Loss:  6.6004779221022805\n",
      "Epoch:  241 Loss:  6.587307130953918\n",
      "Epoch:  242 Loss:  6.574162753837733\n",
      "Epoch:  243 Loss:  6.561044736700352\n",
      "Epoch:  244 Loss:  6.547953025622805\n",
      "Epoch:  245 Loss:  6.5348875668196085\n",
      "Epoch:  246 Loss:  6.521848306637873\n",
      "Epoch:  247 Loss:  6.508835191556427\n",
      "Epoch:  248 Loss:  6.495848168184961\n",
      "Epoch:  249 Loss:  6.482887183263182\n",
      "Epoch:  250 Loss:  6.469952183660006\n",
      "Epoch:  251 Loss:  6.457043116372734\n",
      "Epoch:  252 Loss:  6.444159928526294\n",
      "Epoch:  253 Loss:  6.431302567372441\n",
      "Epoch:  254 Loss:  6.418470980289018\n",
      "Epoch:  255 Loss:  6.405665114779214\n",
      "Epoch:  256 Loss:  6.39288491847083\n",
      "Epoch:  257 Loss:  6.380130339115582\n",
      "Epoch:  258 Loss:  6.367401324588395\n",
      "Epoch:  259 Loss:  6.354697822886715\n",
      "Epoch:  260 Loss:  6.342019782129849\n",
      "Epoch:  261 Loss:  6.329367150558301\n",
      "Epoch:  262 Loss:  6.316739876533125\n",
      "Epoch:  263 Loss:  6.304137908535293\n",
      "Epoch:  264 Loss:  6.291561195165073\n",
      "Epoch:  265 Loss:  6.2790096851414186\n",
      "Epoch:  266 Loss:  6.266483327301369\n",
      "Epoch:  267 Loss:  6.253982070599457\n",
      "Epoch:  268 Loss:  6.241505864107133\n",
      "Epoch:  269 Loss:  6.229054657012195\n",
      "Epoch:  270 Loss:  6.216628398618231\n",
      "Epoch:  271 Loss:  6.204227038344064\n",
      "Epoch:  272 Loss:  6.1918505257232175\n",
      "Epoch:  273 Loss:  6.179498810403381\n",
      "Epoch:  274 Loss:  6.167171842145892\n",
      "Epoch:  275 Loss:  6.154869570825217\n",
      "Epoch:  276 Loss:  6.142591946428442\n",
      "Epoch:  277 Loss:  6.13033891905478\n",
      "Epoch:  278 Loss:  6.118110438915087\n",
      "Epoch:  279 Loss:  6.105906456331366\n",
      "Epoch:  280 Loss:  6.093726921736301\n",
      "Epoch:  281 Loss:  6.081571785672786\n",
      "Epoch:  282 Loss:  6.069440998793465\n",
      "Epoch:  283 Loss:  6.057334511860282\n",
      "Epoch:  284 Loss:  6.045252275744024\n",
      "Epoch:  285 Loss:  6.033194241423888\n",
      "Epoch:  286 Loss:  6.021160359987048\n",
      "Epoch:  287 Loss:  6.009150582628214\n",
      "Epoch:  288 Loss:  5.997164860649236\n",
      "Epoch:  289 Loss:  5.98520314545865\n",
      "Epoch:  290 Loss:  5.973265388571299\n",
      "Epoch:  291 Loss:  5.961351541607919\n",
      "Epoch:  292 Loss:  5.94946155629473\n",
      "Epoch:  293 Loss:  5.937595384463048\n",
      "Epoch:  294 Loss:  5.9257529780489\n",
      "Epoch:  295 Loss:  5.9139342890926345\n",
      "Epoch:  296 Loss:  5.902139269738542\n",
      "Epoch:  297 Loss:  5.890367872234481\n",
      "Epoch:  298 Loss:  5.8786200489315075\n",
      "Epoch:  299 Loss:  5.8668957522835115\n",
      "Epoch:  300 Loss:  5.855194934846854\n",
      "Epoch:  301 Loss:  5.843517549280012\n",
      "Epoch:  302 Loss:  5.831863548343225\n",
      "Epoch:  303 Loss:  5.820232884898145\n",
      "Epoch:  304 Loss:  5.808625511907494\n",
      "Epoch:  305 Loss:  5.797041382434726\n",
      "Epoch:  306 Loss:  5.785480449643687\n",
      "Epoch:  307 Loss:  5.7739426667982805\n",
      "Epoch:  308 Loss:  5.762427987262144\n",
      "Epoch:  309 Loss:  5.750936364498319\n",
      "Epoch:  310 Loss:  5.739467752068933\n",
      "Epoch:  311 Loss:  5.728022103634873\n",
      "Epoch:  312 Loss:  5.716599372955479\n",
      "Epoch:  313 Loss:  5.705199513888233\n",
      "Epoch:  314 Loss:  5.693822480388434\n",
      "Epoch:  315 Loss:  5.6824682265089175\n",
      "Epoch:  316 Loss:  5.6711367063997296\n",
      "Epoch:  317 Loss:  5.65982787430784\n",
      "Epoch:  318 Loss:  5.648541684576846\n",
      "Epoch:  319 Loss:  5.637278091646668\n",
      "Epoch:  320 Loss:  5.626037050053273\n",
      "Epoch:  321 Loss:  5.614818514428372\n",
      "Epoch:  322 Loss:  5.603622439499146\n",
      "Epoch:  323 Loss:  5.592448780087958\n",
      "Epoch:  324 Loss:  5.581297491112066\n",
      "Epoch:  325 Loss:  5.570168527583362\n",
      "Epoch:  326 Loss:  5.559061844608076\n",
      "Epoch:  327 Loss:  5.547977397386518\n",
      "Epoch:  328 Loss:  5.536915141212805\n",
      "Epoch:  329 Loss:  5.525875031474581\n",
      "Epoch:  330 Loss:  5.514857023652768\n",
      "Epoch:  331 Loss:  5.503861073321288\n",
      "Epoch:  332 Loss:  5.492887136146814\n",
      "Epoch:  333 Loss:  5.481935167888494\n",
      "Epoch:  334 Loss:  5.47100512439771\n",
      "Epoch:  335 Loss:  5.4600969616178086\n",
      "Epoch:  336 Loss:  5.449210635583862\n",
      "Epoch:  337 Loss:  5.438346102422398\n",
      "Epoch:  338 Loss:  5.427503318351172\n",
      "Epoch:  339 Loss:  5.416682239678899\n",
      "Epoch:  340 Loss:  5.405882822805021\n",
      "Epoch:  341 Loss:  5.395105024219457\n",
      "Epoch:  342 Loss:  5.384348800502362\n",
      "Epoch:  343 Loss:  5.373614108323891\n",
      "Epoch:  344 Loss:  5.36290090444395\n",
      "Epoch:  345 Loss:  5.352209145711964\n",
      "Epoch:  346 Loss:  5.341538789066651\n",
      "Epoch:  347 Loss:  5.330889791535769\n",
      "Epoch:  348 Loss:  5.3202621102359\n",
      "Epoch:  349 Loss:  5.309655702372215\n",
      "Epoch:  350 Loss:  5.2990705252382355\n",
      "Epoch:  351 Loss:  5.288506536215618\n",
      "Epoch:  352 Loss:  5.277963692773924\n",
      "Epoch:  353 Loss:  5.267441952470396\n",
      "Epoch:  354 Loss:  5.256941272949727\n",
      "Epoch:  355 Loss:  5.24646161194385\n",
      "Epoch:  356 Loss:  5.236002927271708\n",
      "Epoch:  357 Loss:  5.225565176839042\n",
      "Epoch:  358 Loss:  5.215148318638167\n",
      "Epoch:  359 Loss:  5.204752310747756\n",
      "Epoch:  360 Loss:  5.194377111332633\n",
      "Epoch:  361 Loss:  5.18402267864355\n",
      "Epoch:  362 Loss:  5.173688971016977\n",
      "Epoch:  363 Loss:  5.1633759468748925\n",
      "Epoch:  364 Loss:  5.153083564724569\n",
      "Epoch:  365 Loss:  5.142811783158367\n",
      "Epoch:  366 Loss:  5.132560560853529\n",
      "Epoch:  367 Loss:  5.1223298565719695\n",
      "Epoch:  368 Loss:  5.112119629160069\n",
      "Epoch:  369 Loss:  5.101929837548468\n",
      "Epoch:  370 Loss:  5.091760440751874\n",
      "Epoch:  371 Loss:  5.081611397868844\n",
      "Epoch:  372 Loss:  5.071482668081593\n",
      "Epoch:  373 Loss:  5.061374210655791\n",
      "Epoch:  374 Loss:  5.051285984940367\n",
      "Epoch:  375 Loss:  5.041217950367303\n",
      "Epoch:  376 Loss:  5.031170066451444\n",
      "Epoch:  377 Loss:  5.021142292790301\n",
      "Epoch:  378 Loss:  5.0111345890638495\n",
      "Epoch:  379 Loss:  5.001146915034347\n",
      "Epoch:  380 Loss:  4.991179230546127\n",
      "Epoch:  381 Loss:  4.981231495525414\n",
      "Epoch:  382 Loss:  4.97130366998013\n",
      "Epoch:  383 Loss:  4.961395713999703\n",
      "Epoch:  384 Loss:  4.9515075877548815\n",
      "Epoch:  385 Loss:  4.94163925149754\n",
      "Epoch:  386 Loss:  4.931790665560493\n",
      "Epoch:  387 Loss:  4.921961790357311\n",
      "Epoch:  388 Loss:  4.912152586382128\n",
      "Epoch:  389 Loss:  4.902363014209465\n",
      "Epoch:  390 Loss:  4.892593034494035\n",
      "Epoch:  391 Loss:  4.882842607970567\n",
      "Epoch:  392 Loss:  4.873111695453623\n",
      "Epoch:  393 Loss:  4.863400257837406\n",
      "Epoch:  394 Loss:  4.853708256095591\n",
      "Epoch:  395 Loss:  4.844035651281137\n",
      "Epoch:  396 Loss:  4.834382404526113\n",
      "Epoch:  397 Loss:  4.82474847704151\n",
      "Epoch:  398 Loss:  4.815133830117069\n",
      "Epoch:  399 Loss:  4.805538425121102\n",
      "Epoch:  400 Loss:  4.795962223500317\n",
      "Epoch:  401 Loss:  4.7864051867796364\n",
      "Epoch:  402 Loss:  4.776867276562027\n",
      "Epoch:  403 Loss:  4.767348454528321\n",
      "Epoch:  404 Loss:  4.757848682437045\n",
      "Epoch:  405 Loss:  4.748367922124245\n",
      "Epoch:  406 Loss:  4.738906135503313\n",
      "Epoch:  407 Loss:  4.729463284564816\n",
      "Epoch:  408 Loss:  4.720039331376325\n",
      "Epoch:  409 Loss:  4.710634238082242\n",
      "Epoch:  410 Loss:  4.7012479669036304\n",
      "Epoch:  411 Loss:  4.691880480138048\n",
      "Epoch:  412 Loss:  4.6825317401593765\n",
      "Epoch:  413 Loss:  4.673201709417649\n",
      "Epoch:  414 Loss:  4.663890350438889\n",
      "Epoch:  415 Loss:  4.654597625824942\n",
      "Epoch:  416 Loss:  4.645323498253304\n",
      "Epoch:  417 Loss:  4.63606793047696\n",
      "Epoch:  418 Loss:  4.626830885324218\n",
      "Epoch:  419 Loss:  4.617612325698547\n",
      "Epoch:  420 Loss:  4.608412214578402\n",
      "Epoch:  421 Loss:  4.599230515017076\n",
      "Epoch:  422 Loss:  4.590067190142524\n",
      "Epoch:  423 Loss:  4.58092220315721\n",
      "Epoch:  424 Loss:  4.571795517337936\n",
      "Epoch:  425 Loss:  4.562687096035691\n",
      "Epoch:  426 Loss:  4.553596902675477\n",
      "Epoch:  427 Loss:  4.544524900756166\n",
      "Epoch:  428 Loss:  4.53547105385032\n",
      "Epoch:  429 Loss:  4.526435325604052\n",
      "Epoch:  430 Loss:  4.51741767973685\n",
      "Epoch:  431 Loss:  4.508418080041432\n",
      "Epoch:  432 Loss:  4.499436490383578\n",
      "Epoch:  433 Loss:  4.490472874701983\n",
      "Epoch:  434 Loss:  4.481527197008089\n",
      "Epoch:  435 Loss:  4.472599421385939\n",
      "Epoch:  436 Loss:  4.4636895119920155\n",
      "Epoch:  437 Loss:  4.454797433055087\n",
      "Epoch:  438 Loss:  4.445923148876053\n",
      "Epoch:  439 Loss:  4.437066623827791\n",
      "Epoch:  440 Loss:  4.428227822355001\n",
      "Epoch:  441 Loss:  4.419406708974052\n",
      "Epoch:  442 Loss:  4.4106032482728335\n",
      "Epoch:  443 Loss:  4.401817404910595\n",
      "Epoch:  444 Loss:  4.393049143617804\n",
      "Epoch:  445 Loss:  4.384298429195986\n",
      "Epoch:  446 Loss:  4.37556522651758\n",
      "Epoch:  447 Loss:  4.36684950052578\n",
      "Epoch:  448 Loss:  4.358151216234399\n",
      "Epoch:  449 Loss:  4.349470338727701\n",
      "Epoch:  450 Loss:  4.3408068331602685\n",
      "Epoch:  451 Loss:  4.332160664756841\n",
      "Epoch:  452 Loss:  4.323531798812176\n",
      "Epoch:  453 Loss:  4.3149202006908975\n",
      "Epoch:  454 Loss:  4.306325835827346\n",
      "Epoch:  455 Loss:  4.297748669725436\n",
      "Epoch:  456 Loss:  4.289188667958506\n",
      "Epoch:  457 Loss:  4.280645796169174\n",
      "Epoch:  458 Loss:  4.272120020069192\n",
      "Epoch:  459 Loss:  4.263611305439299\n",
      "Epoch:  460 Loss:  4.255119618129079\n",
      "Epoch:  461 Loss:  4.246644924056811\n",
      "Epoch:  462 Loss:  4.2381871892093335\n",
      "Epoch:  463 Loss:  4.229746379641892\n",
      "Epoch:  464 Loss:  4.221322461477998\n",
      "Epoch:  465 Loss:  4.212915400909291\n",
      "Epoch:  466 Loss:  4.204525164195389\n",
      "Epoch:  467 Loss:  4.196151717663753\n",
      "Epoch:  468 Loss:  4.187795027709541\n",
      "Epoch:  469 Loss:  4.179455060795463\n",
      "Epoch:  470 Loss:  4.17113178345165\n",
      "Epoch:  471 Loss:  4.162825162275505\n",
      "Epoch:  472 Loss:  4.154535163931566\n",
      "Epoch:  473 Loss:  4.146261755151366\n",
      "Epoch:  474 Loss:  4.138004902733293\n",
      "Epoch:  475 Loss:  4.12976457354245\n",
      "Epoch:  476 Loss:  4.121540734510517\n",
      "Epoch:  477 Loss:  4.113333352635614\n",
      "Epoch:  478 Loss:  4.1051423949821615\n",
      "Epoch:  479 Loss:  4.096967828680741\n",
      "Epoch:  480 Loss:  4.088809620927964\n",
      "Epoch:  481 Loss:  4.0806677389863255\n",
      "Epoch:  482 Loss:  4.072542150184076\n",
      "Epoch:  483 Loss:  4.064432821915079\n",
      "Epoch:  484 Loss:  4.05633972163868\n",
      "Epoch:  485 Loss:  4.048262816879569\n",
      "Epoch:  486 Loss:  4.0402020752276435\n",
      "Epoch:  487 Loss:  4.032157464337875\n",
      "Epoch:  488 Loss:  4.024128951930175\n",
      "Epoch:  489 Loss:  4.016116505789262\n",
      "Epoch:  490 Loss:  4.008120093764524\n",
      "Epoch:  491 Loss:  4.000139683769887\n",
      "Epoch:  492 Loss:  3.9921752437836817\n",
      "Epoch:  493 Loss:  3.984226741848512\n",
      "Epoch:  494 Loss:  3.9762941460711203\n",
      "Epoch:  495 Loss:  3.968377424622256\n",
      "Epoch:  496 Loss:  3.9604765457365443\n",
      "Epoch:  497 Loss:  3.952591477712353\n",
      "Epoch:  498 Loss:  3.9447221889116637\n",
      "Epoch:  499 Loss:  3.9368686477599395\n",
      "Epoch:  500 Loss:  3.9290308227459936\n",
      "Epoch:  501 Loss:  3.9212086824218586\n",
      "Epoch:  502 Loss:  3.913402195402663\n",
      "Epoch:  503 Loss:  3.9056113303664906\n",
      "Epoch:  504 Loss:  3.8978360560542606\n",
      "Epoch:  505 Loss:  3.890076341269593\n",
      "Epoch:  506 Loss:  3.8823321548786844\n",
      "Epoch:  507 Loss:  3.8746034658101745\n",
      "Epoch:  508 Loss:  3.8668902430550216\n",
      "Epoch:  509 Loss:  3.859192455666376\n",
      "Epoch:  510 Loss:  3.851510072759446\n",
      "Epoch:  511 Loss:  3.84384306351138\n",
      "Epoch:  512 Loss:  3.836191397161131\n",
      "Epoch:  513 Loss:  3.828555043009337\n",
      "Epoch:  514 Loss:  3.8209339704181886\n",
      "Epoch:  515 Loss:  3.8133281488113098\n",
      "Epoch:  516 Loss:  3.805737547673624\n",
      "Epoch:  517 Loss:  3.798162136551237\n",
      "Epoch:  518 Loss:  3.7906018850513066\n",
      "Epoch:  519 Loss:  3.783056762841921\n",
      "Epoch:  520 Loss:  3.7755267396519696\n",
      "Epoch:  521 Loss:  3.768011785271024\n",
      "Epoch:  522 Loss:  3.7605118695492132\n",
      "Epoch:  523 Loss:  3.7530269623970955\n",
      "Epoch:  524 Loss:  3.7455570337855426\n",
      "Epoch:  525 Loss:  3.7381020537456084\n",
      "Epoch:  526 Loss:  3.730661992368414\n",
      "Epoch:  527 Loss:  3.7232368198050176\n",
      "Epoch:  528 Loss:  3.7158265062663007\n",
      "Epoch:  529 Loss:  3.7084310220228383\n",
      "Epoch:  530 Loss:  3.701050337404782\n",
      "Epoch:  531 Loss:  3.6936844228017365\n",
      "Epoch:  532 Loss:  3.686333248662641\n",
      "Epoch:  533 Loss:  3.6789967854956456\n",
      "Epoch:  534 Loss:  3.671675003867992\n",
      "Epoch:  535 Loss:  3.664367874405894\n",
      "Epoch:  536 Loss:  3.6570753677944148\n",
      "Epoch:  537 Loss:  3.649797454777352\n",
      "Epoch:  538 Loss:  3.6425341061571137\n",
      "Epoch:  539 Loss:  3.635285292794601\n",
      "Epoch:  540 Loss:  3.6280509856090895\n",
      "Epoch:  541 Loss:  3.620831155578111\n",
      "Epoch:  542 Loss:  3.6136257737373336\n",
      "Epoch:  543 Loss:  3.6064348111804447\n",
      "Epoch:  544 Loss:  3.5992582390590337\n",
      "Epoch:  545 Loss:  3.592096028582472\n",
      "Epoch:  546 Loss:  3.584948151017799\n",
      "Epoch:  547 Loss:  3.5778145776896033\n",
      "Epoch:  548 Loss:  3.5706952799799057\n",
      "Epoch:  549 Loss:  3.5635902293280424\n",
      "Epoch:  550 Loss:  3.5564993972305516\n",
      "Epoch:  551 Loss:  3.549422755241053\n",
      "Epoch:  552 Loss:  3.5423602749701364\n",
      "Epoch:  553 Loss:  3.535311928085242\n",
      "Epoch:  554 Loss:  3.5282776863105503\n",
      "Epoch:  555 Loss:  3.5212575214268615\n",
      "Epoch:  556 Loss:  3.514251405271485\n",
      "Epoch:  557 Loss:  3.507259309738123\n",
      "Epoch:  558 Loss:  3.5002812067767577\n",
      "Epoch:  559 Loss:  3.493317068393535\n",
      "Epoch:  560 Loss:  3.486366866650652\n",
      "Epoch:  561 Loss:  3.479430573666247\n",
      "Epoch:  562 Loss:  3.472508161614278\n",
      "Epoch:  563 Loss:  3.4655996027244185\n",
      "Epoch:  564 Loss:  3.4587048692819393\n",
      "Epoch:  565 Loss:  3.4518239336275993\n",
      "Epoch:  566 Loss:  3.4449567681575304\n",
      "Epoch:  567 Loss:  3.438103345323127\n",
      "Epoch:  568 Loss:  3.431263637630934\n",
      "Epoch:  569 Loss:  3.4244376176425364\n",
      "Epoch:  570 Loss:  3.417625257974448\n",
      "Epoch:  571 Loss:  3.4108265312979946\n",
      "Epoch:  572 Loss:  3.404041410339215\n",
      "Epoch:  573 Loss:  3.397269867878739\n",
      "Epoch:  574 Loss:  3.3905118767516824\n",
      "Epoch:  575 Loss:  3.3837674098475365\n",
      "Epoch:  576 Loss:  3.3770364401100594\n",
      "Epoch:  577 Loss:  3.3703189405371634\n",
      "Epoch:  578 Loss:  3.363614884180808\n",
      "Epoch:  579 Loss:  3.356924244146889\n",
      "Epoch:  580 Loss:  3.350246993595134\n",
      "Epoch:  581 Loss:  3.3435831057389866\n",
      "Epoch:  582 Loss:  3.3369325538455046\n",
      "Epoch:  583 Loss:  3.3302953112352474\n",
      "Epoch:  584 Loss:  3.323671351282171\n",
      "Epoch:  585 Loss:  3.3170606474135185\n",
      "Epoch:  586 Loss:  3.310463173109714\n",
      "Epoch:  587 Loss:  3.3038789019042536\n",
      "Epoch:  588 Loss:  3.297307807383601\n",
      "Epoch:  589 Loss:  3.290749863187078\n",
      "Epoch:  590 Loss:  3.2842050430067573\n",
      "Epoch:  591 Loss:  3.2776733205873603\n",
      "Epoch:  592 Loss:  3.2711546697261498\n",
      "Epoch:  593 Loss:  3.2646490642728185\n",
      "Epoch:  594 Loss:  3.258156478129392\n",
      "Epoch:  595 Loss:  3.251676885250117\n",
      "Epoch:  596 Loss:  3.2452102596413597\n",
      "Epoch:  597 Loss:  3.2387565753614966\n",
      "Epoch:  598 Loss:  3.2323158065208157\n",
      "Epoch:  599 Loss:  3.225887927281407\n",
      "Epoch:  600 Loss:  3.2194729118570606\n",
      "Epoch:  601 Loss:  3.2130707345131606\n",
      "Epoch:  602 Loss:  3.206681369566585\n",
      "Epoch:  603 Loss:  3.2003047913855966\n",
      "Epoch:  604 Loss:  3.1939409743897462\n",
      "Epoch:  605 Loss:  3.187589893049763\n",
      "Epoch:  606 Loss:  3.181251521887454\n",
      "Epoch:  607 Loss:  3.1749258354756043\n",
      "Epoch:  608 Loss:  3.1686128084378686\n",
      "Epoch:  609 Loss:  3.162312415448675\n",
      "Epoch:  610 Loss:  3.1560246312331177\n",
      "Epoch:  611 Loss:  3.1497494305668585\n",
      "Epoch:  612 Loss:  3.1434867882760233\n",
      "Epoch:  613 Loss:  3.1372366792371023\n",
      "Epoch:  614 Loss:  3.1309990783768455\n",
      "Epoch:  615 Loss:  3.1247739606721656\n",
      "Epoch:  616 Loss:  3.118561301150034\n",
      "Epoch:  617 Loss:  3.1123610748873833\n",
      "Epoch:  618 Loss:  3.106173257011002\n",
      "Epoch:  619 Loss:  3.0999978226974396\n",
      "Epoch:  620 Loss:  3.0938347471729046\n",
      "Epoch:  621 Loss:  3.087684005713162\n",
      "Epoch:  622 Loss:  3.0815455736434387\n",
      "Epoch:  623 Loss:  3.0754194263383194\n",
      "Epoch:  624 Loss:  3.0693055392216517\n",
      "Epoch:  625 Loss:  3.0632038877664405\n",
      "Epoch:  626 Loss:  3.057114447494759\n",
      "Epoch:  627 Loss:  3.051037193977642\n",
      "Epoch:  628 Loss:  3.04497210283499\n",
      "Epoch:  629 Loss:  3.038919149735472\n",
      "Epoch:  630 Loss:  3.032878310396427\n",
      "Epoch:  631 Loss:  3.0268495605837664\n",
      "Epoch:  632 Loss:  3.0208328761118755\n",
      "Epoch:  633 Loss:  3.0148282328435165\n",
      "Epoch:  634 Loss:  3.0088356066897335\n",
      "Epoch:  635 Loss:  3.0028549736097516\n",
      "Epoch:  636 Loss:  2.996886309610885\n",
      "Epoch:  637 Loss:  2.9909295907484346\n",
      "Epoch:  638 Loss:  2.9849847931255957\n",
      "Epoch:  639 Loss:  2.9790518928933634\n",
      "Epoch:  640 Loss:  2.9731308662504334\n",
      "Epoch:  641 Loss:  2.9672216894431043\n",
      "Epoch:  642 Loss:  2.9613243387651886\n",
      "Epoch:  643 Loss:  2.955438790557911\n",
      "Epoch:  644 Loss:  2.949565021209818\n",
      "Epoch:  645 Loss:  2.943703007156678\n",
      "Epoch:  646 Loss:  2.937852724881394\n",
      "Epoch:  647 Loss:  2.9320141509139\n",
      "Epoch:  648 Loss:  2.926187261831073\n",
      "Epoch:  649 Loss:  2.9203720342566366\n",
      "Epoch:  650 Loss:  2.9145684448610676\n",
      "Epoch:  651 Loss:  2.908776470361502\n",
      "Epoch:  652 Loss:  2.9029960875216423\n",
      "Epoch:  653 Loss:  2.8972272731516595\n",
      "Epoch:  654 Loss:  2.8914700041081085\n",
      "Epoch:  655 Loss:  2.8857242572938273\n",
      "Epoch:  656 Loss:  2.8799900096578464\n",
      "Epoch:  657 Loss:  2.8742672381952987\n",
      "Epoch:  658 Loss:  2.868555919947325\n",
      "Epoch:  659 Loss:  2.862856032000981\n",
      "Epoch:  660 Loss:  2.857167551489146\n",
      "Epoch:  661 Loss:  2.8514904555904326\n",
      "Epoch:  662 Loss:  2.8458247215290915\n",
      "Epoch:  663 Loss:  2.840170326574924\n",
      "Epoch:  664 Loss:  2.8345272480431887\n",
      "Epoch:  665 Loss:  2.82889546329451\n",
      "Epoch:  666 Loss:  2.823274949734786\n",
      "Epoch:  667 Loss:  2.8176656848151023\n",
      "Epoch:  668 Loss:  2.8120676460316374\n",
      "Epoch:  669 Loss:  2.806480810925572\n",
      "Epoch:  670 Loss:  2.8009051570830037\n",
      "Epoch:  671 Loss:  2.795340662134849\n",
      "Epoch:  672 Loss:  2.789787303756763\n",
      "Epoch:  673 Loss:  2.7842450596690407\n",
      "Epoch:  674 Loss:  2.7787139076365337\n",
      "Epoch:  675 Loss:  2.773193825468559\n",
      "Epoch:  676 Loss:  2.7676847910188087\n",
      "Epoch:  677 Loss:  2.7621867821852635\n",
      "Epoch:  678 Loss:  2.7566997769101014\n",
      "Epoch:  679 Loss:  2.751223753179611\n",
      "Epoch:  680 Loss:  2.7457586890241026\n",
      "Epoch:  681 Loss:  2.7403045625178213\n",
      "Epoch:  682 Loss:  2.734861351778855\n",
      "Epoch:  683 Loss:  2.7294290349690513\n",
      "Epoch:  684 Loss:  2.724007590293927\n",
      "Epoch:  685 Loss:  2.7185969960025806\n",
      "Epoch:  686 Loss:  2.7131972303876086\n",
      "Epoch:  687 Loss:  2.707808271785012\n",
      "Epoch:  688 Loss:  2.702430098574117\n",
      "Epoch:  689 Loss:  2.6970626891774807\n",
      "Epoch:  690 Loss:  2.6917060220608104\n",
      "Epoch:  691 Loss:  2.6863600757328743\n",
      "Epoch:  692 Loss:  2.6810248287454144\n",
      "Epoch:  693 Loss:  2.6757002596930644\n",
      "Epoch:  694 Loss:  2.67038634721326\n",
      "Epoch:  695 Loss:  2.6650830699861534\n",
      "Epoch:  696 Loss:  2.659790406734532\n",
      "Epoch:  697 Loss:  2.6545083362237265\n",
      "Epoch:  698 Loss:  2.649236837261531\n",
      "Epoch:  699 Loss:  2.6439758886981166\n",
      "Epoch:  700 Loss:  2.6387254694259457\n",
      "Epoch:  701 Loss:  2.6334855583796863\n",
      "Epoch:  702 Loss:  2.628256134536131\n",
      "Epoch:  703 Loss:  2.623037176914109\n",
      "Epoch:  704 Loss:  2.617828664574407\n",
      "Epoch:  705 Loss:  2.6126305766196762\n",
      "Epoch:  706 Loss:  2.60744289219436\n",
      "Epoch:  707 Loss:  2.6022655904845995\n",
      "Epoch:  708 Loss:  2.5970986507181593\n",
      "Epoch:  709 Loss:  2.591942052164336\n",
      "Epoch:  710 Loss:  2.5867957741338814\n",
      "Epoch:  711 Loss:  2.581659795978919\n",
      "Epoch:  712 Loss:  2.5765340970928543\n",
      "Epoch:  713 Loss:  2.571418656910302\n",
      "Epoch:  714 Loss:  2.5663134549069957\n",
      "Epoch:  715 Loss:  2.56121847059971\n",
      "Epoch:  716 Loss:  2.5561336835461774\n",
      "Epoch:  717 Loss:  2.5510590733450056\n",
      "Epoch:  718 Loss:  2.5459946196355956\n",
      "Epoch:  719 Loss:  2.5409403020980608\n",
      "Epoch:  720 Loss:  2.5358961004531455\n",
      "Epoch:  721 Loss:  2.530861994462144\n",
      "Epoch:  722 Loss:  2.5258379639268194\n",
      "Epoch:  723 Loss:  2.5208239886893193\n",
      "Epoch:  724 Loss:  2.5158200486321016\n",
      "Epoch:  725 Loss:  2.5108261236778473\n",
      "Epoch:  726 Loss:  2.5058421937893836\n",
      "Epoch:  727 Loss:  2.5008682389696046\n",
      "Epoch:  728 Loss:  2.495904239261386\n",
      "Epoch:  729 Loss:  2.490950174747512\n",
      "Epoch:  730 Loss:  2.4860060255505885\n",
      "Epoch:  731 Loss:  2.4810717718329705\n",
      "Epoch:  732 Loss:  2.4761473937966763\n",
      "Epoch:  733 Loss:  2.4712328716833123\n",
      "Epoch:  734 Loss:  2.4663281857739907\n",
      "Epoch:  735 Loss:  2.4614333163892548\n",
      "Epoch:  736 Loss:  2.456548243888994\n",
      "Epoch:  737 Loss:  2.451672948672373\n",
      "Epoch:  738 Loss:  2.446807411177745\n",
      "Epoch:  739 Loss:  2.441951611882577\n",
      "Epoch:  740 Loss:  2.4371055313033767\n",
      "Epoch:  741 Loss:  2.4322691499956024\n",
      "Epoch:  742 Loss:  2.4274424485535984\n",
      "Epoch:  743 Loss:  2.422625407610507\n",
      "Epoch:  744 Loss:  2.417818007838196\n",
      "Epoch:  745 Loss:  2.4130202299471804\n",
      "Epoch:  746 Loss:  2.4082320546865454\n",
      "Epoch:  747 Loss:  2.4034534628438666\n",
      "Epoch:  748 Loss:  2.398684435245138\n",
      "Epoch:  749 Loss:  2.3939249527546895\n",
      "Epoch:  750 Loss:  2.3891749962751154\n",
      "Epoch:  751 Loss:  2.384434546747194\n",
      "Epoch:  752 Loss:  2.379703585149813\n",
      "Epoch:  753 Loss:  2.3749820924998937\n",
      "Epoch:  754 Loss:  2.3702700498523144\n",
      "Epoch:  755 Loss:  2.3655674382998333\n",
      "Epoch:  756 Loss:  2.360874238973015\n",
      "Epoch:  757 Loss:  2.356190433040153\n",
      "Epoch:  758 Loss:  2.351516001707196\n",
      "Epoch:  759 Loss:  2.3468509262176696\n",
      "Epoch:  760 Loss:  2.342195187852606\n",
      "Epoch:  761 Loss:  2.3375487679304654\n",
      "Epoch:  762 Loss:  2.3329116478070597\n",
      "Epoch:  763 Loss:  2.3282838088754843\n",
      "Epoch:  764 Loss:  2.323665232566035\n",
      "Epoch:  765 Loss:  2.319055900346141\n",
      "Epoch:  766 Loss:  2.314455793720286\n",
      "Epoch:  767 Loss:  2.3098648942299382\n",
      "Epoch:  768 Loss:  2.30528318345347\n",
      "Epoch:  769 Loss:  2.300710643006093\n",
      "Epoch:  770 Loss:  2.2961472545397745\n",
      "Epoch:  771 Loss:  2.2915929997431728\n",
      "Epoch:  772 Loss:  2.2870478603415587\n",
      "Epoch:  773 Loss:  2.2825118180967436\n",
      "Epoch:  774 Loss:  2.2779848548070065\n",
      "Epoch:  775 Loss:  2.2734669523070226\n",
      "Epoch:  776 Loss:  2.2689580924677872\n",
      "Epoch:  777 Loss:  2.2644582571965453\n",
      "Epoch:  778 Loss:  2.25996742843672\n",
      "Epoch:  779 Loss:  2.2554855881678373\n",
      "Epoch:  780 Loss:  2.251012718405458\n",
      "Epoch:  781 Loss:  2.2465488012011003\n",
      "Epoch:  782 Loss:  2.2420938186421737\n",
      "Epoch:  783 Loss:  2.237647752851903\n",
      "Epoch:  784 Loss:  2.23321058598926\n",
      "Epoch:  785 Loss:  2.228782300248886\n",
      "Epoch:  786 Loss:  2.2243628778610303\n",
      "Epoch:  787 Loss:  2.2199523010914692\n",
      "Epoch:  788 Loss:  2.2155505522414414\n",
      "Epoch:  789 Loss:  2.211157613647574\n",
      "Epoch:  790 Loss:  2.2067734676818143\n",
      "Epoch:  791 Loss:  2.202398096751355\n",
      "Epoch:  792 Loss:  2.1980314832985695\n",
      "Epoch:  793 Loss:  2.193673609800936\n",
      "Epoch:  794 Loss:  2.1893244587709715\n",
      "Epoch:  795 Loss:  2.184984012756159\n",
      "Epoch:  796 Loss:  2.1806522543388787\n",
      "Epoch:  797 Loss:  2.1763291661363398\n",
      "Epoch:  798 Loss:  2.172014730800508\n",
      "Epoch:  799 Loss:  2.1677089310180375\n",
      "Epoch:  800 Loss:  2.163411749510202\n",
      "Epoch:  801 Loss:  2.1591231690328248\n",
      "Epoch:  802 Loss:  2.1548431723762094\n",
      "Epoch:  803 Loss:  2.1505717423650736\n",
      "Epoch:  804 Loss:  2.146308861858475\n",
      "Epoch:  805 Loss:  2.142054513749748\n",
      "Epoch:  806 Loss:  2.1378086809664327\n",
      "Epoch:  807 Loss:  2.1335713464702066\n",
      "Epoch:  808 Loss:  2.1293424932568157\n",
      "Epoch:  809 Loss:  2.1251221043560093\n",
      "Epoch:  810 Loss:  2.120910162831469\n",
      "Epoch:  811 Loss:  2.116706651780741\n",
      "Epoch:  812 Loss:  2.1125115543351716\n",
      "Epoch:  813 Loss:  2.108324853659836\n",
      "Epoch:  814 Loss:  2.104146532953472\n",
      "Epoch:  815 Loss:  2.0999765754484145\n",
      "Epoch:  816 Loss:  2.095814964410526\n",
      "Epoch:  817 Loss:  2.0916616831391304\n",
      "Epoch:  818 Loss:  2.087516714966947\n",
      "Epoch:  819 Loss:  2.0833800432600227\n",
      "Epoch:  820 Loss:  2.079251651417665\n",
      "Epoch:  821 Loss:  2.0751315228723777\n",
      "Epoch:  822 Loss:  2.071019641089792\n",
      "Epoch:  823 Loss:  2.066915989568602\n",
      "Epoch:  824 Loss:  2.0628205518404985\n",
      "Epoch:  825 Loss:  2.0587333114701014\n",
      "Epoch:  826 Loss:  2.0546542520548963\n",
      "Epoch:  827 Loss:  2.0505833572251655\n",
      "Epoch:  828 Loss:  2.0465206106439284\n",
      "Epoch:  829 Loss:  2.0424659960068694\n",
      "Epoch:  830 Loss:  2.038419497042276\n",
      "Epoch:  831 Loss:  2.0343810975109737\n",
      "Epoch:  832 Loss:  2.030350781206261\n",
      "Epoch:  833 Loss:  2.026328531953841\n",
      "Epoch:  834 Loss:  2.022314333611765\n",
      "Epoch:  835 Loss:  2.018308170070357\n",
      "Epoch:  836 Loss:  2.0143100252521586\n",
      "Epoch:  837 Loss:  2.0103198831118583\n",
      "Epoch:  838 Loss:  2.00633772763623\n",
      "Epoch:  839 Loss:  2.0023635428440714\n",
      "Epoch:  840 Loss:  1.998397312786133\n",
      "Epoch:  841 Loss:  1.9944390215450607\n",
      "Epoch:  842 Loss:  1.9904886532353308\n",
      "Epoch:  843 Loss:  1.9865461920031822\n",
      "Epoch:  844 Loss:  1.9826116220265597\n",
      "Epoch:  845 Loss:  1.9786849275150449\n",
      "Epoch:  846 Loss:  1.9747660927097952\n",
      "Epoch:  847 Loss:  1.9708551018834823\n",
      "Epoch:  848 Loss:  1.9669519393402264\n",
      "Epoch:  849 Loss:  1.963056589415534\n",
      "Epoch:  850 Loss:  1.959169036476237\n",
      "Epoch:  851 Loss:  1.955289264920429\n",
      "Epoch:  852 Loss:  1.9514172591774017\n",
      "Epoch:  853 Loss:  1.9475530037075837\n",
      "Epoch:  854 Loss:  1.9436964830024783\n",
      "Epoch:  855 Loss:  1.9398476815846026\n",
      "Epoch:  856 Loss:  1.9360065840074219\n",
      "Epoch:  857 Loss:  1.9321731748552922\n",
      "Epoch:  858 Loss:  1.9283474387433952\n",
      "Epoch:  859 Loss:  1.924529360317678\n",
      "Epoch:  860 Loss:  1.9207189242547928\n",
      "Epoch:  861 Loss:  1.916916115262033\n",
      "Epoch:  862 Loss:  1.9131209180772737\n",
      "Epoch:  863 Loss:  1.9093333174689104\n",
      "Epoch:  864 Loss:  1.905553298235798\n",
      "Epoch:  865 Loss:  1.90178084520719\n",
      "Epoch:  866 Loss:  1.8980159432426764\n",
      "Epoch:  867 Loss:  1.894258577232125\n",
      "Epoch:  868 Loss:  1.8905087320956202\n",
      "Epoch:  869 Loss:  1.8867663927834024\n",
      "Epoch:  870 Loss:  1.8830315442758079\n",
      "Epoch:  871 Loss:  1.879304171583209\n",
      "Epoch:  872 Loss:  1.875584259745953\n",
      "Epoch:  873 Loss:  1.8718717938343044\n",
      "Epoch:  874 Loss:  1.8681667589483828\n",
      "Epoch:  875 Loss:  1.8644691402181044\n",
      "Epoch:  876 Loss:  1.8607789228031224\n",
      "Epoch:  877 Loss:  1.8570960918927675\n",
      "Epoch:  878 Loss:  1.853420632705989\n",
      "Epoch:  879 Loss:  1.8497525304912943\n",
      "Epoch:  880 Loss:  1.8460917705266917\n",
      "Epoch:  881 Loss:  1.8424383381196305\n",
      "Epoch:  882 Loss:  1.8387922186069412\n",
      "Epoch:  883 Loss:  1.8351533973547791\n",
      "Epoch:  884 Loss:  1.8315218597585634\n",
      "Epoch:  885 Loss:  1.82789759124292\n",
      "Epoch:  886 Loss:  1.824280577261624\n",
      "Epoch:  887 Loss:  1.8206708032975387\n",
      "Epoch:  888 Loss:  1.8170682548625603\n",
      "Epoch:  889 Loss:  1.8134729174975581\n",
      "Epoch:  890 Loss:  1.8098847767723174\n",
      "Epoch:  891 Loss:  1.8063038182854814\n",
      "Epoch:  892 Loss:  1.8027300276644944\n",
      "Epoch:  893 Loss:  1.7991633905655429\n",
      "Epoch:  894 Loss:  1.7956038926734992\n",
      "Epoch:  895 Loss:  1.7920515197018643\n",
      "Epoch:  896 Loss:  1.7885062573927077\n",
      "Epoch:  897 Loss:  1.7849680915166164\n",
      "Epoch:  898 Loss:  1.7814370078726316\n",
      "Epoch:  899 Loss:  1.7779129922881949\n",
      "Epoch:  900 Loss:  1.7743960306190922\n",
      "Epoch:  901 Loss:  1.7708861087493954\n",
      "Epoch:  902 Loss:  1.767383212591407\n",
      "Epoch:  903 Loss:  1.7638873280856027\n",
      "Epoch:  904 Loss:  1.7603984412005753\n",
      "Epoch:  905 Loss:  1.7569165379329803\n",
      "Epoch:  906 Loss:  1.7534416043074765\n",
      "Epoch:  907 Loss:  1.7499736263766734\n",
      "Epoch:  908 Loss:  1.7465125902210745\n",
      "Epoch:  909 Loss:  1.743058481949019\n",
      "Epoch:  910 Loss:  1.7396112876966294\n",
      "Epoch:  911 Loss:  1.7361709936277538\n",
      "Epoch:  912 Loss:  1.7327375859339142\n",
      "Epoch:  913 Loss:  1.7293110508342446\n",
      "Epoch:  914 Loss:  1.7258913745754425\n",
      "Epoch:  915 Loss:  1.72247854343171\n",
      "Epoch:  916 Loss:  1.7190725437046999\n",
      "Epoch:  917 Loss:  1.7156733617234612\n",
      "Epoch:  918 Loss:  1.7122809838443838\n",
      "Epoch:  919 Loss:  1.7088953964511437\n",
      "Epoch:  920 Loss:  1.70551658595465\n",
      "Epoch:  921 Loss:  1.7021445387929874\n",
      "Epoch:  922 Loss:  1.698779241431366\n",
      "Epoch:  923 Loss:  1.6954206803620642\n",
      "Epoch:  924 Loss:  1.6920688421043748\n",
      "Epoch:  925 Loss:  1.6887237132045518\n",
      "Epoch:  926 Loss:  1.6853852802357565\n",
      "Epoch:  927 Loss:  1.6820535297980046\n",
      "Epoch:  928 Loss:  1.6787284485181098\n",
      "Epoch:  929 Loss:  1.6754100230496323\n",
      "Epoch:  930 Loss:  1.6720982400728264\n",
      "Epoch:  931 Loss:  1.6687930862945848\n",
      "Epoch:  932 Loss:  1.665494548448386\n",
      "Epoch:  933 Loss:  1.6622026132942427\n",
      "Epoch:  934 Loss:  1.6589172676186463\n",
      "Epoch:  935 Loss:  1.6556384982345167\n",
      "Epoch:  936 Loss:  1.6523662919811473\n",
      "Epoch:  937 Loss:  1.6491006357241529\n",
      "Epoch:  938 Loss:  1.6458415163554196\n",
      "Epoch:  939 Loss:  1.6425889207930464\n",
      "Epoch:  940 Loss:  1.6393428359812994\n",
      "Epoch:  941 Loss:  1.6361032488905556\n",
      "Epoch:  942 Loss:  1.6328701465172517\n",
      "Epoch:  943 Loss:  1.6296435158838325\n",
      "Epoch:  944 Loss:  1.626423344038698\n",
      "Epoch:  945 Loss:  1.6232096180561524\n",
      "Epoch:  946 Loss:  1.6200023250363518\n",
      "Epoch:  947 Loss:  1.6168014521052527\n",
      "Epoch:  948 Loss:  1.6136069864145612\n",
      "Epoch:  949 Loss:  1.6104189151416803\n",
      "Epoch:  950 Loss:  1.6072372254896585\n",
      "Epoch:  951 Loss:  1.604061904687141\n",
      "Epoch:  952 Loss:  1.6008929399883145\n",
      "Epoch:  953 Loss:  1.59773031867286\n",
      "Epoch:  954 Loss:  1.5945740280458995\n",
      "Epoch:  955 Loss:  1.5914240554379464\n",
      "Epoch:  956 Loss:  1.5882803882048542\n",
      "Epoch:  957 Loss:  1.5851430137277651\n",
      "Epoch:  958 Loss:  1.582011919413062\n",
      "Epoch:  959 Loss:  1.578887092692315\n",
      "Epoch:  960 Loss:  1.5757685210222332\n",
      "Epoch:  961 Loss:  1.5726561918846136\n",
      "Epoch:  962 Loss:  1.5695500927862904\n",
      "Epoch:  963 Loss:  1.5664502112590875\n",
      "Epoch:  964 Loss:  1.5633565348597647\n",
      "Epoch:  965 Loss:  1.5602690511699713\n",
      "Epoch:  966 Loss:  1.5571877477961944\n",
      "Epoch:  967 Loss:  1.5541126123697102\n",
      "Epoch:  968 Loss:  1.5510436325465342\n",
      "Epoch:  969 Loss:  1.5479807960073717\n",
      "Epoch:  970 Loss:  1.5449240904575678\n",
      "Epoch:  971 Loss:  1.54187350362706\n",
      "Epoch:  972 Loss:  1.5388290232703277\n",
      "Epoch:  973 Loss:  1.5357906371663421\n",
      "Epoch:  974 Loss:  1.5327583331185202\n",
      "Epoch:  975 Loss:  1.5297320989546737\n",
      "Epoch:  976 Loss:  1.5267119225269605\n",
      "Epoch:  977 Loss:  1.5236977917118366\n",
      "Epoch:  978 Loss:  1.5206896944100081\n",
      "Epoch:  979 Loss:  1.517687618546381\n",
      "Epoch:  980 Loss:  1.5146915520700146\n",
      "Epoch:  981 Loss:  1.5117014829540716\n",
      "Epoch:  982 Loss:  1.5087173991957725\n",
      "Epoch:  983 Loss:  1.5057392888163434\n",
      "Epoch:  984 Loss:  1.502767139860973\n",
      "Epoch:  985 Loss:  1.4998009403987607\n",
      "Epoch:  986 Loss:  1.4968406785226704\n",
      "Epoch:  987 Loss:  1.4938863423494844\n",
      "Epoch:  988 Loss:  1.490937920019752\n",
      "Epoch:  989 Loss:  1.4879953996977449\n",
      "Epoch:  990 Loss:  1.4850587695714093\n",
      "Epoch:  991 Loss:  1.4821280178523186\n",
      "Epoch:  992 Loss:  1.4792031327756259\n",
      "Epoch:  993 Loss:  1.4762841026000166\n",
      "Epoch:  994 Loss:  1.4733709156076618\n",
      "Epoch:  995 Loss:  1.4704635601041713\n",
      "Epoch:  996 Loss:  1.4675620244185472\n",
      "Epoch:  997 Loss:  1.4646662969031354\n",
      "Epoch:  998 Loss:  1.4617763659335827\n",
      "Epoch:  999 Loss:  1.4588922199087853\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#training the model\n",
    "model.fit(X_train, Y_train, X_test, Y_test, learning_rate=0.1, epochs = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "Accuracy on test set: 100.0\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test)\n",
    "\n",
    "correct_predictions = np.sum(predictions == test_y_arr)\n",
    "print(correct_predictions)\n",
    "accuracy = (correct_predictions / Y_test.shape[0])*100\n",
    "print(f\"Accuracy on test set: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "Accuracy on training set: 100.0\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_train)\n",
    "\n",
    "correct_predictions = np.sum(predictions == y_arr)\n",
    "print(correct_predictions)\n",
    "accuracy = (correct_predictions / Y_train.shape[0])*100\n",
    "print(f\"Accuracy on training set: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"model_non_linear.npz\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
