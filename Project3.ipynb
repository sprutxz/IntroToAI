{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required libraries imported below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_dangerous(color_choices):\n",
    "    if 3 in color_choices and 1 not in color_choices:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def one_hot_encode(color):\n",
    "    arr = np.zeros(4)    \n",
    "    arr[color-1] = 1\n",
    "    return arr\n",
    "    \n",
    "def generate_wiring_diagram(part): #part signfies the part of the project the data is being generated for (1 or 2)\n",
    "    if part != 1 and part != 2:\n",
    "        print(\"invalid parameter values entered while calling generate_wiring_diagram(part) function, part can either be 1 or 2\")\n",
    "        return\n",
    "    \n",
    "    diagram = np.zeros((20,20,4), dtype=int) # Initializing a 20x20 blank diagram\n",
    "    color_choices = [1, 2, 3, 4] # 1: Red, 2: Blue, 3: Yellow, 4: Green\n",
    "    row_choices = list(range(0,20))\n",
    "    col_choices= list(range(0,20))\n",
    "    dangerous = False\n",
    "    wire_to_cut = 0 # 0: No wire to cut, 1: Red, 2: Blue, 3: Yellow, 4: Green\n",
    "\n",
    "    start_with_row = random.choice([True, False])\n",
    "    \n",
    "    if start_with_row: # True: Start with row, False: Start with column\n",
    "        for i in range(4):\n",
    "            if i%2 == 0:\n",
    "                row = random.choice(row_choices) # Choosing a random row\n",
    "                color = random.choice(color_choices) # Choosing a random color\n",
    "                diagram[row, :] = one_hot_encode(color)\n",
    "                row_choices.remove(row)\n",
    "                color_choices.remove(color)\n",
    "                if not dangerous:\n",
    "                    dangerous = is_dangerous(color_choices) # checking if the diagram is dangerous based on the remaining colors\n",
    "            else:\n",
    "                col = random.choice(col_choices) # Choosing a random column\n",
    "                color = random.choice(color_choices) # Choosing a random color\n",
    "                diagram[:, col] = one_hot_encode(color)\n",
    "                col_choices.remove(col)\n",
    "                color_choices.remove(color)\n",
    "                if i == 1 and not dangerous:\n",
    "                    dangerous = is_dangerous(color_choices) # checking if the diagram is dangerous based on the remaining colors\n",
    "                    \n",
    "            if dangerous and i==3: # If the diagram is dangerous, the third wire placed is the one to cut\n",
    "                wire_to_cut = color\n",
    "                \n",
    "    else:\n",
    "        for i in range(4):\n",
    "            if i%2 == 0:\n",
    "                col = random.choice(col_choices) # Choosing a random column\n",
    "                color = random.choice(color_choices) # Choosing a random color\n",
    "                diagram[col, :] = one_hot_encode(color)\n",
    "                col_choices.remove(col)\n",
    "                color_choices.remove(color)\n",
    "                if not dangerous:\n",
    "                    dangerous = is_dangerous(color_choices) # checking if the diagram is dangerous based on the remaining colors\n",
    "            else:\n",
    "                row = random.choice(row_choices) # Choosing a random row\n",
    "                color = random.choice(color_choices) # Choosing a random color\n",
    "                diagram[:, row] = one_hot_encode(color)\n",
    "                row_choices.remove(row)\n",
    "                color_choices.remove(color)\n",
    "                if i == 1 and not dangerous:\n",
    "                    dangerous = is_dangerous(color_choices) # checking if the diagram is dangerous based on the remaining colors\n",
    "                    \n",
    "            if dangerous and i==3:\n",
    "                wire_to_cut = color # If the diagram is dangerous, the third wire placed is the one to cut\n",
    "\n",
    "    if part == 1: \n",
    "        return diagram, dangerous # If part is 1, return the diagram and whether it is dangerous or not\n",
    "    elif part == 2:\n",
    "        return diagram, wire_to_cut # If part is 2, return the diagram and the wire to cut\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory Class to store dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a class to create our \"dataset\" for model training\n",
    "class Memory:\n",
    "    # Constructor\n",
    "    def __init__(self, max_memory):\n",
    "        self.max_memory = max_memory # maximum amount of samples to remember\n",
    "        self.samples = [] # the samples\n",
    "\n",
    "    # Adding a sample to memory\n",
    "    def add_sample(self, diagram_vector, y):\n",
    "        self.samples.append([diagram_vector, y])\n",
    "        if len(self.samples) > self.max_memory: # Removing the earliest sample if we reach maximum memory\n",
    "            self.samples.pop(0)\n",
    "\n",
    "    # Sampling the samples we have in memory\n",
    "    def sample(self, no_samples): #no_samples is the number of samples you need\n",
    "        if no_samples > len(self.samples):\n",
    "            return random.sample(self.samples, len(self.samples))\n",
    "        else:\n",
    "            return random.sample(self.samples, no_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data generation for part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagrams = 5000 #number of max diagrams to be stored in Memory class\n",
    "memory = Memory(diagrams) #more than diagrams and the oldest diagram stored will be removed\n",
    "for i in range(diagrams):\n",
    "    diagram, y = generate_wiring_diagram(1) #type:ignore\n",
    "    memory.add_sample(diagram, int(y == True))\n",
    "list1 = memory.sample(500)\n",
    "print(list1[0][1])\n",
    "arr1 = (list1[0])[0].flatten()\n",
    "arr2 = (list1[0])[0].reshape(-1,4)\n",
    "\"\"\"print(arr1)\n",
    "print(arr2)\n",
    "for i in arr1:\n",
    "    print(i, end = \"\")\n",
    "print()\n",
    "for i in arr2:\n",
    "    print(i, end = \"\")\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data generation for part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagrams = 5000 #number of max diagrams to be stored in Memory class\n",
    "train_size = 1000 #number of samples to be generated\n",
    "test_size = 1000 #number of samples to be used for testing\n",
    "memory = Memory(diagrams) #more than diagrams and the oldest diagram stored will be removed\n",
    "\n",
    "#generating 5000 diagrams and storing them in memory\n",
    "i = 0\n",
    "while i != diagrams:\n",
    "    diagram, y = generate_wiring_diagram(2) #type:ignore\n",
    "    if y != 0:\n",
    "        memory.add_sample(diagram, y)\n",
    "        i += 1\n",
    "samples = memory.sample(train_size+test_size)\n",
    "\n",
    "train_samples = samples[:train_size]\n",
    "test_samples = samples[train_size:]\n",
    "\n",
    "diagram_arr = []\n",
    "y_arr = []\n",
    "for diagrams, y in train_samples:\n",
    "    diagram_arr.append(diagrams)\n",
    "    y_arr.append(y)\n",
    "    \n",
    "test_diagram_arr = []\n",
    "test_y_arr = []    \n",
    "for test_diagrams, test_y in test_samples:\n",
    "    test_diagram_arr.append(test_diagrams)\n",
    "    test_y_arr.append(test_y)\n",
    "\n",
    "y_arr = np.array(y_arr) #converting the y_arr to a numpy array for easier computation\n",
    "test_y_arr = np.array(test_y_arr) #converting the test_y_arr to a numpy array for easier computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = np.column_stack((np.ones(len(arr2)), arr2))\n",
    "weights = np.random.randn(len(input_features[0]))    \n",
    "alpha = 0.05 # Left at 0.05 for now\n",
    "y = np.empty(len(list1), dtype=int)\n",
    "\n",
    "for x in range(len(y)):\n",
    "    y[x] = list1[x][1]\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def log(x):\n",
    "    return np.log10(x)\n",
    "\n",
    "def costFunction (features, weights, y): #Loss Function\n",
    "    loss = 0\n",
    "    for i in range(len(weights)):\n",
    "        loss += -1 * y[i] * log(sigmoid(np.dot(weights[i], features[i])) - (1-y[i]) * log(sigmoid(1-np.dot(weights[i], features[i]))))\n",
    "    loss /= 1/len(weights-1)\n",
    "    return loss\n",
    "\n",
    "def gradientDescent(features, weights, alpha, y):\n",
    "    for i in range(len(weights)):\n",
    "        gradient = 0\n",
    "        for j in range(len(weights)):\n",
    "            func = sigmoid(np.dot(features[j], weights))\n",
    "            gradient += (func-y[i]) * features[j][i]\n",
    "        weights[i] = weights[i] - alpha *  gradient\n",
    "\n",
    "def trainModel(features, weights, alpha):\n",
    "    epochs = 200\n",
    "    for x in range(epochs):\n",
    "        gradientDescent(features, weights, alpha, y)\n",
    "\n",
    "def testModel(features, weights, alpha, y):\n",
    "    prediction = sigmoid(np.dot(features, weights))\n",
    "    print(prediction)\n",
    "    print(costFunction(features, weights, y))\n",
    "\n",
    "trainModel(input_features, weights, alpha)\n",
    "\n",
    "testModel([1, 0, 1, 0, 0], weights, alpha, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(diagram_arr, y_arr):\n",
    "    flattened_diagrams = np.array([diagram.flatten() for diagram in diagram_arr])\n",
    "    additional_features = np.array([diagram.sum(axis=(0, 1)) for diagram in diagram_arr])\n",
    "    X_combined = np.concatenate((flattened_diagrams, additional_features), axis=1)\n",
    "    X = np.c_[X_combined, np.ones(X_combined.shape[0])]\n",
    "    Y = np.eye(4)[np.subtract(y_arr,1)]\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1605)\n",
      "(1000, 4)\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train = process_data(diagram_arr, y_arr)\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2 implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the softmax function\n",
    "def softmax(x): \n",
    "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return e_x / e_x.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.epsilon = 1e-10\n",
    "        \n",
    "    def fit(self, X, Y, learning_rate = 0.001, epochs = 10):\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            Z = np.dot(X, self.W) + self.b\n",
    "            A = softmax(Z)\n",
    "            \n",
    "            loss = -np.mean(np.sum(Y * np.log(A + self.epsilon), axis=1)) #calculating the loss\n",
    "            \n",
    "            #calculating the gradients\n",
    "            dW = (1/Y.shape[0]) * np.dot(X.T, (A-Y))\n",
    "            db = (1/Y.shape[0]) * np.sum(A-Y, axis=0)\n",
    "            \n",
    "            #updating the weights and biases\n",
    "            self.W -= learning_rate * dW\n",
    "            self.b -= learning_rate * db\n",
    "            \n",
    "            if i%100 == 0:\n",
    "                print(\"Epoch: \", epoch, \"Loss: \", loss)\n",
    "                \n",
    "    def predict(self, X): #prediction function\n",
    "        Z = np.dot(X, self.W) + self.b\n",
    "        probs = softmax(Z)\n",
    "        return np.argmax(probs, axis=1) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Loss:  4.459261183298905\n",
      "Epoch:  1 Loss:  3.5348059403498517\n",
      "Epoch:  2 Loss:  3.5274122249899635\n",
      "Epoch:  3 Loss:  2.92514322995158\n",
      "Epoch:  4 Loss:  3.080244928030408\n",
      "Epoch:  5 Loss:  2.6911312635074593\n",
      "Epoch:  6 Loss:  2.862208365295396\n",
      "Epoch:  7 Loss:  2.7638104839697046\n",
      "Epoch:  8 Loss:  2.8966338893791916\n",
      "Epoch:  9 Loss:  2.9111502938313008\n",
      "Epoch:  10 Loss:  2.9978259710586586\n",
      "Epoch:  11 Loss:  2.9518273045835577\n",
      "Epoch:  12 Loss:  3.0189203335725296\n",
      "Epoch:  13 Loss:  2.9638830619287404\n",
      "Epoch:  14 Loss:  3.0203088796508886\n",
      "Epoch:  15 Loss:  2.956098296195824\n",
      "Epoch:  16 Loss:  3.0157256768922998\n",
      "Epoch:  17 Loss:  2.955503247020028\n",
      "Epoch:  18 Loss:  3.010967592154519\n",
      "Epoch:  19 Loss:  2.9466713290977014\n",
      "Epoch:  20 Loss:  3.0058097871960676\n",
      "Epoch:  21 Loss:  2.944229001582631\n",
      "Epoch:  22 Loss:  3.0004726729434537\n",
      "Epoch:  23 Loss:  2.9357676730211937\n",
      "Epoch:  24 Loss:  2.99536543011279\n",
      "Epoch:  25 Loss:  2.9323189359248714\n",
      "Epoch:  26 Loss:  2.989765224798751\n",
      "Epoch:  27 Loss:  2.924202055110352\n",
      "Epoch:  28 Loss:  2.9846922075056175\n",
      "Epoch:  29 Loss:  2.9201027099908607\n",
      "Epoch:  30 Loss:  2.97893272587178\n",
      "Epoch:  31 Loss:  2.9122579580222054\n",
      "Epoch:  32 Loss:  2.9738740593699053\n",
      "Epoch:  33 Loss:  2.907716010444947\n",
      "Epoch:  34 Loss:  2.9680062223502053\n",
      "Epoch:  35 Loss:  2.9000773154657162\n",
      "Epoch:  36 Loss:  2.962950054651653\n",
      "Epoch:  37 Loss:  2.895223655778051\n",
      "Epoch:  38 Loss:  2.9569997621974116\n",
      "Epoch:  39 Loss:  2.887737283640213\n",
      "Epoch:  40 Loss:  2.951940565509283\n",
      "Epoch:  41 Loss:  2.882658686027301\n",
      "Epoch:  42 Loss:  2.9459204197592292\n",
      "Epoch:  43 Loss:  2.8752823461096453\n",
      "Epoch:  44 Loss:  2.940857241885329\n",
      "Epoch:  45 Loss:  2.8700389696762976\n",
      "Epoch:  46 Loss:  2.9347721711211916\n",
      "Epoch:  47 Loss:  2.862739444886015\n",
      "Epoch:  48 Loss:  2.929707441640625\n",
      "Epoch:  49 Loss:  2.8573747720902563\n",
      "Epoch:  50 Loss:  2.9235575222233705\n",
      "Epoch:  51 Loss:  2.850125561499688\n",
      "Epoch:  52 Loss:  2.9184963615131165\n",
      "Epoch:  53 Loss:  2.844672354868735\n",
      "Epoch:  54 Loss:  2.9122782245766614\n",
      "Epoch:  55 Loss:  2.837451682239279\n",
      "Epoch:  56 Loss:  2.90722814042361\n",
      "Epoch:  57 Loss:  2.831935725154176\n",
      "Epoch:  58 Loss:  2.900935598711067\n",
      "Epoch:  59 Loss:  2.824724919374051\n",
      "Epoch:  60 Loss:  2.895906483799951\n",
      "Epoch:  61 Loss:  2.8191674698146927\n",
      "Epoch:  62 Loss:  2.889530683015684\n",
      "Epoch:  63 Loss:  2.8119496260693593\n",
      "Epoch:  64 Loss:  2.8845350660689366\n",
      "Epoch:  65 Loss:  2.8063690970655553\n",
      "Epoch:  66 Loss:  2.8780643058488495\n",
      "Epoch:  67 Loss:  2.799127907897492\n",
      "Epoch:  68 Loss:  2.8731178445126235\n",
      "Epoch:  69 Loss:  2.7935410688724285\n",
      "Epoch:  70 Loss:  2.8665371308424183\n",
      "Epoch:  71 Loss:  2.7862597132369995\n",
      "Epoch:  72 Loss:  2.861659365376403\n",
      "Epoch:  73 Loss:  2.7806825817161194\n",
      "Epoch:  74 Loss:  2.8549497089654285\n",
      "Epoch:  75 Loss:  2.7733425562953826\n",
      "Epoch:  76 Loss:  2.8501651253251095\n",
      "Epoch:  77 Loss:  2.767791070315557\n",
      "Epoch:  78 Loss:  2.8433025718158644\n",
      "Epoch:  79 Loss:  2.760370831320425\n",
      "Epoch:  80 Loss:  2.838642053858457\n",
      "Epoch:  81 Loss:  2.7548613316767008\n",
      "Epoch:  82 Loss:  2.8315964154857656\n",
      "Epoch:  83 Loss:  2.7473345804165716\n",
      "Epoch:  84 Loss:  2.8270992008557565\n",
      "Epoch:  85 Loss:  2.741884068960979\n",
      "Epoch:  86 Loss:  2.819832456066522\n",
      "Epoch:  87 Loss:  2.734217452796066\n",
      "Epoch:  88 Loss:  2.8155487483870956\n",
      "Epoch:  89 Loss:  2.7288435091068712\n",
      "Epoch:  90 Loss:  2.8080130940653962\n",
      "Epoch:  91 Loss:  2.7209934110663396\n",
      "Epoch:  92 Loss:  2.8040075184673707\n",
      "Epoch:  93 Loss:  2.715713520971356\n",
      "Epoch:  94 Loss:  2.7961431169273854\n",
      "Epoch:  95 Loss:  2.7076214665750746\n",
      "Epoch:  96 Loss:  2.792499213229371\n",
      "Epoch:  97 Loss:  2.7024513107091352\n",
      "Epoch:  98 Loss:  2.784231805681974\n",
      "Epoch:  99 Loss:  2.6940373314119075\n",
      "Epoch:  100 Loss:  2.7810576692811355\n",
      "Epoch:  101 Loss:  2.688987265687671\n",
      "Epoch:  102 Loss:  2.772296478128694\n",
      "Epoch:  103 Loss:  2.68014037612775\n",
      "Epoch:  104 Loss:  2.769731321677423\n",
      "Epoch:  105 Loss:  2.6752089013817826\n",
      "Epoch:  106 Loss:  2.7603680853693815\n",
      "Epoch:  107 Loss:  2.665773877683599\n",
      "Epoch:  108 Loss:  2.7585885302747464\n",
      "Epoch:  109 Loss:  2.6609364695868876\n",
      "Epoch:  110 Loss:  2.7484990784484515\n",
      "Epoch:  111 Loss:  2.650696979851891\n",
      "Epoch:  112 Loss:  2.7477216114898573\n",
      "Epoch:  113 Loss:  2.6458887958202233\n",
      "Epoch:  114 Loss:  2.736771754478549\n",
      "Epoch:  115 Loss:  2.6345502202402944\n",
      "Epoch:  116 Loss:  2.7372426872078415\n",
      "Epoch:  117 Loss:  2.6296436071416855\n",
      "Epoch:  118 Loss:  2.725299256650397\n",
      "Epoch:  119 Loss:  2.6168279084922537\n",
      "Epoch:  120 Loss:  2.727254760161627\n",
      "Epoch:  121 Loss:  2.6116141216292905\n",
      "Epoch:  122 Loss:  2.7141984318762082\n",
      "Epoch:  123 Loss:  2.596898281438428\n",
      "Epoch:  124 Loss:  2.7177693922560797\n",
      "Epoch:  125 Loss:  2.5911025155248546\n",
      "Epoch:  126 Loss:  2.703498955442691\n",
      "Epoch:  127 Loss:  2.5741579585856416\n",
      "Epoch:  128 Loss:  2.708552903481171\n",
      "Epoch:  129 Loss:  2.56754282793707\n",
      "Epoch:  130 Loss:  2.6929756687132063\n",
      "Epoch:  131 Loss:  2.5484255054230993\n",
      "Epoch:  132 Loss:  2.69898222283667\n",
      "Epoch:  133 Loss:  2.541022921432347\n",
      "Epoch:  134 Loss:  2.682033733632084\n",
      "Epoch:  135 Loss:  2.5205202866913448\n",
      "Epoch:  136 Loss:  2.6881930000657097\n",
      "Epoch:  137 Loss:  2.5128791386383287\n",
      "Epoch:  138 Loss:  2.6699788496386523\n",
      "Epoch:  139 Loss:  2.492526518106293\n",
      "Epoch:  140 Loss:  2.6757465426631506\n",
      "Epoch:  141 Loss:  2.4856271451189604\n",
      "Epoch:  142 Loss:  2.656727129370036\n",
      "Epoch:  143 Loss:  2.4670127965835826\n",
      "Epoch:  144 Loss:  2.662243321607898\n",
      "Epoch:  145 Loss:  2.46165064102939\n",
      "Epoch:  146 Loss:  2.643085843178579\n",
      "Epoch:  147 Loss:  2.445447355015207\n",
      "Epoch:  148 Loss:  2.6488606873715796\n",
      "Epoch:  149 Loss:  2.4416355700656904\n",
      "Epoch:  150 Loss:  2.629963170974519\n",
      "Epoch:  151 Loss:  2.42741913037816\n",
      "Epoch:  152 Loss:  2.636268153351473\n",
      "Epoch:  153 Loss:  2.424537589508081\n",
      "Epoch:  154 Loss:  2.6175888947128767\n",
      "Epoch:  155 Loss:  2.4114912720731896\n",
      "Epoch:  156 Loss:  2.6243583434553113\n",
      "Epoch:  157 Loss:  2.408893294892451\n",
      "Epoch:  158 Loss:  2.605682801987424\n",
      "Epoch:  159 Loss:  2.396442966467242\n",
      "Epoch:  160 Loss:  2.612761463453418\n",
      "Epoch:  161 Loss:  2.3937782136776846\n",
      "Epoch:  162 Loss:  2.5939416736349834\n",
      "Epoch:  163 Loss:  2.3816922065828803\n",
      "Epoch:  164 Loss:  2.601232114195545\n",
      "Epoch:  165 Loss:  2.3788537189061416\n",
      "Epoch:  166 Loss:  2.582220437656565\n",
      "Epoch:  167 Loss:  2.367084013991307\n",
      "Epoch:  168 Loss:  2.589684893370316\n",
      "Epoch:  169 Loss:  2.3640659059877454\n",
      "Epoch:  170 Loss:  2.570484599990438\n",
      "Epoch:  171 Loss:  2.352614860300815\n",
      "Epoch:  172 Loss:  2.5781101074165425\n",
      "Epoch:  173 Loss:  2.349430534257301\n",
      "Epoch:  174 Loss:  2.558737033671573\n",
      "Epoch:  175 Loss:  2.3382995124410018\n",
      "Epoch:  176 Loss:  2.5665144103012416\n",
      "Epoch:  177 Loss:  2.334958344169761\n",
      "Epoch:  178 Loss:  2.5469833569452613\n",
      "Epoch:  179 Loss:  2.3241408793666114\n",
      "Epoch:  180 Loss:  2.554901452425527\n",
      "Epoch:  181 Loss:  2.3206483840642678\n",
      "Epoch:  182 Loss:  2.5352250040971795\n",
      "Epoch:  183 Loss:  2.3101344456283486\n",
      "Epoch:  184 Loss:  2.5432711256577547\n",
      "Epoch:  185 Loss:  2.306495704681999\n",
      "Epoch:  186 Loss:  2.5234611488491776\n",
      "Epoch:  187 Loss:  2.2962746728523733\n",
      "Epoch:  188 Loss:  2.5316221800273673\n",
      "Epoch:  189 Loss:  2.2924957433440283\n",
      "Epoch:  190 Loss:  2.511690672874659\n",
      "Epoch:  191 Loss:  2.28255705936313\n",
      "Epoch:  192 Loss:  2.519953509349896\n",
      "Epoch:  193 Loss:  2.278644959777673\n",
      "Epoch:  194 Loss:  2.4999127148095495\n",
      "Epoch:  195 Loss:  2.268977955634261\n",
      "Epoch:  196 Loss:  2.5082642740440866\n",
      "Epoch:  197 Loss:  2.2649403690553265\n",
      "Epoch:  198 Loss:  2.4881265757261644\n",
      "Epoch:  199 Loss:  2.2555341274830236\n",
      "Epoch:  200 Loss:  2.4965537386138847\n",
      "Epoch:  201 Loss:  2.2513792313495027\n",
      "Epoch:  202 Loss:  2.4763315904055148\n",
      "Epoch:  203 Loss:  2.2422225903537196\n",
      "Epoch:  204 Loss:  2.4848211873442363\n",
      "Epoch:  205 Loss:  2.237958991837071\n",
      "Epoch:  206 Loss:  2.464527094946379\n",
      "Epoch:  207 Loss:  2.229040596549815\n",
      "Epoch:  208 Loss:  2.473065919878079\n",
      "Epoch:  209 Loss:  2.224677293857076\n",
      "Epoch:  210 Loss:  2.45271243769093\n",
      "Epoch:  211 Loss:  2.2159856383794256\n",
      "Epoch:  212 Loss:  2.4612872635983782\n",
      "Epoch:  213 Loss:  2.211531979907666\n",
      "Epoch:  214 Loss:  2.4408869894759673\n",
      "Epoch:  215 Loss:  2.2030554347186198\n",
      "Epoch:  216 Loss:  2.4494845782937595\n",
      "Epoch:  217 Loss:  2.19852107876866\n",
      "Epoch:  218 Loss:  2.42905014611616\n",
      "Epoch:  219 Loss:  2.190247912218994\n",
      "Epoch:  220 Loss:  2.437657255303754\n",
      "Epoch:  221 Loss:  2.1856427923736343\n",
      "Epoch:  222 Loss:  2.4172013283902496\n",
      "Epoch:  223 Loss:  2.1775611900392264\n",
      "Epoch:  224 Loss:  2.425804716234484\n",
      "Epoch:  225 Loss:  2.1728954867519215\n",
      "Epoch:  226 Loss:  2.4053399825288317\n",
      "Epoch:  227 Loss:  2.1649935684980943\n",
      "Epoch:  228 Loss:  2.413926412490687\n",
      "Epoch:  229 Loss:  2.1602776855545893\n",
      "Epoch:  230 Loss:  2.39346558116051\n",
      "Epoch:  231 Loss:  2.1525435197608522\n",
      "Epoch:  232 Loss:  2.402021825060458\n",
      "Epoch:  233 Loss:  2.1477880645723735\n",
      "Epoch:  234 Loss:  2.3815776241096827\n",
      "Epoch:  235 Loss:  2.1402096795803502\n",
      "Epoch:  236 Loss:  2.390090464131332\n",
      "Epoch:  237 Loss:  2.1354254467611953\n",
      "Epoch:  238 Loss:  2.369675638841535\n",
      "Epoch:  239 Loss:  2.127990839887841\n",
      "Epoch:  240 Loss:  2.3781318685069874\n",
      "Epoch:  241 Loss:  2.1231887977030572\n",
      "Epoch:  242 Loss:  2.357759180587237\n",
      "Epoch:  243 Loss:  2.115885942149597\n",
      "Epoch:  244 Loss:  2.366145604893809\n",
      "Epoch:  245 Loss:  2.111077221392347\n",
      "Epoch:  246 Loss:  2.345827832186088\n",
      "Epoch:  247 Loss:  2.103894071339455\n",
      "Epoch:  248 Loss:  2.3541312670885715\n",
      "Epoch:  249 Loss:  2.0990899561822713\n",
      "Epoch:  250 Loss:  2.333881203644389\n",
      "Epoch:  251 Loss:  2.0920144503713294\n",
      "Epoch:  252 Loss:  2.342088475074117\n",
      "Epoch:  253 Loss:  2.087226370743987\n",
      "Epoch:  254 Loss:  2.321918931405555\n",
      "Epoch:  255 Loss:  2.080246434870337\n",
      "Epoch:  256 Loss:  2.330016874034065\n",
      "Epoch:  257 Loss:  2.0754859599241904\n",
      "Epoch:  258 Loss:  2.3099406773356237\n",
      "Epoch:  259 Loss:  2.0685895081925834\n",
      "Epoch:  260 Loss:  2.3179161333069787\n",
      "Epoch:  261 Loss:  2.0638683404074154\n",
      "Epoch:  262 Loss:  2.2979461274373234\n",
      "Epoch:  263 Loss:  2.0570432766200217\n",
      "Epoch:  264 Loss:  2.30578594530412\n",
      "Epoch:  265 Loss:  2.0523732460973205\n",
      "Epoch:  266 Loss:  2.2859349903088257\n",
      "Epoch:  267 Loss:  2.045607464665225\n",
      "Epoch:  268 Loss:  2.2936260244159485\n",
      "Epoch:  269 Loss:  2.0410005231346977\n",
      "Epoch:  270 Loss:  2.2739069953649835\n",
      "Epoch:  271 Loss:  2.0342819104282155\n",
      "Epoch:  272 Loss:  2.2814361059345494\n",
      "Epoch:  273 Loss:  2.029750124474569\n",
      "Epoch:  274 Loss:  2.261861890843749\n",
      "Epoch:  275 Loss:  2.023066560951474\n",
      "Epoch:  276 Loss:  2.269215945020082\n",
      "Epoch:  277 Loss:  2.0186221039416568\n",
      "Epoch:  278 Loss:  2.2497994416204325\n",
      "Epoch:  279 Loss:  2.011961467519573\n",
      "Epoch:  280 Loss:  2.2569653157406027\n",
      "Epoch:  281 Loss:  2.0076166096814565\n",
      "Epoch:  282 Loss:  2.2377194268563194\n",
      "Epoch:  283 Loss:  2.0009667808482794\n",
      "Epoch:  284 Loss:  2.2446840102160905\n",
      "Epoch:  285 Loss:  1.9967338769167766\n",
      "Epoch:  286 Loss:  2.22562163750976\n",
      "Epoch:  287 Loss:  1.990082746102436\n",
      "Epoch:  288 Loss:  2.232371837898454\n",
      "Epoch:  289 Loss:  1.985974219908704\n",
      "Epoch:  290 Loss:  2.2135058737392836\n",
      "Epoch:  291 Loss:  1.979309697672566\n",
      "Epoch:  292 Loss:  2.220028625020638\n",
      "Epoch:  293 Loss:  1.9753380230074291\n",
      "Epoch:  294 Loss:  2.2013719422299687\n",
      "Epoch:  295 Loss:  1.9686480536281186\n",
      "Epoch:  296 Loss:  2.2076542142492066\n",
      "Epoch:  297 Loss:  1.9648257306601877\n",
      "Epoch:  298 Loss:  2.189219653475986\n",
      "Epoch:  299 Loss:  1.9580983097479283\n",
      "Epoch:  300 Loss:  2.195248464576773\n",
      "Epoch:  301 Loss:  1.9544378362207866\n",
      "Epoch:  302 Loss:  2.177048819054136\n",
      "Epoch:  303 Loss:  1.9476610330076183\n",
      "Epoch:  304 Loss:  2.18281125149345\n",
      "Epoch:  305 Loss:  1.9441748693767054\n",
      "Epoch:  306 Loss:  2.164859248924262\n",
      "Epoch:  307 Loss:  1.937336854374185\n",
      "Epoch:  308 Loss:  2.1703424674785157\n",
      "Epoch:  309 Loss:  1.9340373819753054\n",
      "Epoch:  310 Loss:  2.152650748795701\n",
      "Epoch:  311 Loss:  1.9271264607261382\n",
      "Epoch:  312 Loss:  2.157842022859337\n",
      "Epoch:  313 Loss:  1.9240259319879072\n",
      "Epoch:  314 Loss:  2.1404231176004522\n",
      "Epoch:  315 Loss:  1.9170305856714662\n",
      "Epoch:  316 Loss:  2.145309847088254\n",
      "Epoch:  317 Loss:  1.914141065299515\n",
      "Epoch:  318 Loss:  2.128176145118532\n",
      "Epoch:  319 Loss:  1.9070499989853094\n",
      "Epoch:  320 Loss:  2.132745890496663\n",
      "Epoch:  321 Loss:  1.9043832949485198\n",
      "Epoch:  322 Loss:  2.1159096098042776\n",
      "Epoch:  323 Loss:  1.8971854943217137\n",
      "Epoch:  324 Loss:  2.120150126593832\n",
      "Epoch:  325 Loss:  1.8947530773658214\n",
      "Epoch:  326 Loss:  2.10362327686975\n",
      "Epoch:  327 Loss:  1.8874378747741798\n",
      "Epoch:  328 Loss:  2.1075225549898304\n",
      "Epoch:  329 Loss:  1.8852507850703772\n",
      "Epoch:  330 Loss:  2.091316896687962\n",
      "Epoch:  331 Loss:  1.8778079357600594\n",
      "Epoch:  332 Loss:  2.0948632050367713\n",
      "Epoch:  333 Loss:  1.8758766751679843\n",
      "Epoch:  334 Loss:  2.078990203589955\n",
      "Epoch:  335 Loss:  1.8682964445824402\n",
      "Epoch:  336 Loss:  2.082172140300497\n",
      "Epoch:  337 Loss:  1.8666308528668967\n",
      "Epoch:  338 Loss:  2.0666429151420216\n",
      "Epoch:  339 Loss:  1.8589041158731594\n",
      "Epoch:  340 Loss:  2.0694494639968393\n",
      "Epoch:  341 Loss:  1.8575132290641767\n",
      "Epoch:  342 Loss:  2.0542747320065766\n",
      "Epoch:  343 Loss:  1.8496315819389624\n",
      "Epoch:  344 Loss:  2.056695325554081\n",
      "Epoch:  345 Loss:  1.8485234708664253\n",
      "Epoch:  346 Loss:  2.041885338510229\n",
      "Epoch:  347 Loss:  1.840479356809422\n",
      "Epoch:  348 Loss:  2.043909928494069\n",
      "Epoch:  349 Loss:  1.8396609436833915\n",
      "Epoch:  350 Loss:  2.0294744040682073\n",
      "Epoch:  351 Loss:  1.8314477925154988\n",
      "Epoch:  352 Loss:  2.0310935398614354\n",
      "Epoch:  353 Loss:  1.8309246432689648\n",
      "Epoch:  354 Loss:  2.0170415856440065\n",
      "Epoch:  355 Loss:  1.8225370258017362\n",
      "Epoch:  356 Loss:  2.018246501470667\n",
      "Epoch:  357 Loss:  1.8223131157801469\n",
      "Epoch:  358 Loss:  2.0045865314580102\n",
      "Epoch:  359 Loss:  1.8137469130872843\n",
      "Epoch:  360 Loss:  2.0053692432842958\n",
      "Epoch:  361 Loss:  1.8138243635812612\n",
      "Epoch:  362 Loss:  1.9921088861972502\n",
      "Epoch:  363 Loss:  1.8050769510327695\n",
      "Epoch:  364 Loss:  1.9924622992771102\n",
      "Epoch:  365 Loss:  1.8054557341450026\n",
      "Epoch:  366 Loss:  1.9796082980182224\n",
      "Epoch:  367 Loss:  1.7965261795429537\n",
      "Epoch:  368 Loss:  1.9795263261763625\n",
      "Epoch:  369 Loss:  1.7972037890122765\n",
      "Epoch:  370 Loss:  1.967084427670678\n",
      "Epoch:  371 Loss:  1.7880930634440124\n",
      "Epoch:  372 Loss:  1.9665621254848666\n",
      "Epoch:  373 Loss:  1.7890641494019117\n",
      "Epoch:  374 Loss:  1.9545369600953597\n",
      "Epoch:  375 Loss:  1.7797753484419814\n",
      "Epoch:  376 Loss:  1.9535706691758965\n",
      "Epoch:  377 Loss:  1.7810313147689354\n",
      "Epoch:  378 Loss:  1.941965618846509\n",
      "Epoch:  379 Loss:  1.7715698863432459\n",
      "Epoch:  380 Loss:  1.9405531293699405\n",
      "Epoch:  381 Loss:  1.7730984504958691\n",
      "Epoch:  382 Loss:  1.9293701836413661\n",
      "Epoch:  383 Loss:  1.7634724239891366\n",
      "Epoch:  384 Loss:  1.9275109121230063\n",
      "Epoch:  385 Loss:  1.7652571411218705\n",
      "Epoch:  386 Loss:  1.9167505112064418\n",
      "Epoch:  387 Loss:  1.7554773500778798\n",
      "Epoch:  388 Loss:  1.9144456951169593\n",
      "Epoch:  389 Loss:  1.7574971063120097\n",
      "Epoch:  390 Loss:  1.904106559329698\n",
      "Epoch:  391 Loss:  1.7475773942734685\n",
      "Epoch:  392 Loss:  1.9013594684650588\n",
      "Epoch:  393 Loss:  1.749805878497884\n",
      "Epoch:  394 Loss:  1.89143841356685\n",
      "Epoch:  395 Loss:  1.739763274121057\n",
      "Epoch:  396 Loss:  1.8882545769212442\n",
      "Epoch:  397 Loss:  1.7421684442849414\n",
      "Epoch:  398 Loss:  1.8787463153046202\n",
      "Epoch:  399 Loss:  1.7320232878875836\n",
      "Epoch:  400 Loss:  1.8751337603818024\n",
      "Epoch:  401 Loss:  1.734566857001958\n",
      "Epoch:  402 Loss:  1.8660306887550329\n",
      "Epoch:  403 Loss:  1.724342856345193\n",
      "Epoch:  404 Loss:  1.8620001875521175\n",
      "Epoch:  405 Loss:  1.7269798360164872\n",
      "Epoch:  406 Loss:  1.8532921628467587\n",
      "Epoch:  407 Loss:  1.7167040247965533\n",
      "Epoch:  408 Loss:  1.8488574749053337\n",
      "Epoch:  409 Loss:  1.7193823806220712\n",
      "Epoch:  410 Loss:  1.8405315818546943\n",
      "Epoch:  411 Loss:  1.7090849496090372\n",
      "Epoch:  412 Loss:  1.8357096795813876\n",
      "Epoch:  413 Loss:  1.711745443302063\n",
      "Epoch:  414 Loss:  1.8277499960672772\n",
      "Epoch:  415 Loss:  1.7014594124966402\n",
      "Epoch:  416 Loss:  1.8225612509034674\n",
      "Epoch:  417 Loss:  1.7040357293706168\n",
      "Epoch:  418 Loss:  1.8149486212116994\n",
      "Epoch:  419 Loss:  1.69379643162226\n",
      "Epoch:  420 Loss:  1.8094169214325635\n",
      "Epoch:  421 Loss:  1.6962157165130882\n",
      "Epoch:  422 Loss:  1.8021287535977648\n",
      "Epoch:  423 Loss:  1.68606007077582\n",
      "Epoch:  424 Loss:  1.7962815163878825\n",
      "Epoch:  425 Loss:  1.6882440153066172\n",
      "Epoch:  426 Loss:  1.7892916285369846\n",
      "Epoch:  427 Loss:  1.678209583067516\n",
      "Epoch:  428 Loss:  1.7831596622780634\n",
      "Epoch:  429 Loss:  1.680076213187064\n",
      "Epoch:  430 Loss:  1.7764382148245337\n",
      "Epoch:  431 Loss:  1.6702000556393812\n",
      "Epoch:  432 Loss:  1.7700553851506913\n",
      "Epoch:  433 Loss:  1.6716663470461512\n",
      "Epoch:  434 Loss:  1.7635689506424106\n",
      "Epoch:  435 Loss:  1.6619837321443707\n",
      "Epoch:  436 Loss:  1.7569716097332564\n",
      "Epoch:  437 Loss:  1.6629691157958189\n",
      "Epoch:  438 Loss:  1.7506834481382325\n",
      "Epoch:  439 Loss:  1.6535121586421042\n",
      "Epoch:  440 Loss:  1.7439096051047782\n",
      "Epoch:  441 Loss:  1.6539428536094014\n",
      "Epoch:  442 Loss:  1.7377802242143052\n",
      "Epoch:  443 Loss:  1.6447392016158768\n",
      "Epoch:  444 Loss:  1.7308684677349693\n",
      "Epoch:  445 Loss:  1.6445531231084047\n",
      "Epoch:  446 Loss:  1.7248565468129173\n",
      "Epoch:  447 Loss:  1.635624807182076\n",
      "Epoch:  448 Loss:  1.7178447771965248\n",
      "Epoch:  449 Loss:  1.6347765623076287\n",
      "Epoch:  450 Loss:  1.711908504147348\n",
      "Epoch:  451 Loss:  1.6261391170920807\n",
      "Epoch:  452 Loss:  1.7048325812808784\n",
      "Epoch:  453 Loss:  1.6246043741551452\n",
      "Epoch:  454 Loss:  1.6989313876960759\n",
      "Epoch:  455 Loss:  1.616266283989019\n",
      "Epoch:  456 Loss:  1.691823837426607\n",
      "Epoch:  457 Loss:  1.6140446728124018\n",
      "Epoch:  458 Loss:  1.6859204107164596\n",
      "Epoch:  459 Loss:  1.6060071413778905\n",
      "Epoch:  460 Loss:  1.6788093374936455\n",
      "Epoch:  461 Loss:  1.6031229132384337\n",
      "Epoch:  462 Loss:  1.6728716654296896\n",
      "Epoch:  463 Loss:  1.5953799135364544\n",
      "Epoch:  464 Loss:  1.6657799855421271\n",
      "Epoch:  465 Loss:  1.5918799198731788\n",
      "Epoch:  466 Loss:  1.6597830919301035\n",
      "Epoch:  467 Loss:  1.5844184848877838\n",
      "Epoch:  468 Loss:  1.6527281404220884\n",
      "Epoch:  469 Loss:  1.5803675845164844\n",
      "Epoch:  470 Loss:  1.6466551585430176\n",
      "Epoch:  471 Loss:  1.57316834953053\n",
      "Epoch:  472 Loss:  1.6396486614814694\n",
      "Epoch:  473 Loss:  1.5686429641762394\n",
      "Epoch:  474 Loss:  1.6334909998288063\n",
      "Epoch:  475 Loss:  1.5616810318504801\n",
      "Epoch:  476 Loss:  1.626539368466656\n",
      "Epoch:  477 Loss:  1.5567620108509173\n",
      "Epoch:  478 Loss:  1.6202959318513235\n",
      "Epoch:  479 Loss:  1.5500082205436938\n",
      "Epoch:  480 Loss:  1.6134008308570964\n",
      "Epoch:  481 Loss:  1.5447742669745872\n",
      "Epoch:  482 Loss:  1.6070764921015352\n",
      "Epoch:  483 Loss:  1.5381968732977966\n",
      "Epoch:  484 Loss:  1.6002356393894657\n",
      "Epoch:  485 Loss:  1.5327194994126103\n",
      "Epoch:  486 Loss:  1.5938393172338385\n",
      "Epoch:  487 Loss:  1.5262861242031363\n",
      "Epoch:  488 Loss:  1.5870474622357036\n",
      "Epoch:  489 Loss:  1.5206265880009655\n",
      "Epoch:  490 Loss:  1.580590192688164\n",
      "Epoch:  491 Loss:  1.5143061797931168\n",
      "Epoch:  492 Loss:  1.5738401882101503\n",
      "Epoch:  493 Loss:  1.5085143306339888\n",
      "Epoch:  494 Loss:  1.5673334900848246\n",
      "Epoch:  495 Loss:  1.5022788228929926\n",
      "Epoch:  496 Loss:  1.560617340871479\n",
      "Epoch:  497 Loss:  1.4963934296849462\n",
      "Epoch:  498 Loss:  1.554072032053923\n",
      "Epoch:  499 Loss:  1.490218858928984\n",
      "Epoch:  500 Loss:  1.5473817976453046\n",
      "Epoch:  501 Loss:  1.4842688673156492\n",
      "Epoch:  502 Loss:  1.5408072825974817\n",
      "Epoch:  503 Loss:  1.478135857425156\n",
      "Epoch:  504 Loss:  1.5341357432170035\n",
      "Epoch:  505 Loss:  1.4721420740043327\n",
      "Epoch:  506 Loss:  1.527539702447915\n",
      "Epoch:  507 Loss:  1.4660357450970822\n",
      "Epoch:  508 Loss:  1.5208807522450618\n",
      "Epoch:  509 Loss:  1.4600125850397605\n",
      "Epoch:  510 Loss:  1.51426912591473\n",
      "Epoch:  511 Loss:  1.453922048154639\n",
      "Epoch:  512 Loss:  1.5076179146521917\n",
      "Epoch:  513 Loss:  1.447879129763437\n",
      "Epoch:  514 Loss:  1.500995070916094\n",
      "Epoch:  515 Loss:  1.44179676196136\n",
      "Epoch:  516 Loss:  1.4943479541399347\n",
      "Epoch:  517 Loss:  1.4357402472367427\n",
      "Epoch:  518 Loss:  1.4877169506065215\n",
      "Epoch:  519 Loss:  1.4296609179584046\n",
      "Epoch:  520 Loss:  1.4810713225795589\n",
      "Epoch:  521 Loss:  1.4235945735798885\n",
      "Epoch:  522 Loss:  1.474434193483546\n",
      "Epoch:  523 Loss:  1.4175149396609386\n",
      "Epoch:  524 Loss:  1.4677882706929326\n",
      "Epoch:  525 Loss:  1.411440935415154\n",
      "Epoch:  526 Loss:  1.4611462960836497\n",
      "Epoch:  527 Loss:  1.405358864874214\n",
      "Epoch:  528 Loss:  1.454498901105808\n",
      "Epoch:  529 Loss:  1.3992783472738406\n",
      "Epoch:  530 Loss:  1.447852834394144\n",
      "Epoch:  531 Loss:  1.3931924866387217\n",
      "Epoch:  532 Loss:  1.441203209235643\n",
      "Epoch:  533 Loss:  1.387105972749029\n",
      "Epoch:  534 Loss:  1.4345534546598069\n",
      "Epoch:  535 Loss:  1.3810154445035032\n",
      "Epoch:  536 Loss:  1.427901115249061\n",
      "Epoch:  537 Loss:  1.3749230802486332\n",
      "Epoch:  538 Loss:  1.4212478570194225\n",
      "Epoch:  539 Loss:  1.3688272841919755\n",
      "Epoch:  540 Loss:  1.4145924887116526\n",
      "Epoch:  541 Loss:  1.3627290060903765\n",
      "Epoch:  542 Loss:  1.4079357792561122\n",
      "Epoch:  543 Loss:  1.3566274962757348\n",
      "Epoch:  544 Loss:  1.4012771669358384\n",
      "Epoch:  545 Loss:  1.3505231280592396\n",
      "Epoch:  546 Loss:  1.3946169837443299\n",
      "Epoch:  547 Loss:  1.344415540655576\n",
      "Epoch:  548 Loss:  1.3879549680100158\n",
      "Epoch:  549 Loss:  1.3383048483323967\n",
      "Epoch:  550 Loss:  1.3812912482599955\n",
      "Epoch:  551 Loss:  1.3321908615784999\n",
      "Epoch:  552 Loss:  1.374625699613547\n",
      "Epoch:  553 Loss:  1.3260735833944077\n",
      "Epoch:  554 Loss:  1.3679583601767553\n",
      "Epoch:  555 Loss:  1.3199528965766703\n",
      "Epoch:  556 Loss:  1.361289164740891\n",
      "Epoch:  557 Loss:  1.313828758625329\n",
      "Epoch:  558 Loss:  1.3546181132006907\n",
      "Epoch:  559 Loss:  1.3077010817008858\n",
      "Epoch:  560 Loss:  1.3479451653332817\n",
      "Epoch:  561 Loss:  1.301569805771865\n",
      "Epoch:  562 Loss:  1.3412703058196784\n",
      "Epoch:  563 Loss:  1.2954348546288839\n",
      "Epoch:  564 Loss:  1.3345935046031219\n",
      "Epoch:  565 Loss:  1.289296162087915\n",
      "Epoch:  566 Loss:  1.3279147408238554\n",
      "Epoch:  567 Loss:  1.2831536566377253\n",
      "Epoch:  568 Loss:  1.3212339886064064\n",
      "Epoch:  569 Loss:  1.2770072703954474\n",
      "Epoch:  570 Loss:  1.3145512254555438\n",
      "Epoch:  571 Loss:  1.2708569340202935\n",
      "Epoch:  572 Loss:  1.307866427422499\n",
      "Epoch:  573 Loss:  1.2647025796397957\n",
      "Epoch:  574 Loss:  1.3011795719159907\n",
      "Epoch:  575 Loss:  1.2585441392646235\n",
      "Epoch:  576 Loss:  1.2944906361547477\n",
      "Epoch:  577 Loss:  1.252381545713346\n",
      "Epoch:  578 Loss:  1.2877995980742034\n",
      "Epoch:  579 Loss:  1.2462147321616817\n",
      "Epoch:  580 Loss:  1.2811064358716886\n",
      "Epoch:  581 Loss:  1.2400436324380568\n",
      "Epoch:  582 Loss:  1.2744111282986832\n",
      "Epoch:  583 Loss:  1.2338681809225642\n",
      "Epoch:  584 Loss:  1.2677136545509875\n",
      "Epoch:  585 Loss:  1.2276883126564984\n",
      "Epoch:  586 Loss:  1.2610139943744698\n",
      "Epoch:  587 Loss:  1.2215039633444635\n",
      "Epoch:  588 Loss:  1.2543121280590694\n",
      "Epoch:  589 Loss:  1.2153150694133465\n",
      "Epoch:  590 Loss:  1.2476080364922242\n",
      "Epoch:  591 Loss:  1.2091215680446694\n",
      "Epoch:  592 Loss:  1.2409017011837136\n",
      "Epoch:  593 Loss:  1.2029233972221782\n",
      "Epoch:  594 Loss:  1.2341931043063994\n",
      "Epoch:  595 Loss:  1.1967204957740523\n",
      "Epoch:  596 Loss:  1.2274822287310039\n",
      "Epoch:  597 Loss:  1.1905128034200225\n",
      "Epoch:  598 Loss:  1.2207690580657178\n",
      "Epoch:  599 Loss:  1.1843002608183584\n",
      "Epoch:  600 Loss:  1.2140535766953153\n",
      "Epoch:  601 Loss:  1.1780828096152887\n",
      "Epoch:  602 Loss:  1.2073357698227012\n",
      "Epoch:  603 Loss:  1.1718603924958857\n",
      "Epoch:  604 Loss:  1.2006156235114787\n",
      "Epoch:  605 Loss:  1.1656329532367227\n",
      "Epoch:  606 Loss:  1.1938931247302935\n",
      "Epoch:  607 Loss:  1.1594004367605648\n",
      "Epoch:  608 Loss:  1.1871682613988466\n",
      "Epoch:  609 Loss:  1.1531627891929548\n",
      "Epoch:  610 Loss:  1.1804410224357869\n",
      "Epoch:  611 Loss:  1.1469199579209908\n",
      "Epoch:  612 Loss:  1.1737113978084024\n",
      "Epoch:  613 Loss:  1.1406718916542167\n",
      "Epoch:  614 Loss:  1.166979378584415\n",
      "Epoch:  615 Loss:  1.1344185404879727\n",
      "Epoch:  616 Loss:  1.1602449569859774\n",
      "Epoch:  617 Loss:  1.128159855969141\n",
      "Epoch:  618 Loss:  1.1535081264458849\n",
      "Epoch:  619 Loss:  1.121895791164563\n",
      "Epoch:  620 Loss:  1.1467688816664163\n",
      "Epoch:  621 Loss:  1.1156263007323077\n",
      "Epoch:  622 Loss:  1.1400272186806406\n",
      "Epoch:  623 Loss:  1.1093513409957099\n",
      "Epoch:  624 Loss:  1.133283134916544\n",
      "Epoch:  625 Loss:  1.103070870020686\n",
      "Epoch:  626 Loss:  1.1265366292642884\n",
      "Epoch:  627 Loss:  1.0967848476963782\n",
      "Epoch:  628 Loss:  1.1197877021464901\n",
      "Epoch:  629 Loss:  1.0904932358191999\n",
      "Epoch:  630 Loss:  1.1130363555919616\n",
      "Epoch:  631 Loss:  1.0841959981807276\n",
      "Epoch:  632 Loss:  1.106282593313114\n",
      "Epoch:  633 Loss:  1.0778931006595596\n",
      "Epoch:  634 Loss:  1.0995264207872693\n",
      "Epoch:  635 Loss:  1.0715845113174132\n",
      "Epoch:  636 Loss:  1.0927678453420118\n",
      "Epoch:  637 Loss:  1.065270200499617\n",
      "Epoch:  638 Loss:  1.0860068762450723\n",
      "Epoch:  639 Loss:  1.058950140940596\n",
      "Epoch:  640 Loss:  1.0792435247989205\n",
      "Epoch:  641 Loss:  1.052624307874208\n",
      "Epoch:  642 Loss:  1.0724778044404029\n",
      "Epoch:  643 Loss:  1.046292679149702\n",
      "Epoch:  644 Loss:  1.0657097308458119\n",
      "Epoch:  645 Loss:  1.0399552353533212\n",
      "Epoch:  646 Loss:  1.0589393220417769\n",
      "Epoch:  647 Loss:  1.033611959936129\n",
      "Epoch:  648 Loss:  1.0521665985222544\n",
      "Epoch:  649 Loss:  1.02726283934827\n",
      "Epoch:  650 Loss:  1.0453915833722005\n",
      "Epoch:  651 Loss:  1.0209078631803348\n",
      "Epoch:  652 Loss:  1.038614302398372\n",
      "Epoch:  653 Loss:  1.0145470243120582\n",
      "Epoch:  654 Loss:  1.0318347842675297\n",
      "Epoch:  655 Loss:  1.0081803190689107\n",
      "Epoch:  656 Loss:  1.025053060652931\n",
      "Epoch:  657 Loss:  1.0018077473872258\n",
      "Epoch:  658 Loss:  1.018269166389378\n",
      "Epoch:  659 Loss:  0.995429312988229\n",
      "Epoch:  660 Loss:  1.011483139637561\n",
      "Epoch:  661 Loss:  0.989045023561691\n",
      "Epoch:  662 Loss:  1.0046950220583648\n",
      "Epoch:  663 Loss:  0.982654890959843\n",
      "Epoch:  664 Loss:  0.9979048589977437\n",
      "Epoch:  665 Loss:  0.9762589314021708\n",
      "Epoch:  666 Loss:  0.9911126996830447\n",
      "Epoch:  667 Loss:  0.9698571656919268\n",
      "Epoch:  668 Loss:  0.9843185974314816\n",
      "Epoch:  669 Loss:  0.9634496194451164\n",
      "Epoch:  670 Loss:  0.9775226098717463\n",
      "Epoch:  671 Loss:  0.9570363233328181\n",
      "Epoch:  672 Loss:  0.9707247991795491\n",
      "Epoch:  673 Loss:  0.9506173133377247\n",
      "Epoch:  674 Loss:  0.9639252323282371\n",
      "Epoch:  675 Loss:  0.944192631025966\n",
      "Epoch:  676 Loss:  0.9571239813554818\n",
      "Epoch:  677 Loss:  0.9377623238353124\n",
      "Epoch:  678 Loss:  0.9503211236473447\n",
      "Epoch:  679 Loss:  0.9313264453808818\n",
      "Epoch:  680 Loss:  0.943516742240897\n",
      "Epoch:  681 Loss:  0.9248850557796207\n",
      "Epoch:  682 Loss:  0.9367109261467275\n",
      "Epoch:  683 Loss:  0.9184382219948817\n",
      "Epoch:  684 Loss:  0.9299037706929739\n",
      "Epoch:  685 Loss:  0.9119860182027155\n",
      "Epoch:  686 Loss:  0.9230953778923413\n",
      "Epoch:  687 Loss:  0.9055285261813142\n",
      "Epoch:  688 Loss:  0.9162858568339578\n",
      "Epoch:  689 Loss:  0.8990658357255288\n",
      "Epoch:  690 Loss:  0.9094753241019651\n",
      "Epoch:  691 Loss:  0.8925980450882068\n",
      "Epoch:  692 Loss:  0.9026639042227576\n",
      "Epoch:  693 Loss:  0.8861252614504328\n",
      "Epoch:  694 Loss:  0.8958517301432192\n",
      "Epoch:  695 Loss:  0.8796476014230313\n",
      "Epoch:  696 Loss:  0.8890389437424083\n",
      "Epoch:  697 Loss:  0.8731651915816263\n",
      "Epoch:  698 Loss:  0.8822256963790988\n",
      "Epoch:  699 Loss:  0.8666781690379283\n",
      "Epoch:  700 Loss:  0.8754121494782658\n",
      "Epoch:  701 Loss:  0.8601866820501881\n",
      "Epoch:  702 Loss:  0.8685984751593985\n",
      "Epoch:  703 Loss:  0.853690890675936\n",
      "Epoch:  704 Loss:  0.8617848569101677\n",
      "Epoch:  705 Loss:  0.847190967470387\n",
      "Epoch:  706 Loss:  0.8549714903088925\n",
      "Epoch:  707 Loss:  0.8406870982342088\n",
      "Epoch:  708 Loss:  0.8481585837998636\n",
      "Epoch:  709 Loss:  0.8341794828147572\n",
      "Epoch:  710 Loss:  0.84134635952583\n",
      "Epoch:  711 Loss:  0.8276683359651466\n",
      "Epoch:  712 Loss:  0.8345350542222107\n",
      "Epoch:  713 Loss:  0.8211538882659075\n",
      "Epoch:  714 Loss:  0.8277249201780426\n",
      "Epoch:  715 Loss:  0.8146363871144149\n",
      "Epoch:  716 Loss:  0.8209162262693278\n",
      "Epoch:  717 Loss:  0.8081160977878739\n",
      "Epoch:  718 Loss:  0.814109259070394\n",
      "Epoch:  719 Loss:  0.8015933045856887\n",
      "Epoch:  720 Loss:  0.8073043240499174\n",
      "Epoch:  721 Loss:  0.7950683120581755\n",
      "Epoch:  722 Loss:  0.8005017468583492\n",
      "Epoch:  723 Loss:  0.7885414463285577\n",
      "Epoch:  724 Loss:  0.7937018747143436\n",
      "Epoch:  725 Loss:  0.7820130565161304\n",
      "Epoch:  726 Loss:  0.7869050778980703\n",
      "Epoch:  727 Loss:  0.7754835162688564\n",
      "Epoch:  728 Loss:  0.7801117513601993\n",
      "Epoch:  729 Loss:  0.7689532254145697\n",
      "Epoch:  730 Loss:  0.7733223164558033\n",
      "Epoch:  731 Loss:  0.7624226117403096\n",
      "Epoch:  732 Loss:  0.7665372228130262\n",
      "Epoch:  733 Loss:  0.755892132910269\n",
      "Epoch:  734 Loss:  0.7597569503472371\n",
      "Epoch:  735 Loss:  0.749362278533295\n",
      "Epoch:  736 Loss:  0.7529820114318054\n",
      "Epoch:  737 Loss:  0.7428335723919799\n",
      "Epoch:  738 Loss:  0.7462129532377564\n",
      "Epoch:  739 Loss:  0.7363065748456409\n",
      "Epoch:  740 Loss:  0.7394503602544807\n",
      "Epoch:  741 Loss:  0.7297818854203165\n",
      "Epoch:  742 Loss:  0.7326948570050272\n",
      "Epoch:  743 Loss:  0.7232601455998058\n",
      "Epoch:  744 Loss:  0.7259471109696738\n",
      "Epoch:  745 Loss:  0.7167420418319105\n",
      "Epoch:  746 Loss:  0.7192078357319986\n",
      "Epoch:  747 Loss:  0.7102283087648026\n",
      "Epoch:  748 Loss:  0.712477794361974\n",
      "Epoch:  749 Loss:  0.7037197327286544\n",
      "Epoch:  750 Loss:  0.7057578030509136\n",
      "Epoch:  751 Loss:  0.6972171554776998\n",
      "Epoch:  752 Loss:  0.6990487350128328\n",
      "Epoch:  753 Loss:  0.6907214782079358\n",
      "Epoch:  754 Loss:  0.6923515246665556\n",
      "Epoch:  755 Loss:  0.6842336658651081\n",
      "Epoch:  756 Loss:  0.6856671721122852\n",
      "Epoch:  757 Loss:  0.6777547517567567\n",
      "Epoch:  758 Loss:  0.678996747914827\n",
      "Epoch:  759 Loss:  0.6712858424807375\n",
      "Epoch:  760 Loss:  0.6723413982043035\n",
      "Epoch:  761 Loss:  0.6648281231807086\n",
      "Epoch:  762 Loss:  0.6657023501026051\n",
      "Epoch:  763 Loss:  0.65838286313627\n",
      "Epoch:  764 Loss:  0.6590809174804224\n",
      "Epoch:  765 Loss:  0.6519514216916432\n",
      "Epoch:  766 Loss:  0.652478507045694\n",
      "Epoch:  767 Loss:  0.6455352545223467\n",
      "Epoch:  768 Loss:  0.6458966247586756\n",
      "Epoch:  769 Loss:  0.6391359202329248\n",
      "Epoch:  770 Loss:  0.6393368825621247\n",
      "Epoch:  771 Loss:  0.6327550872714955\n",
      "Epoch:  772 Loss:  0.6328010054065636\n",
      "Epoch:  773 Loss:  0.6263945411374543\n",
      "Epoch:  774 Loss:  0.62629083854031\n",
      "Epoch:  775 Loss:  0.6200561918474378\n",
      "Epoch:  776 Loss:  0.6198083550215936\n",
      "Epoch:  777 Loss:  0.6137420816110599\n",
      "Epoch:  778 Loss:  0.6133556633952643\n",
      "Epoch:  779 Loss:  0.6074543926519834\n",
      "Epoch:  780 Loss:  0.6069350154591941\n",
      "Epoch:  781 Loss:  0.6011954550908447\n",
      "Epoch:  782 Loss:  0.6005488140251769\n",
      "Epoch:  783 Loss:  0.5949677547848773\n",
      "Epoch:  784 Loss:  0.5941996205558255\n",
      "Epoch:  785 Loss:  0.5887739409940241\n",
      "Epoch:  786 Loss:  0.587890162532462\n",
      "Epoch:  787 Loss:  0.5826168337149776\n",
      "Epoch:  788 Loss:  0.5816233403791848\n",
      "Epoch:  789 Loss:  0.5764994304932819\n",
      "Epoch:  790 Loss:  0.5754022337356024\n",
      "Epoch:  791 Loss:  0.5704249124890018\n",
      "Epoch:  792 Loss:  0.5692301068349264\n",
      "Epoch:  793 Loss:  0.5643966495343408\n",
      "Epoch:  794 Loss:  0.5631104127062873\n",
      "Epoch:  795 Loss:  0.5584182038825962\n",
      "Epoch:  796 Loss:  0.5570467958807145\n",
      "Epoch:  797 Loss:  0.5524933323077975\n",
      "Epoch:  798 Loss:  0.5510430932406757\n",
      "Epoch:  799 Loss:  0.5466259861749312\n",
      "Epoch:  800 Loss:  0.5451033326148642\n",
      "Epoch:  801 Loss:  0.5408203090633951\n",
      "Epoch:  802 Loss:  0.5392317286850852\n",
      "Epoch:  803 Loss:  0.5350806314939934\n",
      "Epoch:  804 Loss:  0.5334326757436945\n",
      "Epoch:  805 Loss:  0.5294114622848937\n",
      "Epoch:  806 Loss:  0.5277107368204986\n",
      "Epoch:  807 Loss:  0.523817476048129\n",
      "Epoch:  808 Loss:  0.5220706286918637\n",
      "Epoch:  809 Loss:  0.5183034963395035\n",
      "Epoch:  810 Loss:  0.516517202295302\n",
      "Epoch:  811 Loss:  0.5128744739948539\n",
      "Epoch:  812 Loss:  0.5110554181050349\n",
      "Epoch:  813 Loss:  0.5075354602297857\n",
      "Epoch:  814 Loss:  0.5056903160819646\n",
      "Epoch:  815 Loss:  0.5022915741518152\n",
      "Epoch:  816 Loss:  0.5004269798997261\n",
      "Epoch:  817 Loss:  0.49714796443752984\n",
      "Epoch:  818 Loss:  0.49527049526994404\n",
      "Epoch:  819 Loss:  0.49210976506603876\n",
      "Epoch:  820 Loss:  0.4902259023476594\n",
      "Epoch:  821 Loss:  0.48718204517473657\n",
      "Epoch:  822 Loss:  0.4852981423918735\n",
      "Epoch:  823 Loss:  0.482369753313977\n",
      "Epoch:  824 Loss:  0.4804919990851594\n",
      "Epoch:  825 Loss:  0.47767765662066736\n",
      "Epoch:  826 Loss:  0.47581203517540427\n",
      "Epoch:  827 Loss:  0.4731102757003452\n",
      "Epoch:  828 Loss:  0.4712625253841172\n",
      "Epoch:  829 Loss:  0.46867181629404514\n",
      "Epoch:  830 Loss:  0.46684738681783744\n",
      "Epoch:  831 Loss:  0.46436609909644294\n",
      "Epoch:  832 Loss:  0.46257010840703294\n",
      "Epoch:  833 Loss:  0.46019648936934493\n",
      "Epoch:  834 Loss:  0.45843368116225613\n",
      "Epoch:  835 Loss:  0.45616582824025814\n",
      "Epoch:  836 Loss:  0.45444053126000983\n",
      "Epoch:  837 Loss:  0.45227636776897034\n",
      "Epoch:  838 Loss:  0.4505924581293267\n",
      "Epoch:  839 Loss:  0.448529711985085\n",
      "Epoch:  840 Loss:  0.4468905797843841\n",
      "Epoch:  841 Loss:  0.4449267661272107\n",
      "Epoch:  842 Loss:  0.4433352876205451\n",
      "Epoch:  843 Loss:  0.44146769623501475\n",
      "Epoch:  844 Loss:  0.43992621274871285\n",
      "Epoch:  845 Loss:  0.43815190104911744\n",
      "Epoch:  846 Loss:  0.4366622056800451\n",
      "Epoch:  847 Loss:  0.4349779978591185\n",
      "Epoch:  848 Loss:  0.4335413307924774\n",
      "Epoch:  849 Loss:  0.43194382351403066\n",
      "Epoch:  850 Loss:  0.43056087652466146\n",
      "Epoch:  851 Loss:  0.4290464512881821\n",
      "Epoch:  852 Loss:  0.42771738167299506\n",
      "Epoch:  853 Loss:  0.42628222370447894\n",
      "Epoch:  854 Loss:  0.4250066775437109\n",
      "Epoch:  855 Loss:  0.4236468007881049\n",
      "Epoch:  856 Loss:  0.42242394507039027\n",
      "Epoch:  857 Loss:  0.4211352225949591\n",
      "Epoch:  858 Loss:  0.4199637853877455\n",
      "Epoch:  859 Loss:  0.41874198426951637\n",
      "Epoch:  860 Loss:  0.4176203017950303\n",
      "Epoch:  861 Loss:  0.41646112137475494\n",
      "Epoch:  862 Loss:  0.41538719058300283\n",
      "Epoch:  863 Loss:  0.41428630283486606\n",
      "Epoch:  864 Loss:  0.4132578378669504\n",
      "Epoch:  865 Loss:  0.412210928565502\n",
      "Epoch:  866 Loss:  0.41122541938452084\n",
      "Epoch:  867 Loss:  0.4102282287507559\n",
      "Epoch:  868 Loss:  0.40928300019011404\n",
      "Epoch:  869 Loss:  0.4083313617652118\n",
      "Epoch:  870 Loss:  0.40742363130416337\n",
      "Epoch:  871 Loss:  0.4065135079251759\n",
      "Epoch:  872 Loss:  0.4056404406418765\n",
      "Epoch:  873 Loss:  0.4047679565680918\n",
      "Epoch:  874 Loss:  0.4039267159283541\n",
      "Epoch:  875 Loss:  0.40308818437656757\n",
      "Epoch:  876 Loss:  0.40227597777523133\n",
      "Epoch:  877 Loss:  0.4014679233520781\n",
      "Epoch:  878 Loss:  0.40068204161397125\n",
      "Epoch:  879 Loss:  0.3999012173688167\n",
      "Epoch:  880 Loss:  0.39913906771806257\n",
      "Epoch:  881 Loss:  0.39838246676670597\n",
      "Epoch:  882 Loss:  0.3976415990676584\n",
      "Epoch:  883 Loss:  0.39690646094378335\n",
      "Epoch:  884 Loss:  0.3961845872883001\n",
      "Epoch:  885 Loss:  0.3954683993565808\n",
      "Epoch:  886 Loss:  0.39476340730772247\n",
      "Epoch:  887 Loss:  0.39406390171413513\n",
      "Epoch:  888 Loss:  0.3933738617073508\n",
      "Epoch:  889 Loss:  0.39268900844504606\n",
      "Epoch:  890 Loss:  0.392012175989865\n",
      "Epoch:  891 Loss:  0.39134017272310395\n",
      "Epoch:  892 Loss:  0.39067498614082175\n",
      "Epoch:  893 Loss:  0.3900142454568301\n",
      "Epoch:  894 Loss:  0.38935931993525524\n",
      "Epoch:  895 Loss:  0.3887084546887196\n",
      "Epoch:  896 Loss:  0.3880625734389953\n",
      "Epoch:  897 Loss:  0.3874203808215548\n",
      "Epoch:  898 Loss:  0.38678248409122595\n",
      "Epoch:  899 Loss:  0.38614792900478234\n",
      "Epoch:  900 Loss:  0.38551710164338215\n",
      "Epoch:  901 Loss:  0.3848892998875023\n",
      "Epoch:  902 Loss:  0.3842647580840537\n",
      "Epoch:  903 Loss:  0.38364295979020846\n",
      "Epoch:  904 Loss:  0.3830240375134294\n",
      "Epoch:  905 Loss:  0.38240761117758354\n",
      "Epoch:  906 Loss:  0.3817937467562221\n",
      "Epoch:  907 Loss:  0.3811821641408041\n",
      "Epoch:  908 Loss:  0.38057288732874783\n",
      "Epoch:  909 Loss:  0.3799657094290562\n",
      "Epoch:  910 Loss:  0.37936062921215313\n",
      "Epoch:  911 Loss:  0.3787574934132011\n",
      "Epoch:  912 Loss:  0.37815628673518936\n",
      "Epoch:  913 Loss:  0.3775568952247614\n",
      "Epoch:  914 Loss:  0.37695929674027095\n",
      "Epoch:  915 Loss:  0.3763634061933253\n",
      "Epoch:  916 Loss:  0.37576919909763545\n",
      "Epoch:  917 Loss:  0.3751766116064964\n",
      "Epoch:  918 Loss:  0.3745856195446976\n",
      "Epoch:  919 Loss:  0.37399617473817237\n",
      "Epoch:  920 Loss:  0.37340825476020256\n",
      "Epoch:  921 Loss:  0.37282182303241007\n",
      "Epoch:  922 Loss:  0.37223685953406094\n",
      "Epoch:  923 Loss:  0.37165333628906727\n",
      "Epoch:  924 Loss:  0.3710712358614411\n",
      "Epoch:  925 Loss:  0.37049053667179316\n",
      "Epoch:  926 Loss:  0.3699112237713261\n",
      "Epoch:  927 Loss:  0.3693332803460166\n",
      "Epoch:  928 Loss:  0.36875669369276776\n",
      "Epoch:  929 Loss:  0.36818145055181206\n",
      "Epoch:  930 Loss:  0.36760754016399355\n",
      "Epoch:  931 Loss:  0.3670349519216104\n",
      "Epoch:  932 Loss:  0.36646367669791247\n",
      "Epoch:  933 Loss:  0.36589370586335657\n",
      "Epoch:  934 Loss:  0.3653250316306399\n",
      "Epoch:  935 Loss:  0.3647576468440491\n",
      "Epoch:  936 Loss:  0.36419154479541466\n",
      "Epoch:  937 Loss:  0.36362671942512076\n",
      "Epoch:  938 Loss:  0.36306316488158485\n",
      "Epoch:  939 Loss:  0.3625008759184626\n",
      "Epoch:  940 Loss:  0.3619398473558768\n",
      "Epoch:  941 Loss:  0.3613800745492603\n",
      "Epoch:  942 Loss:  0.36082155284024436\n",
      "Epoch:  943 Loss:  0.3602642780283053\n",
      "Epoch:  944 Loss:  0.35970824585674294\n",
      "Epoch:  945 Loss:  0.35915345245184144\n",
      "Epoch:  946 Loss:  0.3585998938644078\n",
      "Epoch:  947 Loss:  0.35804756646078467\n",
      "Epoch:  948 Loss:  0.3574964665262671\n",
      "Epoch:  949 Loss:  0.3569465906034097\n",
      "Epoch:  950 Loss:  0.3563979351560073\n",
      "Epoch:  951 Loss:  0.3558504968561367\n",
      "Epoch:  952 Loss:  0.3553042723035916\n",
      "Epoch:  953 Loss:  0.3547592582660543\n",
      "Epoch:  954 Loss:  0.35421545144735567\n",
      "Epoch:  955 Loss:  0.35367284868632914\n",
      "Epoch:  956 Loss:  0.35313144676700287\n",
      "Epoch:  957 Loss:  0.3525912425818884\n",
      "Epoch:  958 Loss:  0.3520522329775348\n",
      "Epoch:  959 Loss:  0.3515144148877582\n",
      "Epoch:  960 Loss:  0.35097778520863526\n",
      "Epoch:  961 Loss:  0.3504423409065946\n",
      "Epoch:  962 Loss:  0.34990807891772807\n",
      "Epoch:  963 Loss:  0.34937499623505386\n",
      "Epoch:  964 Loss:  0.3488430898277493\n",
      "Epoch:  965 Loss:  0.3483123567112853\n",
      "Epoch:  966 Loss:  0.3477827938828922\n",
      "Epoch:  967 Loss:  0.3472543983776974\n",
      "Epoch:  968 Loss:  0.34672716721734165\n",
      "Epoch:  969 Loss:  0.346201097454745\n",
      "Epoch:  970 Loss:  0.34567618613331713\n",
      "Epoch:  971 Loss:  0.34515243032254206\n",
      "Epoch:  972 Loss:  0.34462982708569767\n",
      "Epoch:  973 Loss:  0.34410837350803436\n",
      "Epoch:  974 Loss:  0.3435880666713201\n",
      "Epoch:  975 Loss:  0.3430689036760643\n",
      "Epoch:  976 Loss:  0.34255088162151925\n",
      "Epoch:  977 Loss:  0.3420339976231405\n",
      "Epoch:  978 Loss:  0.34151824879692255\n",
      "Epoch:  979 Loss:  0.3410036322730816\n",
      "Epoch:  980 Loss:  0.34049014518380216\n",
      "Epoch:  981 Loss:  0.3399777846739444\n",
      "Epoch:  982 Loss:  0.3394665478914804\n",
      "Epoch:  983 Loss:  0.3389564319958308\n",
      "Epoch:  984 Loss:  0.3384474341504455\n",
      "Epoch:  985 Loss:  0.33793955152926336\n",
      "Epoch:  986 Loss:  0.3374327813109651\n",
      "Epoch:  987 Loss:  0.33692712068397007\n",
      "Epoch:  988 Loss:  0.3364225668420028\n",
      "Epoch:  989 Loss:  0.3359191169879292\n",
      "Epoch:  990 Loss:  0.3354167683303557\n",
      "Epoch:  991 Loss:  0.3349155180865712\n",
      "Epoch:  992 Loss:  0.3344153634799294\n",
      "Epoch:  993 Loss:  0.33391630174210674\n",
      "Epoch:  994 Loss:  0.3334183301110955\n",
      "Epoch:  995 Loss:  0.3329214458329229\n",
      "Epoch:  996 Loss:  0.332425646160122\n",
      "Epoch:  997 Loss:  0.33193092835302584\n",
      "Epoch:  998 Loss:  0.33143728967861613\n",
      "Epoch:  999 Loss:  0.3309447274115001\n"
     ]
    }
   ],
   "source": [
    "num_classes = 4\n",
    "num_features = X_train.shape[1]\n",
    "W = np.random.rand(num_features, num_classes)\n",
    "b = np.zeros(num_classes)\n",
    "model = Model(W, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Loss:  0.11490648146861379\n",
      "Epoch:  1 Loss:  0.11482316096496757\n",
      "Epoch:  2 Loss:  0.11473995520304676\n",
      "Epoch:  3 Loss:  0.11465686396704072\n",
      "Epoch:  4 Loss:  0.11457388704162846\n",
      "Epoch:  5 Loss:  0.11449102421197878\n",
      "Epoch:  6 Loss:  0.11440827526375076\n",
      "Epoch:  7 Loss:  0.114325639983087\n",
      "Epoch:  8 Loss:  0.11424311815661854\n",
      "Epoch:  9 Loss:  0.11416070957146078\n",
      "Epoch:  10 Loss:  0.11407841401521064\n",
      "Epoch:  11 Loss:  0.11399623127594916\n",
      "Epoch:  12 Loss:  0.11391416114223606\n",
      "Epoch:  13 Loss:  0.11383220340311365\n",
      "Epoch:  14 Loss:  0.11375035784809932\n",
      "Epoch:  15 Loss:  0.1136686242671901\n",
      "Epoch:  16 Loss:  0.11358700245085751\n",
      "Epoch:  17 Loss:  0.11350549219004832\n",
      "Epoch:  18 Loss:  0.11342409327618234\n",
      "Epoch:  19 Loss:  0.11334280550115264\n",
      "Epoch:  20 Loss:  0.11326162865732342\n",
      "Epoch:  21 Loss:  0.1131805625375266\n",
      "Epoch:  22 Loss:  0.1130996069350651\n",
      "Epoch:  23 Loss:  0.11301876164370867\n",
      "Epoch:  24 Loss:  0.1129380264576933\n",
      "Epoch:  25 Loss:  0.11285740117171988\n",
      "Epoch:  26 Loss:  0.11277688558095357\n",
      "Epoch:  27 Loss:  0.11269647948102199\n",
      "Epoch:  28 Loss:  0.11261618266801426\n",
      "Epoch:  29 Loss:  0.11253599493848006\n",
      "Epoch:  30 Loss:  0.11245591608942826\n",
      "Epoch:  31 Loss:  0.11237594591832616\n",
      "Epoch:  32 Loss:  0.11229608422309616\n",
      "Epoch:  33 Loss:  0.11221633080211955\n",
      "Epoch:  34 Loss:  0.11213668545422908\n",
      "Epoch:  35 Loss:  0.11205714797871304\n",
      "Epoch:  36 Loss:  0.11197771817530991\n",
      "Epoch:  37 Loss:  0.11189839584421137\n",
      "Epoch:  38 Loss:  0.11181918078605838\n",
      "Epoch:  39 Loss:  0.11174007280193868\n",
      "Epoch:  40 Loss:  0.11166107169339061\n",
      "Epoch:  41 Loss:  0.1115821772623984\n",
      "Epoch:  42 Loss:  0.1115033893113893\n",
      "Epoch:  43 Loss:  0.11142470764323725\n",
      "Epoch:  44 Loss:  0.11134613206125904\n",
      "Epoch:  45 Loss:  0.1112676623692119\n",
      "Epoch:  46 Loss:  0.11118929837129576\n",
      "Epoch:  47 Loss:  0.11111103987214892\n",
      "Epoch:  48 Loss:  0.11103288667684919\n",
      "Epoch:  49 Loss:  0.11095483859091118\n",
      "Epoch:  50 Loss:  0.11087689542028688\n",
      "Epoch:  51 Loss:  0.1107990569713615\n",
      "Epoch:  52 Loss:  0.1107213230509575\n",
      "Epoch:  53 Loss:  0.11064369346632787\n",
      "Epoch:  54 Loss:  0.11056616802515828\n",
      "Epoch:  55 Loss:  0.11048874653556601\n",
      "Epoch:  56 Loss:  0.11041142880609661\n",
      "Epoch:  57 Loss:  0.11033421464572617\n",
      "Epoch:  58 Loss:  0.11025710386385691\n",
      "Epoch:  59 Loss:  0.11018009627031823\n",
      "Epoch:  60 Loss:  0.11010319167536524\n",
      "Epoch:  61 Loss:  0.11002638988967639\n",
      "Epoch:  62 Loss:  0.10994969072435336\n",
      "Epoch:  63 Loss:  0.1098730939909224\n",
      "Epoch:  64 Loss:  0.10979659950132757\n",
      "Epoch:  65 Loss:  0.10972020706793555\n",
      "Epoch:  66 Loss:  0.10964391650353064\n",
      "Epoch:  67 Loss:  0.10956772762131559\n",
      "Epoch:  68 Loss:  0.10949164023490997\n",
      "Epoch:  69 Loss:  0.10941565415834868\n",
      "Epoch:  70 Loss:  0.10933976920608254\n",
      "Epoch:  71 Loss:  0.10926398519297402\n",
      "Epoch:  72 Loss:  0.10918830193430108\n",
      "Epoch:  73 Loss:  0.10911271924575135\n",
      "Epoch:  74 Loss:  0.10903723694342392\n",
      "Epoch:  75 Loss:  0.10896185484382545\n",
      "Epoch:  76 Loss:  0.10888657276387423\n",
      "Epoch:  77 Loss:  0.10881139052089349\n",
      "Epoch:  78 Loss:  0.10873630793261473\n",
      "Epoch:  79 Loss:  0.10866132481717382\n",
      "Epoch:  80 Loss:  0.1085864409931101\n",
      "Epoch:  81 Loss:  0.10851165627936898\n",
      "Epoch:  82 Loss:  0.10843697049529535\n",
      "Epoch:  83 Loss:  0.10836238346063731\n",
      "Epoch:  84 Loss:  0.1082878949955422\n",
      "Epoch:  85 Loss:  0.10821350492055726\n",
      "Epoch:  86 Loss:  0.1081392130566278\n",
      "Epoch:  87 Loss:  0.10806501922509651\n",
      "Epoch:  88 Loss:  0.10799092324770247\n",
      "Epoch:  89 Loss:  0.10791692494657978\n",
      "Epoch:  90 Loss:  0.10784302414425657\n",
      "Epoch:  91 Loss:  0.10776922066365578\n",
      "Epoch:  92 Loss:  0.10769551432808948\n",
      "Epoch:  93 Loss:  0.10762190496126504\n",
      "Epoch:  94 Loss:  0.10754839238727618\n",
      "Epoch:  95 Loss:  0.10747497643061021\n",
      "Epoch:  96 Loss:  0.10740165691613829\n",
      "Epoch:  97 Loss:  0.10732843366912216\n",
      "Epoch:  98 Loss:  0.10725530651520981\n",
      "Epoch:  99 Loss:  0.10718227528043195\n",
      "Epoch:  100 Loss:  0.10710933979120657\n",
      "Epoch:  101 Loss:  0.10703649987433424\n",
      "Epoch:  102 Loss:  0.10696375535699787\n",
      "Epoch:  103 Loss:  0.10689110606676169\n",
      "Epoch:  104 Loss:  0.10681855183157123\n",
      "Epoch:  105 Loss:  0.10674609247975098\n",
      "Epoch:  106 Loss:  0.10667372784000374\n",
      "Epoch:  107 Loss:  0.10660145774141182\n",
      "Epoch:  108 Loss:  0.10652928201343283\n",
      "Epoch:  109 Loss:  0.10645720048589935\n",
      "Epoch:  110 Loss:  0.10638521298902151\n",
      "Epoch:  111 Loss:  0.10631331935338142\n",
      "Epoch:  112 Loss:  0.10624151940993402\n",
      "Epoch:  113 Loss:  0.10616981299000762\n",
      "Epoch:  114 Loss:  0.1060981999253006\n",
      "Epoch:  115 Loss:  0.10602668004788114\n",
      "Epoch:  116 Loss:  0.10595525319018907\n",
      "Epoch:  117 Loss:  0.10588391918502928\n",
      "Epoch:  118 Loss:  0.10581267786557608\n",
      "Epoch:  119 Loss:  0.10574152906537099\n",
      "Epoch:  120 Loss:  0.10567047261831854\n",
      "Epoch:  121 Loss:  0.10559950835868921\n",
      "Epoch:  122 Loss:  0.10552863612111782\n",
      "Epoch:  123 Loss:  0.10545785574060139\n",
      "Epoch:  124 Loss:  0.10538716705249986\n",
      "Epoch:  125 Loss:  0.10531656989253067\n",
      "Epoch:  126 Loss:  0.10524606409677698\n",
      "Epoch:  127 Loss:  0.1051756495016756\n",
      "Epoch:  128 Loss:  0.10510532594402595\n",
      "Epoch:  129 Loss:  0.10503509326098139\n",
      "Epoch:  130 Loss:  0.10496495129005484\n",
      "Epoch:  131 Loss:  0.10489489986911178\n",
      "Epoch:  132 Loss:  0.10482493883637599\n",
      "Epoch:  133 Loss:  0.10475506803042099\n",
      "Epoch:  134 Loss:  0.10468528729017625\n",
      "Epoch:  135 Loss:  0.10461559645492192\n",
      "Epoch:  136 Loss:  0.10454599536429003\n",
      "Epoch:  137 Loss:  0.1044764838582625\n",
      "Epoch:  138 Loss:  0.10440706177717095\n",
      "Epoch:  139 Loss:  0.10433772896169478\n",
      "Epoch:  140 Loss:  0.10426848525286252\n",
      "Epoch:  141 Loss:  0.10419933049204769\n",
      "Epoch:  142 Loss:  0.10413026452097068\n",
      "Epoch:  143 Loss:  0.10406128718169756\n",
      "Epoch:  144 Loss:  0.10399239831663683\n",
      "Epoch:  145 Loss:  0.10392359776854267\n",
      "Epoch:  146 Loss:  0.10385488538050985\n",
      "Epoch:  147 Loss:  0.10378626099597602\n",
      "Epoch:  148 Loss:  0.10371772445871866\n",
      "Epoch:  149 Loss:  0.1036492756128546\n",
      "Epoch:  150 Loss:  0.10358091430284173\n",
      "Epoch:  151 Loss:  0.1035126403734753\n",
      "Epoch:  152 Loss:  0.10344445366988703\n",
      "Epoch:  153 Loss:  0.10337635403754682\n",
      "Epoch:  154 Loss:  0.10330834132225826\n",
      "Epoch:  155 Loss:  0.10324041537016189\n",
      "Epoch:  156 Loss:  0.10317257602773047\n",
      "Epoch:  157 Loss:  0.1031048231417717\n",
      "Epoch:  158 Loss:  0.10303715655942386\n",
      "Epoch:  159 Loss:  0.10296957612815737\n",
      "Epoch:  160 Loss:  0.10290208169577537\n",
      "Epoch:  161 Loss:  0.10283467311040762\n",
      "Epoch:  162 Loss:  0.10276735022051499\n",
      "Epoch:  163 Loss:  0.10270011287488723\n",
      "Epoch:  164 Loss:  0.10263296092263795\n",
      "Epoch:  165 Loss:  0.10256589421321251\n",
      "Epoch:  166 Loss:  0.10249891259637654\n",
      "Epoch:  167 Loss:  0.10243201592222616\n",
      "Epoch:  168 Loss:  0.10236520404117777\n",
      "Epoch:  169 Loss:  0.10229847680397267\n",
      "Epoch:  170 Loss:  0.10223183406167434\n",
      "Epoch:  171 Loss:  0.10216527566566813\n",
      "Epoch:  172 Loss:  0.10209880146766076\n",
      "Epoch:  173 Loss:  0.10203241131967819\n",
      "Epoch:  174 Loss:  0.10196610507406702\n",
      "Epoch:  175 Loss:  0.1018998825834911\n",
      "Epoch:  176 Loss:  0.10183374370093409\n",
      "Epoch:  177 Loss:  0.10176768827969387\n",
      "Epoch:  178 Loss:  0.10170171617338655\n",
      "Epoch:  179 Loss:  0.10163582723594303\n",
      "Epoch:  180 Loss:  0.10157002132160933\n",
      "Epoch:  181 Loss:  0.10150429828494427\n",
      "Epoch:  182 Loss:  0.10143865798082109\n",
      "Epoch:  183 Loss:  0.10137310026442503\n",
      "Epoch:  184 Loss:  0.1013076249912512\n",
      "Epoch:  185 Loss:  0.10124223201710827\n",
      "Epoch:  186 Loss:  0.10117692119811283\n",
      "Epoch:  187 Loss:  0.10111169239069136\n",
      "Epoch:  188 Loss:  0.10104654545157973\n",
      "Epoch:  189 Loss:  0.10098148023781905\n",
      "Epoch:  190 Loss:  0.1009164966067602\n",
      "Epoch:  191 Loss:  0.10085159441605912\n",
      "Epoch:  192 Loss:  0.10078677352367617\n",
      "Epoch:  193 Loss:  0.1007220337878772\n",
      "Epoch:  194 Loss:  0.10065737506723284\n",
      "Epoch:  195 Loss:  0.10059279722061566\n",
      "Epoch:  196 Loss:  0.10052830010720121\n",
      "Epoch:  197 Loss:  0.10046388358646709\n",
      "Epoch:  198 Loss:  0.10039954751819044\n",
      "Epoch:  199 Loss:  0.1003352917624495\n",
      "Epoch:  200 Loss:  0.100271116179623\n",
      "Epoch:  201 Loss:  0.10020702063038588\n",
      "Epoch:  202 Loss:  0.10014300497571332\n",
      "Epoch:  203 Loss:  0.10007906907687644\n",
      "Epoch:  204 Loss:  0.10001521279544341\n",
      "Epoch:  205 Loss:  0.09995143599327944\n",
      "Epoch:  206 Loss:  0.0998877385325404\n",
      "Epoch:  207 Loss:  0.09982412027568188\n",
      "Epoch:  208 Loss:  0.09976058108544957\n",
      "Epoch:  209 Loss:  0.09969712082488356\n",
      "Epoch:  210 Loss:  0.09963373935731526\n",
      "Epoch:  211 Loss:  0.09957043654636795\n",
      "Epoch:  212 Loss:  0.09950721225595492\n",
      "Epoch:  213 Loss:  0.09944406635028127\n",
      "Epoch:  214 Loss:  0.09938099869383865\n",
      "Epoch:  215 Loss:  0.0993180091514095\n",
      "Epoch:  216 Loss:  0.09925509758806349\n",
      "Epoch:  217 Loss:  0.09919226386915679\n",
      "Epoch:  218 Loss:  0.09912950786033196\n",
      "Epoch:  219 Loss:  0.09906682942751957\n",
      "Epoch:  220 Loss:  0.09900422843693178\n",
      "Epoch:  221 Loss:  0.09894170475506701\n",
      "Epoch:  222 Loss:  0.09887925824870859\n",
      "Epoch:  223 Loss:  0.09881688878491901\n",
      "Epoch:  224 Loss:  0.09875459623104652\n",
      "Epoch:  225 Loss:  0.09869238045471991\n",
      "Epoch:  226 Loss:  0.09863024132384732\n",
      "Epoch:  227 Loss:  0.09856817870661916\n",
      "Epoch:  228 Loss:  0.09850619247150368\n",
      "Epoch:  229 Loss:  0.09844428248725012\n",
      "Epoch:  230 Loss:  0.09838244862288271\n",
      "Epoch:  231 Loss:  0.09832069074770562\n",
      "Epoch:  232 Loss:  0.09825900873129861\n",
      "Epoch:  233 Loss:  0.09819740244351868\n",
      "Epoch:  234 Loss:  0.09813587175449558\n",
      "Epoch:  235 Loss:  0.09807441653463679\n",
      "Epoch:  236 Loss:  0.09801303665462266\n",
      "Epoch:  237 Loss:  0.09795173198540616\n",
      "Epoch:  238 Loss:  0.09789050239821466\n",
      "Epoch:  239 Loss:  0.09782934776454516\n",
      "Epoch:  240 Loss:  0.09776826795616843\n",
      "Epoch:  241 Loss:  0.09770726284512418\n",
      "Epoch:  242 Loss:  0.09764633230372426\n",
      "Epoch:  243 Loss:  0.09758547620454663\n",
      "Epoch:  244 Loss:  0.09752469442044114\n",
      "Epoch:  245 Loss:  0.09746398682452466\n",
      "Epoch:  246 Loss:  0.09740335329018086\n",
      "Epoch:  247 Loss:  0.09734279369106065\n",
      "Epoch:  248 Loss:  0.09728230790108017\n",
      "Epoch:  249 Loss:  0.09722189579442314\n",
      "Epoch:  250 Loss:  0.09716155724553664\n",
      "Epoch:  251 Loss:  0.09710129212913109\n",
      "Epoch:  252 Loss:  0.0970411003201823\n",
      "Epoch:  253 Loss:  0.09698098169392766\n",
      "Epoch:  254 Loss:  0.0969209361258672\n",
      "Epoch:  255 Loss:  0.09686096349176353\n",
      "Epoch:  256 Loss:  0.09680106366763755\n",
      "Epoch:  257 Loss:  0.09674123652977323\n",
      "Epoch:  258 Loss:  0.09668148195471281\n",
      "Epoch:  259 Loss:  0.09662179981925828\n",
      "Epoch:  260 Loss:  0.09656219000047056\n",
      "Epoch:  261 Loss:  0.09650265237566558\n",
      "Epoch:  262 Loss:  0.09644318682241977\n",
      "Epoch:  263 Loss:  0.09638379321856447\n",
      "Epoch:  264 Loss:  0.09632447144218753\n",
      "Epoch:  265 Loss:  0.09626522137163088\n",
      "Epoch:  266 Loss:  0.09620604288549299\n",
      "Epoch:  267 Loss:  0.09614693586262525\n",
      "Epoch:  268 Loss:  0.09608790018213255\n",
      "Epoch:  269 Loss:  0.09602893572337262\n",
      "Epoch:  270 Loss:  0.09597004236595569\n",
      "Epoch:  271 Loss:  0.09591121998974338\n",
      "Epoch:  272 Loss:  0.09585246847484895\n",
      "Epoch:  273 Loss:  0.09579378770163413\n",
      "Epoch:  274 Loss:  0.0957351775507129\n",
      "Epoch:  275 Loss:  0.09567663790294671\n",
      "Epoch:  276 Loss:  0.09561816863944657\n",
      "Epoch:  277 Loss:  0.0955597696415709\n",
      "Epoch:  278 Loss:  0.09550144079092536\n",
      "Epoch:  279 Loss:  0.09544318196936256\n",
      "Epoch:  280 Loss:  0.09538499305898117\n",
      "Epoch:  281 Loss:  0.09532687394212638\n",
      "Epoch:  282 Loss:  0.09526882450138671\n",
      "Epoch:  283 Loss:  0.09521084461959643\n",
      "Epoch:  284 Loss:  0.09515293417983299\n",
      "Epoch:  285 Loss:  0.09509509306541707\n",
      "Epoch:  286 Loss:  0.09503732115991165\n",
      "Epoch:  287 Loss:  0.09497961834712225\n",
      "Epoch:  288 Loss:  0.09492198451109644\n",
      "Epoch:  289 Loss:  0.0948644195361208\n",
      "Epoch:  290 Loss:  0.09480692330672415\n",
      "Epoch:  291 Loss:  0.09474949570767364\n",
      "Epoch:  292 Loss:  0.09469213662397645\n",
      "Epoch:  293 Loss:  0.09463484594087733\n",
      "Epoch:  294 Loss:  0.09457762354386016\n",
      "Epoch:  295 Loss:  0.09452046931864497\n",
      "Epoch:  296 Loss:  0.0944633831511897\n",
      "Epoch:  297 Loss:  0.09440636492768759\n",
      "Epoch:  298 Loss:  0.09434941453456852\n",
      "Epoch:  299 Loss:  0.09429253185849704\n",
      "Epoch:  300 Loss:  0.09423571678637223\n",
      "Epoch:  301 Loss:  0.09417896920532619\n",
      "Epoch:  302 Loss:  0.09412228900272666\n",
      "Epoch:  303 Loss:  0.09406567606617189\n",
      "Epoch:  304 Loss:  0.09400913028349422\n",
      "Epoch:  305 Loss:  0.0939526515427566\n",
      "Epoch:  306 Loss:  0.09389623973225357\n",
      "Epoch:  307 Loss:  0.09383989474051088\n",
      "Epoch:  308 Loss:  0.09378361645628223\n",
      "Epoch:  309 Loss:  0.09372740476855398\n",
      "Epoch:  310 Loss:  0.09367125956653896\n",
      "Epoch:  311 Loss:  0.09361518073967873\n",
      "Epoch:  312 Loss:  0.09355916817764426\n",
      "Epoch:  313 Loss:  0.09350322177033202\n",
      "Epoch:  314 Loss:  0.09344734140786633\n",
      "Epoch:  315 Loss:  0.09339152698059734\n",
      "Epoch:  316 Loss:  0.09333577837910145\n",
      "Epoch:  317 Loss:  0.09328009549417886\n",
      "Epoch:  318 Loss:  0.09322447821685609\n",
      "Epoch:  319 Loss:  0.09316892643838233\n",
      "Epoch:  320 Loss:  0.09311344005023117\n",
      "Epoch:  321 Loss:  0.09305801894409807\n",
      "Epoch:  322 Loss:  0.09300266301190277\n",
      "Epoch:  323 Loss:  0.09294737214578512\n",
      "Epoch:  324 Loss:  0.09289214623810829\n",
      "Epoch:  325 Loss:  0.09283698518145338\n",
      "Epoch:  326 Loss:  0.09278188886862529\n",
      "Epoch:  327 Loss:  0.09272685719264638\n",
      "Epoch:  328 Loss:  0.09267189004675828\n",
      "Epoch:  329 Loss:  0.092616987324423\n",
      "Epoch:  330 Loss:  0.09256214891931944\n",
      "Epoch:  331 Loss:  0.09250737472534508\n",
      "Epoch:  332 Loss:  0.0924526646366131\n",
      "Epoch:  333 Loss:  0.09239801854745584\n",
      "Epoch:  334 Loss:  0.09234343635241866\n",
      "Epoch:  335 Loss:  0.09228891794626512\n",
      "Epoch:  336 Loss:  0.09223446322397345\n",
      "Epoch:  337 Loss:  0.09218007208073567\n",
      "Epoch:  338 Loss:  0.09212574441195784\n",
      "Epoch:  339 Loss:  0.09207148011326063\n",
      "Epoch:  340 Loss:  0.09201727908047727\n",
      "Epoch:  341 Loss:  0.09196314120965353\n",
      "Epoch:  342 Loss:  0.09190906639704653\n",
      "Epoch:  343 Loss:  0.09185505453912655\n",
      "Epoch:  344 Loss:  0.09180110553257398\n",
      "Epoch:  345 Loss:  0.09174721927427879\n",
      "Epoch:  346 Loss:  0.09169339566134314\n",
      "Epoch:  347 Loss:  0.09163963459107777\n",
      "Epoch:  348 Loss:  0.09158593596100215\n",
      "Epoch:  349 Loss:  0.09153229966884442\n",
      "Epoch:  350 Loss:  0.09147872561254021\n",
      "Epoch:  351 Loss:  0.09142521369023417\n",
      "Epoch:  352 Loss:  0.09137176380027767\n",
      "Epoch:  353 Loss:  0.09131837584122753\n",
      "Epoch:  354 Loss:  0.09126504971184755\n",
      "Epoch:  355 Loss:  0.09121178531110755\n",
      "Epoch:  356 Loss:  0.0911585825381811\n",
      "Epoch:  357 Loss:  0.09110544129244763\n",
      "Epoch:  358 Loss:  0.09105236147349016\n",
      "Epoch:  359 Loss:  0.09099934298109567\n",
      "Epoch:  360 Loss:  0.0909463857152546\n",
      "Epoch:  361 Loss:  0.09089348957615888\n",
      "Epoch:  362 Loss:  0.09084065446420328\n",
      "Epoch:  363 Loss:  0.09078788027998604\n",
      "Epoch:  364 Loss:  0.09073516692430296\n",
      "Epoch:  365 Loss:  0.09068251429815435\n",
      "Epoch:  366 Loss:  0.09062992230273868\n",
      "Epoch:  367 Loss:  0.09057739083945456\n",
      "Epoch:  368 Loss:  0.09052491980990027\n",
      "Epoch:  369 Loss:  0.09047250911587419\n",
      "Epoch:  370 Loss:  0.09042015865937017\n",
      "Epoch:  371 Loss:  0.0903678683425828\n",
      "Epoch:  372 Loss:  0.09031563806790231\n",
      "Epoch:  373 Loss:  0.09026346773791735\n",
      "Epoch:  374 Loss:  0.09021135725541155\n",
      "Epoch:  375 Loss:  0.09015930652336719\n",
      "Epoch:  376 Loss:  0.09010731544495917\n",
      "Epoch:  377 Loss:  0.09005538392356022\n",
      "Epoch:  378 Loss:  0.09000351186273486\n",
      "Epoch:  379 Loss:  0.08995169916624454\n",
      "Epoch:  380 Loss:  0.08989994573804405\n",
      "Epoch:  381 Loss:  0.08984825148227972\n",
      "Epoch:  382 Loss:  0.08979661630329261\n",
      "Epoch:  383 Loss:  0.08974504010561567\n",
      "Epoch:  384 Loss:  0.0896935227939731\n",
      "Epoch:  385 Loss:  0.08964206427328218\n",
      "Epoch:  386 Loss:  0.08959066444864956\n",
      "Epoch:  387 Loss:  0.08953932322537336\n",
      "Epoch:  388 Loss:  0.0894880405089425\n",
      "Epoch:  389 Loss:  0.08943681620503396\n",
      "Epoch:  390 Loss:  0.08938565021951558\n",
      "Epoch:  391 Loss:  0.08933454245844367\n",
      "Epoch:  392 Loss:  0.08928349282806172\n",
      "Epoch:  393 Loss:  0.08923250123480311\n",
      "Epoch:  394 Loss:  0.08918156758528681\n",
      "Epoch:  395 Loss:  0.08913069178632096\n",
      "Epoch:  396 Loss:  0.08907987374489779\n",
      "Epoch:  397 Loss:  0.08902911336819903\n",
      "Epoch:  398 Loss:  0.08897841056358768\n",
      "Epoch:  399 Loss:  0.08892776523861694\n",
      "Epoch:  400 Loss:  0.08887717730102114\n",
      "Epoch:  401 Loss:  0.08882664665872099\n",
      "Epoch:  402 Loss:  0.08877617321982093\n",
      "Epoch:  403 Loss:  0.0887257568926085\n",
      "Epoch:  404 Loss:  0.08867539758555483\n",
      "Epoch:  405 Loss:  0.08862509520731353\n",
      "Epoch:  406 Loss:  0.0885748496667203\n",
      "Epoch:  407 Loss:  0.08852466087279368\n",
      "Epoch:  408 Loss:  0.08847452873473255\n",
      "Epoch:  409 Loss:  0.08842445316191781\n",
      "Epoch:  410 Loss:  0.08837443406391003\n",
      "Epoch:  411 Loss:  0.08832447135045027\n",
      "Epoch:  412 Loss:  0.08827456493145984\n",
      "Epoch:  413 Loss:  0.08822471471703895\n",
      "Epoch:  414 Loss:  0.0881749206174667\n",
      "Epoch:  415 Loss:  0.08812518254320038\n",
      "Epoch:  416 Loss:  0.08807550040487704\n",
      "Epoch:  417 Loss:  0.08802587411330944\n",
      "Epoch:  418 Loss:  0.08797630357948802\n",
      "Epoch:  419 Loss:  0.08792678871458022\n",
      "Epoch:  420 Loss:  0.08787732942993107\n",
      "Epoch:  421 Loss:  0.08782792563706004\n",
      "Epoch:  422 Loss:  0.08777857724766298\n",
      "Epoch:  423 Loss:  0.0877292841736106\n",
      "Epoch:  424 Loss:  0.08768004632694905\n",
      "Epoch:  425 Loss:  0.08763086361989758\n",
      "Epoch:  426 Loss:  0.08758173596485082\n",
      "Epoch:  427 Loss:  0.08753266327437631\n",
      "Epoch:  428 Loss:  0.08748364546121537\n",
      "Epoch:  429 Loss:  0.08743468243828079\n",
      "Epoch:  430 Loss:  0.08738577411865912\n",
      "Epoch:  431 Loss:  0.08733692041560867\n",
      "Epoch:  432 Loss:  0.08728812124255798\n",
      "Epoch:  433 Loss:  0.08723937651310863\n",
      "Epoch:  434 Loss:  0.087190686141032\n",
      "Epoch:  435 Loss:  0.08714205004026983\n",
      "Epoch:  436 Loss:  0.08709346812493399\n",
      "Epoch:  437 Loss:  0.08704494030930618\n",
      "Epoch:  438 Loss:  0.08699646650783709\n",
      "Epoch:  439 Loss:  0.08694804663514615\n",
      "Epoch:  440 Loss:  0.08689968060602163\n",
      "Epoch:  441 Loss:  0.0868513683354185\n",
      "Epoch:  442 Loss:  0.08680310973846159\n",
      "Epoch:  443 Loss:  0.08675490473044122\n",
      "Epoch:  444 Loss:  0.0867067532268152\n",
      "Epoch:  445 Loss:  0.08665865514320785\n",
      "Epoch:  446 Loss:  0.08661061039540933\n",
      "Epoch:  447 Loss:  0.08656261889937573\n",
      "Epoch:  448 Loss:  0.08651468057122852\n",
      "Epoch:  449 Loss:  0.08646679532725408\n",
      "Epoch:  450 Loss:  0.08641896308390312\n",
      "Epoch:  451 Loss:  0.08637118375779107\n",
      "Epoch:  452 Loss:  0.08632345726569628\n",
      "Epoch:  453 Loss:  0.08627578352456113\n",
      "Epoch:  454 Loss:  0.08622816245149159\n",
      "Epoch:  455 Loss:  0.08618059396375562\n",
      "Epoch:  456 Loss:  0.08613307797878318\n",
      "Epoch:  457 Loss:  0.0860856144141671\n",
      "Epoch:  458 Loss:  0.0860382031876605\n",
      "Epoch:  459 Loss:  0.08599084421717887\n",
      "Epoch:  460 Loss:  0.08594353742079781\n",
      "Epoch:  461 Loss:  0.08589628271675423\n",
      "Epoch:  462 Loss:  0.08584908002344399\n",
      "Epoch:  463 Loss:  0.08580192925942393\n",
      "Epoch:  464 Loss:  0.08575483034340846\n",
      "Epoch:  465 Loss:  0.08570778319427257\n",
      "Epoch:  466 Loss:  0.08566078773104911\n",
      "Epoch:  467 Loss:  0.08561384387292913\n",
      "Epoch:  468 Loss:  0.08556695153926246\n",
      "Epoch:  469 Loss:  0.08552011064955531\n",
      "Epoch:  470 Loss:  0.085473321123472\n",
      "Epoch:  471 Loss:  0.08542658288083221\n",
      "Epoch:  472 Loss:  0.08537989584161344\n",
      "Epoch:  473 Loss:  0.08533325992594942\n",
      "Epoch:  474 Loss:  0.08528667505412835\n",
      "Epoch:  475 Loss:  0.08524014114659466\n",
      "Epoch:  476 Loss:  0.08519365812394757\n",
      "Epoch:  477 Loss:  0.08514722590694096\n",
      "Epoch:  478 Loss:  0.08510084441648327\n",
      "Epoch:  479 Loss:  0.0850545135736362\n",
      "Epoch:  480 Loss:  0.08500823329961577\n",
      "Epoch:  481 Loss:  0.08496200351579099\n",
      "Epoch:  482 Loss:  0.08491582414368361\n",
      "Epoch:  483 Loss:  0.08486969510496829\n",
      "Epoch:  484 Loss:  0.08482361632147144\n",
      "Epoch:  485 Loss:  0.08477758771517147\n",
      "Epoch:  486 Loss:  0.08473160920819904\n",
      "Epoch:  487 Loss:  0.08468568072283367\n",
      "Epoch:  488 Loss:  0.08463980218150881\n",
      "Epoch:  489 Loss:  0.08459397350680646\n",
      "Epoch:  490 Loss:  0.08454819462145884\n",
      "Epoch:  491 Loss:  0.08450246544834858\n",
      "Epoch:  492 Loss:  0.0844567859105076\n",
      "Epoch:  493 Loss:  0.08441115593111581\n",
      "Epoch:  494 Loss:  0.08436557543350393\n",
      "Epoch:  495 Loss:  0.08432004434114926\n",
      "Epoch:  496 Loss:  0.08427456257767761\n",
      "Epoch:  497 Loss:  0.08422913006686425\n",
      "Epoch:  498 Loss:  0.08418374673262886\n",
      "Epoch:  499 Loss:  0.08413841249904085\n",
      "Epoch:  500 Loss:  0.08409312729031464\n",
      "Epoch:  501 Loss:  0.08404789103081188\n",
      "Epoch:  502 Loss:  0.08400270364504034\n",
      "Epoch:  503 Loss:  0.0839575650576528\n",
      "Epoch:  504 Loss:  0.08391247519344854\n",
      "Epoch:  505 Loss:  0.0838674339773713\n",
      "Epoch:  506 Loss:  0.08382244133450928\n",
      "Epoch:  507 Loss:  0.08377749719009583\n",
      "Epoch:  508 Loss:  0.08373260146950719\n",
      "Epoch:  509 Loss:  0.08368775409826465\n",
      "Epoch:  510 Loss:  0.08364295500203295\n",
      "Epoch:  511 Loss:  0.08359820410661886\n",
      "Epoch:  512 Loss:  0.08355350133797265\n",
      "Epoch:  513 Loss:  0.08350884662218712\n",
      "Epoch:  514 Loss:  0.08346423988549688\n",
      "Epoch:  515 Loss:  0.0834196810542788\n",
      "Epoch:  516 Loss:  0.08337517005505096\n",
      "Epoch:  517 Loss:  0.08333070681447251\n",
      "Epoch:  518 Loss:  0.08328629125934336\n",
      "Epoch:  519 Loss:  0.0832419233166047\n",
      "Epoch:  520 Loss:  0.08319760291333697\n",
      "Epoch:  521 Loss:  0.08315332997676139\n",
      "Epoch:  522 Loss:  0.08310910443423822\n",
      "Epoch:  523 Loss:  0.08306492621326746\n",
      "Epoch:  524 Loss:  0.08302079524148777\n",
      "Epoch:  525 Loss:  0.08297671144667623\n",
      "Epoch:  526 Loss:  0.08293267475674819\n",
      "Epoch:  527 Loss:  0.082888685099758\n",
      "Epoch:  528 Loss:  0.0828447424038977\n",
      "Epoch:  529 Loss:  0.08280084659749425\n",
      "Epoch:  530 Loss:  0.0827569976090151\n",
      "Epoch:  531 Loss:  0.08271319536706247\n",
      "Epoch:  532 Loss:  0.08266943980037564\n",
      "Epoch:  533 Loss:  0.08262573083783051\n",
      "Epoch:  534 Loss:  0.08258206840843729\n",
      "Epoch:  535 Loss:  0.08253845244134284\n",
      "Epoch:  536 Loss:  0.08249488286582989\n",
      "Epoch:  537 Loss:  0.08245135961131543\n",
      "Epoch:  538 Loss:  0.08240788260735074\n",
      "Epoch:  539 Loss:  0.082364451783622\n",
      "Epoch:  540 Loss:  0.08232106706994884\n",
      "Epoch:  541 Loss:  0.08227772839628587\n",
      "Epoch:  542 Loss:  0.08223443569272001\n",
      "Epoch:  543 Loss:  0.08219118888947091\n",
      "Epoch:  544 Loss:  0.08214798791689283\n",
      "Epoch:  545 Loss:  0.08210483270547062\n",
      "Epoch:  546 Loss:  0.0820617231858236\n",
      "Epoch:  547 Loss:  0.08201865928870006\n",
      "Epoch:  548 Loss:  0.08197564094498301\n",
      "Epoch:  549 Loss:  0.08193266808568467\n",
      "Epoch:  550 Loss:  0.08188974064194923\n",
      "Epoch:  551 Loss:  0.0818468585450519\n",
      "Epoch:  552 Loss:  0.08180402172639768\n",
      "Epoch:  553 Loss:  0.08176123011752216\n",
      "Epoch:  554 Loss:  0.08171848365009021\n",
      "Epoch:  555 Loss:  0.08167578225589793\n",
      "Epoch:  556 Loss:  0.08163312586686859\n",
      "Epoch:  557 Loss:  0.08159051441505634\n",
      "Epoch:  558 Loss:  0.08154794783264296\n",
      "Epoch:  559 Loss:  0.08150542605193876\n",
      "Epoch:  560 Loss:  0.08146294900538274\n",
      "Epoch:  561 Loss:  0.08142051662554164\n",
      "Epoch:  562 Loss:  0.08137812884510986\n",
      "Epoch:  563 Loss:  0.08133578559690793\n",
      "Epoch:  564 Loss:  0.08129348681388544\n",
      "Epoch:  565 Loss:  0.08125123242911737\n",
      "Epoch:  566 Loss:  0.08120902237580549\n",
      "Epoch:  567 Loss:  0.08116685658727767\n",
      "Epoch:  568 Loss:  0.08112473499698726\n",
      "Epoch:  569 Loss:  0.0810826575385145\n",
      "Epoch:  570 Loss:  0.0810406241455636\n",
      "Epoch:  571 Loss:  0.08099863475196509\n",
      "Epoch:  572 Loss:  0.08095668929167274\n",
      "Epoch:  573 Loss:  0.08091478769876616\n",
      "Epoch:  574 Loss:  0.0808729299074493\n",
      "Epoch:  575 Loss:  0.08083111585204844\n",
      "Epoch:  576 Loss:  0.08078934546701498\n",
      "Epoch:  577 Loss:  0.08074761868692391\n",
      "Epoch:  578 Loss:  0.08070593544647167\n",
      "Epoch:  579 Loss:  0.08066429568047971\n",
      "Epoch:  580 Loss:  0.08062269932389043\n",
      "Epoch:  581 Loss:  0.08058114631176896\n",
      "Epoch:  582 Loss:  0.08053963657930299\n",
      "Epoch:  583 Loss:  0.08049817006180154\n",
      "Epoch:  584 Loss:  0.08045674669469545\n",
      "Epoch:  585 Loss:  0.08041536641353551\n",
      "Epoch:  586 Loss:  0.08037402915399611\n",
      "Epoch:  587 Loss:  0.08033273485186908\n",
      "Epoch:  588 Loss:  0.08029148344306972\n",
      "Epoch:  589 Loss:  0.08025027486363108\n",
      "Epoch:  590 Loss:  0.08020910904970777\n",
      "Epoch:  591 Loss:  0.0801679859375732\n",
      "Epoch:  592 Loss:  0.0801269054636202\n",
      "Epoch:  593 Loss:  0.0800858675643608\n",
      "Epoch:  594 Loss:  0.08004487217642622\n",
      "Epoch:  595 Loss:  0.08000391923656572\n",
      "Epoch:  596 Loss:  0.07996300868164698\n",
      "Epoch:  597 Loss:  0.07992214044865588\n",
      "Epoch:  598 Loss:  0.07988131447469639\n",
      "Epoch:  599 Loss:  0.07984053069698928\n",
      "Epoch:  600 Loss:  0.07979978905287323\n",
      "Epoch:  601 Loss:  0.07975908947980347\n",
      "Epoch:  602 Loss:  0.07971843191535229\n",
      "Epoch:  603 Loss:  0.07967781629720881\n",
      "Epoch:  604 Loss:  0.07963724256317721\n",
      "Epoch:  605 Loss:  0.0795967106511792\n",
      "Epoch:  606 Loss:  0.07955622049925079\n",
      "Epoch:  607 Loss:  0.07951577204554468\n",
      "Epoch:  608 Loss:  0.079475365228328\n",
      "Epoch:  609 Loss:  0.079434999985983\n",
      "Epoch:  610 Loss:  0.0793946762570071\n",
      "Epoch:  611 Loss:  0.07935439398001141\n",
      "Epoch:  612 Loss:  0.07931415309372199\n",
      "Epoch:  613 Loss:  0.07927395353697869\n",
      "Epoch:  614 Loss:  0.07923379524873435\n",
      "Epoch:  615 Loss:  0.07919367816805623\n",
      "Epoch:  616 Loss:  0.07915360223412472\n",
      "Epoch:  617 Loss:  0.07911356738623211\n",
      "Epoch:  618 Loss:  0.07907357356378505\n",
      "Epoch:  619 Loss:  0.07903362070630103\n",
      "Epoch:  620 Loss:  0.07899370875341113\n",
      "Epoch:  621 Loss:  0.07895383764485754\n",
      "Epoch:  622 Loss:  0.07891400732049453\n",
      "Epoch:  623 Loss:  0.07887421772028776\n",
      "Epoch:  624 Loss:  0.07883446878431428\n",
      "Epoch:  625 Loss:  0.07879476045276168\n",
      "Epoch:  626 Loss:  0.07875509266592891\n",
      "Epoch:  627 Loss:  0.07871546536422507\n",
      "Epoch:  628 Loss:  0.07867587848817004\n",
      "Epoch:  629 Loss:  0.07863633197839312\n",
      "Epoch:  630 Loss:  0.07859682577563351\n",
      "Epoch:  631 Loss:  0.07855735982073982\n",
      "Epoch:  632 Loss:  0.07851793405467075\n",
      "Epoch:  633 Loss:  0.0784785484184931\n",
      "Epoch:  634 Loss:  0.07843920285338328\n",
      "Epoch:  635 Loss:  0.07839989730062628\n",
      "Epoch:  636 Loss:  0.07836063170161466\n",
      "Epoch:  637 Loss:  0.07832140599784934\n",
      "Epoch:  638 Loss:  0.07828222013094042\n",
      "Epoch:  639 Loss:  0.07824307404260378\n",
      "Epoch:  640 Loss:  0.07820396767466392\n",
      "Epoch:  641 Loss:  0.07816490096905192\n",
      "Epoch:  642 Loss:  0.07812587386780667\n",
      "Epoch:  643 Loss:  0.07808688631307294\n",
      "Epoch:  644 Loss:  0.07804793824710146\n",
      "Epoch:  645 Loss:  0.07800902961225134\n",
      "Epoch:  646 Loss:  0.07797016035098576\n",
      "Epoch:  647 Loss:  0.07793133040587401\n",
      "Epoch:  648 Loss:  0.07789253971959206\n",
      "Epoch:  649 Loss:  0.07785378823491959\n",
      "Epoch:  650 Loss:  0.07781507589474292\n",
      "Epoch:  651 Loss:  0.07777640264205248\n",
      "Epoch:  652 Loss:  0.07773776841994243\n",
      "Epoch:  653 Loss:  0.07769917317161393\n",
      "Epoch:  654 Loss:  0.07766061684036976\n",
      "Epoch:  655 Loss:  0.07762209936961811\n",
      "Epoch:  656 Loss:  0.07758362070286955\n",
      "Epoch:  657 Loss:  0.07754518078374077\n",
      "Epoch:  658 Loss:  0.0775067795559493\n",
      "Epoch:  659 Loss:  0.07746841696331633\n",
      "Epoch:  660 Loss:  0.077430092949766\n",
      "Epoch:  661 Loss:  0.07739180745932583\n",
      "Epoch:  662 Loss:  0.07735356043612483\n",
      "Epoch:  663 Loss:  0.07731535182439468\n",
      "Epoch:  664 Loss:  0.07727718156846823\n",
      "Epoch:  665 Loss:  0.07723904961278225\n",
      "Epoch:  666 Loss:  0.0772009559018714\n",
      "Epoch:  667 Loss:  0.0771629003803749\n",
      "Epoch:  668 Loss:  0.07712488299303207\n",
      "Epoch:  669 Loss:  0.07708690368468268\n",
      "Epoch:  670 Loss:  0.07704896240026757\n",
      "Epoch:  671 Loss:  0.0770110590848272\n",
      "Epoch:  672 Loss:  0.07697319368350376\n",
      "Epoch:  673 Loss:  0.07693536614153769\n",
      "Epoch:  674 Loss:  0.07689757640427042\n",
      "Epoch:  675 Loss:  0.07685982441714309\n",
      "Epoch:  676 Loss:  0.07682211012569533\n",
      "Epoch:  677 Loss:  0.07678443347556706\n",
      "Epoch:  678 Loss:  0.07674679441249549\n",
      "Epoch:  679 Loss:  0.07670919288231749\n",
      "Epoch:  680 Loss:  0.07667162883096901\n",
      "Epoch:  681 Loss:  0.0766341022044833\n",
      "Epoch:  682 Loss:  0.0765966129489923\n",
      "Epoch:  683 Loss:  0.07655916101072517\n",
      "Epoch:  684 Loss:  0.07652174633601008\n",
      "Epoch:  685 Loss:  0.0764843688712706\n",
      "Epoch:  686 Loss:  0.07644702856302922\n",
      "Epoch:  687 Loss:  0.07640972535790533\n",
      "Epoch:  688 Loss:  0.07637245920261396\n",
      "Epoch:  689 Loss:  0.07633523004396846\n",
      "Epoch:  690 Loss:  0.07629803782887713\n",
      "Epoch:  691 Loss:  0.07626088250434516\n",
      "Epoch:  692 Loss:  0.07622376401747295\n",
      "Epoch:  693 Loss:  0.07618668231545819\n",
      "Epoch:  694 Loss:  0.07614963734559328\n",
      "Epoch:  695 Loss:  0.07611262905526567\n",
      "Epoch:  696 Loss:  0.07607565739195828\n",
      "Epoch:  697 Loss:  0.07603872230324966\n",
      "Epoch:  698 Loss:  0.0760018237368119\n",
      "Epoch:  699 Loss:  0.07596496164041223\n",
      "Epoch:  700 Loss:  0.07592813596191354\n",
      "Epoch:  701 Loss:  0.07589134664926979\n",
      "Epoch:  702 Loss:  0.07585459365053245\n",
      "Epoch:  703 Loss:  0.07581787691384403\n",
      "Epoch:  704 Loss:  0.07578119638744257\n",
      "Epoch:  705 Loss:  0.07574455201965719\n",
      "Epoch:  706 Loss:  0.07570794375891315\n",
      "Epoch:  707 Loss:  0.07567137155372566\n",
      "Epoch:  708 Loss:  0.07563483535270546\n",
      "Epoch:  709 Loss:  0.0755983351045535\n",
      "Epoch:  710 Loss:  0.07556187075806457\n",
      "Epoch:  711 Loss:  0.07552544226212467\n",
      "Epoch:  712 Loss:  0.07548904956571291\n",
      "Epoch:  713 Loss:  0.07545269261789907\n",
      "Epoch:  714 Loss:  0.07541637136784551\n",
      "Epoch:  715 Loss:  0.07538008576480544\n",
      "Epoch:  716 Loss:  0.0753438357581224\n",
      "Epoch:  717 Loss:  0.07530762129723258\n",
      "Epoch:  718 Loss:  0.07527144233166275\n",
      "Epoch:  719 Loss:  0.07523529881102838\n",
      "Epoch:  720 Loss:  0.07519919068503796\n",
      "Epoch:  721 Loss:  0.07516311790348926\n",
      "Epoch:  722 Loss:  0.07512708041626844\n",
      "Epoch:  723 Loss:  0.0750910781733539\n",
      "Epoch:  724 Loss:  0.07505511112481238\n",
      "Epoch:  725 Loss:  0.07501917922080127\n",
      "Epoch:  726 Loss:  0.07498328241156556\n",
      "Epoch:  727 Loss:  0.07494742064743966\n",
      "Epoch:  728 Loss:  0.07491159387884883\n",
      "Epoch:  729 Loss:  0.07487580205630505\n",
      "Epoch:  730 Loss:  0.07484004513040882\n",
      "Epoch:  731 Loss:  0.07480432305185031\n",
      "Epoch:  732 Loss:  0.07476863577140647\n",
      "Epoch:  733 Loss:  0.07473298323994335\n",
      "Epoch:  734 Loss:  0.07469736540841358\n",
      "Epoch:  735 Loss:  0.07466178222785887\n",
      "Epoch:  736 Loss:  0.07462623364940663\n",
      "Epoch:  737 Loss:  0.07459071962427333\n",
      "Epoch:  738 Loss:  0.07455524010376115\n",
      "Epoch:  739 Loss:  0.0745197950392595\n",
      "Epoch:  740 Loss:  0.07448438438224389\n",
      "Epoch:  741 Loss:  0.07444900808427789\n",
      "Epoch:  742 Loss:  0.07441366609700967\n",
      "Epoch:  743 Loss:  0.07437835837217449\n",
      "Epoch:  744 Loss:  0.07434308486159327\n",
      "Epoch:  745 Loss:  0.07430784551717272\n",
      "Epoch:  746 Loss:  0.07427264029090518\n",
      "Epoch:  747 Loss:  0.07423746913486758\n",
      "Epoch:  748 Loss:  0.07420233200122436\n",
      "Epoch:  749 Loss:  0.0741672288422223\n",
      "Epoch:  750 Loss:  0.07413215961019436\n",
      "Epoch:  751 Loss:  0.07409712425755839\n",
      "Epoch:  752 Loss:  0.07406212273681606\n",
      "Epoch:  753 Loss:  0.07402715500055368\n",
      "Epoch:  754 Loss:  0.07399222100144237\n",
      "Epoch:  755 Loss:  0.07395732069223535\n",
      "Epoch:  756 Loss:  0.07392245402577204\n",
      "Epoch:  757 Loss:  0.07388762095497382\n",
      "Epoch:  758 Loss:  0.07385282143284577\n",
      "Epoch:  759 Loss:  0.0738180554124764\n",
      "Epoch:  760 Loss:  0.07378332284703777\n",
      "Epoch:  761 Loss:  0.07374862368978449\n",
      "Epoch:  762 Loss:  0.07371395789405376\n",
      "Epoch:  763 Loss:  0.07367932541326533\n",
      "Epoch:  764 Loss:  0.07364472620092138\n",
      "Epoch:  765 Loss:  0.07361016021060679\n",
      "Epoch:  766 Loss:  0.07357562739598802\n",
      "Epoch:  767 Loss:  0.0735411277108131\n",
      "Epoch:  768 Loss:  0.07350666110891324\n",
      "Epoch:  769 Loss:  0.07347222754419934\n",
      "Epoch:  770 Loss:  0.07343782697066435\n",
      "Epoch:  771 Loss:  0.07340345934238279\n",
      "Epoch:  772 Loss:  0.07336912461351017\n",
      "Epoch:  773 Loss:  0.0733348227382828\n",
      "Epoch:  774 Loss:  0.07330055367101687\n",
      "Epoch:  775 Loss:  0.07326631736611025\n",
      "Epoch:  776 Loss:  0.07323211377804063\n",
      "Epoch:  777 Loss:  0.07319794286136552\n",
      "Epoch:  778 Loss:  0.0731638045707236\n",
      "Epoch:  779 Loss:  0.0731296988608321\n",
      "Epoch:  780 Loss:  0.07309562568648846\n",
      "Epoch:  781 Loss:  0.07306158500256987\n",
      "Epoch:  782 Loss:  0.07302757676403308\n",
      "Epoch:  783 Loss:  0.07299360092591274\n",
      "Epoch:  784 Loss:  0.07295965744332401\n",
      "Epoch:  785 Loss:  0.07292574627146065\n",
      "Epoch:  786 Loss:  0.07289186736559375\n",
      "Epoch:  787 Loss:  0.07285802068107446\n",
      "Epoch:  788 Loss:  0.07282420617333188\n",
      "Epoch:  789 Loss:  0.07279042379787314\n",
      "Epoch:  790 Loss:  0.07275667351028316\n",
      "Epoch:  791 Loss:  0.07272295526622606\n",
      "Epoch:  792 Loss:  0.07268926902144156\n",
      "Epoch:  793 Loss:  0.07265561473174849\n",
      "Epoch:  794 Loss:  0.0726219923530428\n",
      "Epoch:  795 Loss:  0.0725884018412974\n",
      "Epoch:  796 Loss:  0.07255484315256254\n",
      "Epoch:  797 Loss:  0.07252131624296503\n",
      "Epoch:  798 Loss:  0.0724878210687078\n",
      "Epoch:  799 Loss:  0.07245435758607283\n",
      "Epoch:  800 Loss:  0.07242092575141626\n",
      "Epoch:  801 Loss:  0.07238752552117067\n",
      "Epoch:  802 Loss:  0.07235415685184615\n",
      "Epoch:  803 Loss:  0.07232081970002696\n",
      "Epoch:  804 Loss:  0.07228751402237453\n",
      "Epoch:  805 Loss:  0.0722542397756251\n",
      "Epoch:  806 Loss:  0.0722209969165911\n",
      "Epoch:  807 Loss:  0.07218778540216024\n",
      "Epoch:  808 Loss:  0.07215460518929433\n",
      "Epoch:  809 Loss:  0.07212145623503176\n",
      "Epoch:  810 Loss:  0.07208833849648466\n",
      "Epoch:  811 Loss:  0.07205525193084009\n",
      "Epoch:  812 Loss:  0.07202219649535999\n",
      "Epoch:  813 Loss:  0.0719891721473804\n",
      "Epoch:  814 Loss:  0.0719561788443115\n",
      "Epoch:  815 Loss:  0.07192321654363906\n",
      "Epoch:  816 Loss:  0.07189028520292029\n",
      "Epoch:  817 Loss:  0.07185738477978756\n",
      "Epoch:  818 Loss:  0.0718245152319472\n",
      "Epoch:  819 Loss:  0.07179167651717808\n",
      "Epoch:  820 Loss:  0.07175886859333334\n",
      "Epoch:  821 Loss:  0.07172609141833902\n",
      "Epoch:  822 Loss:  0.07169334495019344\n",
      "Epoch:  823 Loss:  0.07166062914696905\n",
      "Epoch:  824 Loss:  0.07162794396680977\n",
      "Epoch:  825 Loss:  0.07159528936793359\n",
      "Epoch:  826 Loss:  0.071562665308629\n",
      "Epoch:  827 Loss:  0.07153007174725905\n",
      "Epoch:  828 Loss:  0.07149750864225655\n",
      "Epoch:  829 Loss:  0.07146497595212864\n",
      "Epoch:  830 Loss:  0.07143247363545212\n",
      "Epoch:  831 Loss:  0.07140000165087723\n",
      "Epoch:  832 Loss:  0.07136755995712428\n",
      "Epoch:  833 Loss:  0.07133514851298679\n",
      "Epoch:  834 Loss:  0.07130276727732753\n",
      "Epoch:  835 Loss:  0.07127041620908199\n",
      "Epoch:  836 Loss:  0.07123809526725541\n",
      "Epoch:  837 Loss:  0.0712058044109246\n",
      "Epoch:  838 Loss:  0.07117354359923718\n",
      "Epoch:  839 Loss:  0.0711413127914103\n",
      "Epoch:  840 Loss:  0.07110911194673258\n",
      "Epoch:  841 Loss:  0.07107694102456211\n",
      "Epoch:  842 Loss:  0.07104479998432803\n",
      "Epoch:  843 Loss:  0.07101268878552801\n",
      "Epoch:  844 Loss:  0.07098060738773065\n",
      "Epoch:  845 Loss:  0.07094855575057447\n",
      "Epoch:  846 Loss:  0.07091653383376573\n",
      "Epoch:  847 Loss:  0.07088454159708175\n",
      "Epoch:  848 Loss:  0.07085257900036918\n",
      "Epoch:  849 Loss:  0.07082064600354221\n",
      "Epoch:  850 Loss:  0.07078874256658589\n",
      "Epoch:  851 Loss:  0.07075686864955248\n",
      "Epoch:  852 Loss:  0.07072502421256341\n",
      "Epoch:  853 Loss:  0.07069320921580945\n",
      "Epoch:  854 Loss:  0.07066142361954887\n",
      "Epoch:  855 Loss:  0.07062966738410807\n",
      "Epoch:  856 Loss:  0.07059794046988262\n",
      "Epoch:  857 Loss:  0.07056624283733481\n",
      "Epoch:  858 Loss:  0.07053457444699564\n",
      "Epoch:  859 Loss:  0.07050293525946301\n",
      "Epoch:  860 Loss:  0.07047132523540389\n",
      "Epoch:  861 Loss:  0.07043974433555036\n",
      "Epoch:  862 Loss:  0.07040819252070428\n",
      "Epoch:  863 Loss:  0.07037666975173265\n",
      "Epoch:  864 Loss:  0.07034517598957107\n",
      "Epoch:  865 Loss:  0.07031371119522024\n",
      "Epoch:  866 Loss:  0.07028227532974947\n",
      "Epoch:  867 Loss:  0.07025086835429296\n",
      "Epoch:  868 Loss:  0.07021949023005288\n",
      "Epoch:  869 Loss:  0.07018814091829702\n",
      "Epoch:  870 Loss:  0.07015682038035946\n",
      "Epoch:  871 Loss:  0.07012552857763986\n",
      "Epoch:  872 Loss:  0.07009426547160501\n",
      "Epoch:  873 Loss:  0.07006303102378585\n",
      "Epoch:  874 Loss:  0.07003182519578043\n",
      "Epoch:  875 Loss:  0.07000064794925198\n",
      "Epoch:  876 Loss:  0.06996949924592831\n",
      "Epoch:  877 Loss:  0.06993837904760364\n",
      "Epoch:  878 Loss:  0.0699072873161361\n",
      "Epoch:  879 Loss:  0.06987622401345062\n",
      "Epoch:  880 Loss:  0.0698451891015347\n",
      "Epoch:  881 Loss:  0.0698141825424425\n",
      "Epoch:  882 Loss:  0.06978320429829253\n",
      "Epoch:  883 Loss:  0.06975225433126615\n",
      "Epoch:  884 Loss:  0.06972133260361099\n",
      "Epoch:  885 Loss:  0.06969043907763763\n",
      "Epoch:  886 Loss:  0.06965957371572154\n",
      "Epoch:  887 Loss:  0.06962873648030177\n",
      "Epoch:  888 Loss:  0.06959792733388205\n",
      "Epoch:  889 Loss:  0.06956714623902742\n",
      "Epoch:  890 Loss:  0.06953639315836962\n",
      "Epoch:  891 Loss:  0.06950566805460096\n",
      "Epoch:  892 Loss:  0.06947497089047912\n",
      "Epoch:  893 Loss:  0.06944430162882474\n",
      "Epoch:  894 Loss:  0.06941366023252032\n",
      "Epoch:  895 Loss:  0.06938304666451185\n",
      "Epoch:  896 Loss:  0.0693524608878091\n",
      "Epoch:  897 Loss:  0.06932190286548306\n",
      "Epoch:  898 Loss:  0.06929137256066818\n",
      "Epoch:  899 Loss:  0.06926086993656057\n",
      "Epoch:  900 Loss:  0.06923039495641985\n",
      "Epoch:  901 Loss:  0.06919994758356689\n",
      "Epoch:  902 Loss:  0.06916952778138442\n",
      "Epoch:  903 Loss:  0.06913913551331796\n",
      "Epoch:  904 Loss:  0.06910877074287389\n",
      "Epoch:  905 Loss:  0.06907843343362126\n",
      "Epoch:  906 Loss:  0.06904812354918956\n",
      "Epoch:  907 Loss:  0.06901784105327133\n",
      "Epoch:  908 Loss:  0.06898758590961811\n",
      "Epoch:  909 Loss:  0.06895735808204478\n",
      "Epoch:  910 Loss:  0.06892715753442581\n",
      "Epoch:  911 Loss:  0.06889698423069791\n",
      "Epoch:  912 Loss:  0.06886683813485726\n",
      "Epoch:  913 Loss:  0.06883671921096143\n",
      "Epoch:  914 Loss:  0.0688066274231288\n",
      "Epoch:  915 Loss:  0.06877656273553748\n",
      "Epoch:  916 Loss:  0.0687465251124265\n",
      "Epoch:  917 Loss:  0.06871651451809456\n",
      "Epoch:  918 Loss:  0.06868653091690204\n",
      "Epoch:  919 Loss:  0.06865657427326664\n",
      "Epoch:  920 Loss:  0.068626644551668\n",
      "Epoch:  921 Loss:  0.06859674171664451\n",
      "Epoch:  922 Loss:  0.06856686573279498\n",
      "Epoch:  923 Loss:  0.06853701656477679\n",
      "Epoch:  924 Loss:  0.06850719417730662\n",
      "Epoch:  925 Loss:  0.06847739853516172\n",
      "Epoch:  926 Loss:  0.06844762960317742\n",
      "Epoch:  927 Loss:  0.0684178873462479\n",
      "Epoch:  928 Loss:  0.0683881717293271\n",
      "Epoch:  929 Loss:  0.068358482717426\n",
      "Epoch:  930 Loss:  0.068328820275617\n",
      "Epoch:  931 Loss:  0.06829918436902908\n",
      "Epoch:  932 Loss:  0.06826957496285015\n",
      "Epoch:  933 Loss:  0.06823999202232565\n",
      "Epoch:  934 Loss:  0.06821043551276057\n",
      "Epoch:  935 Loss:  0.06818090539951795\n",
      "Epoch:  936 Loss:  0.06815140164801659\n",
      "Epoch:  937 Loss:  0.06812192422373615\n",
      "Epoch:  938 Loss:  0.06809247309221171\n",
      "Epoch:  939 Loss:  0.06806304821903754\n",
      "Epoch:  940 Loss:  0.06803364956986428\n",
      "Epoch:  941 Loss:  0.0680042771104009\n",
      "Epoch:  942 Loss:  0.06797493080641269\n",
      "Epoch:  943 Loss:  0.067945610623723\n",
      "Epoch:  944 Loss:  0.06791631652821138\n",
      "Epoch:  945 Loss:  0.06788704848581503\n",
      "Epoch:  946 Loss:  0.06785780646252706\n",
      "Epoch:  947 Loss:  0.06782859042439925\n",
      "Epoch:  948 Loss:  0.06779940033753747\n",
      "Epoch:  949 Loss:  0.0677702361681057\n",
      "Epoch:  950 Loss:  0.06774109788232337\n",
      "Epoch:  951 Loss:  0.06771198544646756\n",
      "Epoch:  952 Loss:  0.06768289882686955\n",
      "Epoch:  953 Loss:  0.06765383798991823\n",
      "Epoch:  954 Loss:  0.06762480290205793\n",
      "Epoch:  955 Loss:  0.06759579352978855\n",
      "Epoch:  956 Loss:  0.06756680983966559\n",
      "Epoch:  957 Loss:  0.06753785179830103\n",
      "Epoch:  958 Loss:  0.0675089193723615\n",
      "Epoch:  959 Loss:  0.06748001252856893\n",
      "Epoch:  960 Loss:  0.06745113123370113\n",
      "Epoch:  961 Loss:  0.06742227545459062\n",
      "Epoch:  962 Loss:  0.06739344515812531\n",
      "Epoch:  963 Loss:  0.06736464031124782\n",
      "Epoch:  964 Loss:  0.06733586088095507\n",
      "Epoch:  965 Loss:  0.06730710683429995\n",
      "Epoch:  966 Loss:  0.06727837813838866\n",
      "Epoch:  967 Loss:  0.06724967476038353\n",
      "Epoch:  968 Loss:  0.0672209966674989\n",
      "Epoch:  969 Loss:  0.06719234382700552\n",
      "Epoch:  970 Loss:  0.06716371620622757\n",
      "Epoch:  971 Loss:  0.06713511377254344\n",
      "Epoch:  972 Loss:  0.06710653649338491\n",
      "Epoch:  973 Loss:  0.06707798433623842\n",
      "Epoch:  974 Loss:  0.06704945726864352\n",
      "Epoch:  975 Loss:  0.06702095525819418\n",
      "Epoch:  976 Loss:  0.06699247827253692\n",
      "Epoch:  977 Loss:  0.06696402627937235\n",
      "Epoch:  978 Loss:  0.06693559924645436\n",
      "Epoch:  979 Loss:  0.0669071971415903\n",
      "Epoch:  980 Loss:  0.06687881993263992\n",
      "Epoch:  981 Loss:  0.06685046758751692\n",
      "Epoch:  982 Loss:  0.06682214007418659\n",
      "Epoch:  983 Loss:  0.06679383736066845\n",
      "Epoch:  984 Loss:  0.06676555941503443\n",
      "Epoch:  985 Loss:  0.06673730620540788\n",
      "Epoch:  986 Loss:  0.06670907769996628\n",
      "Epoch:  987 Loss:  0.06668087386693858\n",
      "Epoch:  988 Loss:  0.06665269467460612\n",
      "Epoch:  989 Loss:  0.06662454009130232\n",
      "Epoch:  990 Loss:  0.06659641008541306\n",
      "Epoch:  991 Loss:  0.0665683046253768\n",
      "Epoch:  992 Loss:  0.06654022367968196\n",
      "Epoch:  993 Loss:  0.06651216721687095\n",
      "Epoch:  994 Loss:  0.06648413520553612\n",
      "Epoch:  995 Loss:  0.06645612761432249\n",
      "Epoch:  996 Loss:  0.06642814441192632\n",
      "Epoch:  997 Loss:  0.06640018556709448\n",
      "Epoch:  998 Loss:  0.06637225104862646\n",
      "Epoch:  999 Loss:  0.06634434082537174\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, 0.01, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "967\n",
      "Accuracy: 96.7\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 3 Actual Class - 3\n",
      "Predicted Class - 4 Actual Class - 4\n",
      "Predicted Class - 2 Actual Class - 2\n",
      "Predicted Class - 3 Actual Class - 3\n"
     ]
    }
   ],
   "source": [
    "X_test, Y_test = process_data(test_diagram_arr, test_y_arr)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "correct_predictions = np.sum(predictions == test_y_arr)\n",
    "print(correct_predictions)\n",
    "accuracy = (correct_predictions / Y_test.shape[0])*100\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "for i, predicted_class in enumerate(predictions):\n",
    "    print(f\"Predicted Class - {predicted_class} Actual Class - {test_y_arr[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scaling(feature, min_value, max_value):\n",
    "    return (feature - min_value) / (max_value - min_value)\n",
    "\n",
    "# Assuming your features are in the list [0, 1, 0, 1, 1, 0, 19, 20, 18]\n",
    "# Extract the numerical features (assuming the last three are numerical)\n",
    "numerical_features = [0, 1, 0, 1, 1, 0, 19, 20, 18]\n",
    "\n",
    "\n",
    "# Apply Min-Max scaling to each feature\n",
    "scaled_features = [min_max_scaling(feature, min(numerical_features), max(numerical_features)) for feature in numerical_features]\n",
    "\n",
    "print(\"Original Features:\", numerical_features)\n",
    "print(\"Scaled Features:\", scaled_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
